{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effectively evaluating a prompt chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building applications that chain prompts—where one LLM call’s output feeds into the next—it’s best to measure more than a single metric. In this cookbook, we’ll demonstrate how to:\n",
    "\n",
    "1. Trace and evaluate a complete end-to-end agent in Braintrust.\n",
    "2. Isolate and evaluate a particular step in the chain to identify and measure issues.\n",
    "\n",
    "We’ll walk through a travel-planning agent that decides what actions to take (e.g., calling a weather or flight API) and uses a judge function to decide if each step is valid. Finally, it produces an itinerary. We’ll do an end-to-end evaluation, then zoom in on the judge step to see how effectively it flags unnecessary actions.\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Before getting started, make sure you have a [Braintrust account](https://www.braintrust.dev/signup) and an API key for [OpenAI](https://platform.openai.com/). Make sure to plug the OpenAI key into your Braintrust account's [AI providers](https://www.braintrust.dev/app/settings?subroute=secrets) configuration and acquire a [BRAINTRUST_API_KEY](https://www.braintrust.dev/app/settings?subroute=api-keys). You can also add an API key for any other AI provider you'd like but be sure to change the code to use that model. Lastly, add your `BRAINTRUST_API_KEY` to your Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "export BRAINTRUST_API_KEY=\"YOUR_BRAINTRUST_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Callout type=\"info\">\n",
    "Best practice is to export your API key as an environment variable. However, to make it easier to follow along with this cookbook, you can also hardcode it into the code below.\n",
    "</Callout>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required Python dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install braintrust openai autoevals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import all of the modules we need and initialize our OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import openai\n",
    "import jsonschema\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import braintrust\n",
    "import autoevals\n",
    "\n",
    "\n",
    "BRAINTRUST_API_KEY = os.environ.get(\n",
    "    \"BRAINTRUST_API_KEY\", \"sk-2omSuFIj5arE85lA37l1AVwLzx8bgWoVUU0ugTqoZzmxN9JI\"\n",
    ")\n",
    "os.environ[\"BRAINTRUST_API_KEY\"] = BRAINTRUST_API_KEY\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    openai.OpenAI(\n",
    "        api_key=BRAINTRUST_API_KEY,\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock APIs\n",
    "\n",
    "Here we define placeholder “mock” APIs for weather and flight searches. In real applications, you’d call external services or databases. However, for testing and illustration purposes, we simulate dynamic outputs (e.g., randomly chosen weather, airfare prices, seat availability) to confirm the agent logic works without external dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_date() -> str:\n",
    "    base = datetime(2025, 1, 23)\n",
    "    if random.random() < 0.7:\n",
    "        days_ahead = random.randint(1, 10)\n",
    "    else:\n",
    "        days_ahead = random.randint(11, 365)\n",
    "    return (base + timedelta(days=days_ahead)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def mock_weather_api(city: str, date: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"condition\": random.choice([\"sunny\", \"rainy\", \"cloudy\"]),\n",
    "        \"temperature\": random.randint(40, 95),\n",
    "        \"date\": date,\n",
    "    }\n",
    "\n",
    "\n",
    "def mock_flight_api(origin: str, destination: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"economy_price\": random.randint(200, 800),\n",
    "        \"business_price\": random.randint(800, 2000),\n",
    "        \"seats_left\": random.randint(0, 100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON schema & validation helpersfor agent output\n",
    "\n",
    "We use a JSON schema to keep the agent’s output consistent. The agent can only return one of four actions: `GET_WEATHER`, `GET_FLIGHTS`, `GENERATE_ITINERARY`, or `DONE`. This constraint ensures we can reliably parse the agent’s response and handle it safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"action\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"GET_WEATHER\", \"GET_FLIGHTS\", \"GENERATE_ITINERARY\", \"DONE\"],\n",
    "        },\n",
    "        \"parameters\": {\"type\": \"object\"},\n",
    "    },\n",
    "    \"required\": [\"action\"],\n",
    "    \"additionalProperties\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also validate the agent’s JSON output and attempt to fix it if it’s invalid or violates the schema. This prevents downstream parsing errors caused by unexpected or malformed JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm_fix_json(original_content: str, schema: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    If the LLM returns invalid or schema-violating JSON, we ask it to fix it.\n",
    "    Added a brief example of a valid fix to strengthen instructions.\n",
    "    \"\"\"\n",
    "    fix_prompt = f\"\"\"The following JSON is invalid or does not match the required schema:\n",
    "Original JSON:\n",
    "{original_content}\n",
    "\n",
    "Schema (must match exactly):\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Here are examples of valid JSON matching this schema:\n",
    "\n",
    "Example 1 (GET_WEATHER):\n",
    "{{\n",
    "  \"action\": \"GET_WEATHER\",\n",
    "  \"parameters\": {{\n",
    "    \"city\": \"Boston\",\n",
    "    \"date\": \"2025-01-30\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example 2 (GET_FLIGHTS):\n",
    "{{\n",
    "  \"action\": \"GET_FLIGHTS\",\n",
    "  \"parameters\": {{\n",
    "    \"origin\": \"NYC\",\n",
    "    \"destination\": \"Miami\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Now please correct the JSON so it is valid and follows the schema exactly.\n",
    "Return ONLY valid JSON, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a JSON validator. Return valid JSON only.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": fix_prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        braintrust.current_span().log(error=f\"Error during LLM fix: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_json(\n",
    "    content: str, schema: Dict[str, Any], retries: int = 3\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    span = braintrust.current_span()\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            jsonschema.validate(instance=data, schema=schema)\n",
    "            return data\n",
    "        except (json.JSONDecodeError, jsonschema.ValidationError) as e:\n",
    "            span.log(error=f\"JSON validation error (attempt {attempt+1}): {e}\")\n",
    "            fixed_content = prompt_llm_fix_json(content, schema)\n",
    "            if not fixed_content:\n",
    "                break\n",
    "            content = fixed_content\n",
    "    span.log(error=\"Failed to validate JSON after retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step validation and correction\n",
    "\n",
    "The agent may propose actions that are logically unnecessary (e.g., fetching weather it already has) or that contradict existing data. To solve this, we define a judge function to validate each proposed step. For example, if the agent attempts to `GET_WEATHER` a second time for data it already fetched, the judge flags it, then we prompt the LLM to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_step_with_cot(\n",
    "    step_description: str, context_data: Dict[str, Any] = None\n",
    ") -> (bool, str):\n",
    "    \"\"\"\n",
    "    Returns (is_ok, chain_of_thought).\n",
    "    If the final decision is 'N', is_ok=False; if 'Y', is_ok=True.\n",
    "\n",
    "    NOTE: We include some of the context (origin, destination, etc.) so the judge\n",
    "    doesn't assume we have zero info if 'parameters' are empty.\n",
    "    \"\"\"\n",
    "    with braintrust.start_span(name=\"judge_step\") as jspan:\n",
    "        # We gather minimal context details to show the judge that we do have relevant info\n",
    "        context_snippet = \"\"\n",
    "        if context_data:\n",
    "            origin = context_data[\"input_data\"].get(\"origin\", \"\")\n",
    "            destination = context_data[\"input_data\"].get(\"destination\", \"\")\n",
    "            budget = context_data[\"input_data\"].get(\"budget\", \"\")\n",
    "            preferences = context_data[\"input_data\"].get(\"preferences\", {})\n",
    "            # Summarize the existing weather/flight data too\n",
    "            wdata = context_data[\"weather_data\"]\n",
    "            fdata = context_data[\"flight_data\"]\n",
    "\n",
    "            context_snippet = (\n",
    "                f\"Context:\\n\"\n",
    "                f\" - Origin: {origin}\\n\"\n",
    "                f\" - Destination: {destination}\\n\"\n",
    "                f\" - Budget: {budget}\\n\"\n",
    "                f\" - Preferences: {preferences}\\n\"\n",
    "                f\" - Known Weather: {json.dumps(wdata, indent=2)}\\n\"\n",
    "                f\" - Known Flight: {json.dumps(fdata, indent=2)}\\n\"\n",
    "            )\n",
    "\n",
    "        prompt_msg = f\"\"\"You are a strict judge of correctness in a travel-planning chain. Your task is to determine whether or not the next task is a valid step to take. Typically a valid step is if the context/parameter does not yet have information. If the context/parameter already has information, the step is not valid. If all of the context is filled out, then generating the itnerary is a valid itinerary.\n",
    "\n",
    "{context_snippet}\n",
    "\n",
    "Step description:\n",
    "\\\"\\\"\\\"\n",
    "{step_description}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Provide a short chain-of-thought. \n",
    "Then end with: \"Final Decision: Y\" or \"Final Decision: N\"\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a meticulous correctness judge.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt_msg},\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            jspan.log(metadata={\"raw_judge_response\": content})\n",
    "\n",
    "            lines = content.splitlines()\n",
    "            final_decision = \"N\"\n",
    "            rationale_lines = []\n",
    "            for line in lines:\n",
    "                if line.strip().startswith(\"Final Decision:\"):\n",
    "                    if \"Y\" in line.upper():\n",
    "                        final_decision = \"Y\"\n",
    "                    else:\n",
    "                        final_decision = \"N\"\n",
    "                else:\n",
    "                    rationale_lines.append(line)\n",
    "\n",
    "            rationale_text = \"\\n\".join(rationale_lines).strip()\n",
    "            is_ok = final_decision.upper() == \"Y\"\n",
    "            return is_ok, rationale_text\n",
    "\n",
    "        except Exception as e:\n",
    "            jspan.log(error=f\"Judge LLM error: {e}\")\n",
    "            return False, \"Error in judge LLM\"\n",
    "\n",
    "\n",
    "def fix_step_with_cot(current_action_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to fix an incorrect or incomplete travel planning step by providing examples\n",
    "    of a valid step. Strengthened instructions to guide the LLM better.\n",
    "    \"\"\"\n",
    "    fix_prompt = f\"\"\"We have an incorrect or incomplete travel planning step:\n",
    "\n",
    "\\\"\\\"\\\"{current_action_json}\\\"\\\"\\\"\n",
    "\n",
    "Below are examples of valid steps:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "  \"action\": \"GET_WEATHER\",\n",
    "  \"parameters\": {{\n",
    "    \"city\": \"London\",\n",
    "    \"date\": \"2025-05-10\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "  \"action\": \"GET_FLIGHTS\",\n",
    "  \"parameters\": {{\n",
    "    \"origin\": \"SFO\",\n",
    "    \"destination\": \"LAS\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Example 3 (generate final itinerary if we already have enough data):\n",
    "{{\n",
    "  \"action\": \"GENERATE_ITINERARY\",\n",
    "  \"parameters\": {{}}\n",
    "}}\n",
    "\n",
    "Please produce a corrected JSON object that fully addresses the problem, \n",
    "following this schema:\n",
    "{json.dumps(ACTION_SCHEMA, indent=2)}\n",
    "\n",
    "Return ONLY the corrected JSON, no additional commentary.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You fix incomplete travel planning steps into valid JSON.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": fix_prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        braintrust.current_span().log(error=f\"Error fixing step with LLM: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final itinerary generation\n",
    "\n",
    "Once the agent gathers enough information (e.g., weather and flight details), we expect a final itinerary to be generated. Below is a function that takes all the gathered data such as user preferences, API responses, budget details—and constructs a coherent multi-day travel plan. The result is a textual description of the trip, including recommended accommodations, daily activities, or tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_itinerary(context: Dict[str, Any]) -> Optional[str]:\n",
    "    with braintrust.start_span(name=\"generate_itinerary\"):\n",
    "        input_data = context[\"input_data\"]\n",
    "        weather_data = context[\"weather_data\"]\n",
    "        flight_data = context[\"flight_data\"]\n",
    "\n",
    "        prompt = (\n",
    "            f\"Based on the data, generate a travel itinerary.\\n\\n\"\n",
    "            f\"Origin: {input_data['origin']}\\n\"\n",
    "            f\"Destination: {input_data['destination']}\\n\"\n",
    "            f\"Start Date: {input_data['start_date']}\\n\"\n",
    "            f\"Budget: {input_data['budget']}\\n\"\n",
    "            f\"Preferences: {json.dumps(input_data['preferences'])}\\n\\n\"\n",
    "            f\"Weather Data: {json.dumps(weather_data, indent=2)}\\n\"\n",
    "            f\"Flight Data: {json.dumps(flight_data, indent=2)}\\n\\n\"\n",
    "            \"Create a day-by-day plan, mention booking recs, accommodations, etc. \"\n",
    "            \"Use a helpful, concise style.\"\n",
    "        )\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a thorough travel planner.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            braintrust.current_span().log(error=f\"Error generating itinerary: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the agent prompt for the \"next action\"\n",
    "\n",
    "We create a system prompt that summarizes known data (e.g., weather, flights) and reiterates the JSON schema requirements. This ensures the agent doesn’t redundantly fetch data and responds in valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agent_prompt(context: Dict[str, Any]) -> str:\n",
    "    input_data = context[\"input_data\"]\n",
    "    weather_data = context[\"weather_data\"]\n",
    "    flight_data = context[\"flight_data\"]\n",
    "\n",
    "    # System instructions encouraging valid JSON and minimal iteration\n",
    "    system_instructions = (\n",
    "        \"You are an autonomous travel planning assistant. \"\n",
    "        \"You MUST produce strictly valid JSON that matches the schema below. \"\n",
    "        \"If you need to GET_WEATHER, ensure you have both 'city' and 'date'. \"\n",
    "        \"If you need to GET_FLIGHTS, ensure you have 'origin' and 'destination'. \"\n",
    "        \"If the data is sufficient, proceed to GENERATE_ITINERARY. Avoid repeating an action if it's not needed. \"\n",
    "        \"Actions can be: GET_WEATHER, GET_FLIGHTS, GENERATE_ITINERARY, or DONE.\"\n",
    "    )\n",
    "\n",
    "    # Summarize current context\n",
    "    user_prompt = (\n",
    "        \"Current Travel Context:\\n\"\n",
    "        f\" - Origin: {input_data['origin']}\\n\"\n",
    "        f\" - Destination: {input_data['destination']}\\n\"\n",
    "        f\" - Start Date: {input_data['start_date']}\\n\"\n",
    "        f\" - Budget: {input_data['budget']}\\n\"\n",
    "        f\" - Preferences: {json.dumps(input_data['preferences'])}\\n\\n\"\n",
    "    )\n",
    "    if weather_data:\n",
    "        user_prompt += f\"Weather Data: {json.dumps(weather_data, indent=2)}\\n\\n\"\n",
    "    if flight_data:\n",
    "        user_prompt += f\"Flight Data: {json.dumps(flight_data, indent=2)}\\n\\n\"\n",
    "\n",
    "    # State final instructions for returning JSON\n",
    "    user_prompt += (\n",
    "        \"Next action? Respond only with valid JSON like:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"action\": \"GET_WEATHER\" | \"GET_FLIGHTS\" | \"GENERATE_ITINERARY\" | \"DONE\",\\n'\n",
    "        '  \"parameters\": { ... }\\n'\n",
    "        \"}\\n\\n\"\n",
    "        \"Here is the schema to follow:\\n\"\n",
    "        f\"{json.dumps(ACTION_SCHEMA, indent=2)}\"\n",
    "    )\n",
    "\n",
    "    return system_instructions + \"\\n\\n\" + user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main agent loop\n",
    "\n",
    "Here we build the core loop that powers our entire travel planning agent. It runs for a maximum number of iterations, doing the following each time:\n",
    "\n",
    "- **Prompt** the LLM for the next action.\n",
    "- **Validate** the JSON response against our schema.\n",
    "- **Judge** if the step is logical in context. If it fails, attempt to fix it.\n",
    "- **Execute** the step if valid (e.g., calling the mock weather/flight APIs).\n",
    "- If the agent indicates `GENERATE_ITINERARY`, produce the final itinerary and exit.\n",
    "\n",
    "By iterating until a final plan is reached (or until we exhaust retries), we create a semi-autonomous workflow that can correct missteps along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@braintrust.traced\n",
    "def agent_loop(client: openai.OpenAI, input_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Up to 10 iterations. If step is invalid or judge says 'N', we fix the step.\n",
    "    Returns a JSON string with final itinerary + iteration logs.\n",
    "    \"\"\"\n",
    "    context: Dict[str, Any] = {\n",
    "        \"input_data\": input_data,\n",
    "        \"weather_data\": {},\n",
    "        \"flight_data\": {},\n",
    "        \"decisions\": {},\n",
    "        \"itinerary\": None,\n",
    "        \"iteration_logs\": [],\n",
    "    }\n",
    "\n",
    "    max_iterations = 10\n",
    "    iteration = 0\n",
    "    current_action_json = \"\"\n",
    "    fix_attempts = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        with braintrust.start_span(\n",
    "            name=f\"travel_planning_iteration_{iteration}\"\n",
    "        ) as iter_span:\n",
    "            # 1) Acquire or fix the next action JSON\n",
    "            if not current_action_json:\n",
    "                llm_prompt = generate_agent_prompt(context)\n",
    "                try:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=\"gpt-4\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": llm_prompt},\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                    )\n",
    "                    current_action_json = resp.choices[0].message.content.strip()\n",
    "                except Exception as e:\n",
    "                    iter_span.log(error=f\"Error calling LLM for next action: {e}\")\n",
    "                    context[\"itinerary\"] = \"Failed to call LLM.\"\n",
    "                    break\n",
    "            else:\n",
    "                # We have an action JSON that was flagged, fix it\n",
    "                fixed = fix_step_with_cot(current_action_json)\n",
    "                if fixed:\n",
    "                    current_action_json = fixed\n",
    "                else:\n",
    "                    iter_span.log(error=\"Failed to fix step with LLM.\")\n",
    "                    context[\"itinerary\"] = \"Could not fix step.\"\n",
    "                    break\n",
    "\n",
    "            # 2) Validate\n",
    "            action_data = validate_json(current_action_json, ACTION_SCHEMA)\n",
    "            if not action_data:\n",
    "                fix_attempts += 1\n",
    "                if fix_attempts > 3:\n",
    "                    iter_span.log(\n",
    "                        error=\"Could not produce valid action after multiple fixes.\"\n",
    "                    )\n",
    "                    context[\"itinerary\"] = \"Invalid action, gave up.\"\n",
    "                    break\n",
    "                # re-try\n",
    "                continue\n",
    "\n",
    "            fix_attempts = 0\n",
    "            action = action_data[\"action\"]\n",
    "            parameters = action_data.get(\"parameters\", {})\n",
    "\n",
    "            # 3) Judge step - pass in some context so it doesn't automatically reject\n",
    "            step_desc = f\"Action: {action}, Params: {parameters}\"\n",
    "            is_ok, rationale = judge_step_with_cot(step_desc, context)\n",
    "\n",
    "            iteration_log = {\n",
    "                \"iteration\": iteration,\n",
    "                \"raw_llm_json\": current_action_json,\n",
    "                \"action\": action,\n",
    "                \"parameters\": parameters,\n",
    "                \"judge_decision\": \"Y\" if is_ok else \"N\",\n",
    "                \"judge_rationale\": rationale,\n",
    "            }\n",
    "            context[\"iteration_logs\"].append(iteration_log)\n",
    "\n",
    "            current_action_json = \"\"\n",
    "            if not is_ok:\n",
    "                iter_span.log(error=\"Judge flagged an error => fix next iteration.\")\n",
    "                current_action_json = iteration_log[\"raw_llm_json\"]\n",
    "                fix_attempts += 1\n",
    "                if fix_attempts > 3:\n",
    "                    context[\"itinerary\"] = \"Judge flagged error repeatedly. Gave up.\"\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 4) Execute action\n",
    "            if action == \"GET_WEATHER\":\n",
    "                city = parameters.get(\"city\")\n",
    "                date = parameters.get(\"date\")\n",
    "                if not city or not date:\n",
    "                    iter_span.log(\n",
    "                        error=\"Missing GET_WEATHER params => fix next iteration.\"\n",
    "                    )\n",
    "                    current_action_json = iteration_log[\"raw_llm_json\"]\n",
    "                    fix_attempts += 1\n",
    "                    continue\n",
    "                wdata = mock_weather_api(city, date)\n",
    "                context[\"weather_data\"][date] = wdata\n",
    "                iter_span.log(metadata={\"fetched_weather\": wdata})\n",
    "\n",
    "            elif action == \"GET_FLIGHTS\":\n",
    "                origin = parameters.get(\"origin\")\n",
    "                dest = parameters.get(\"destination\")\n",
    "                if not origin or not dest:\n",
    "                    iter_span.log(\n",
    "                        error=\"Missing GET_FLIGHTS params => fix next iteration.\"\n",
    "                    )\n",
    "                    current_action_json = iteration_log[\"raw_llm_json\"]\n",
    "                    fix_attempts += 1\n",
    "                    continue\n",
    "                fdata = mock_flight_api(origin, dest)\n",
    "                context[\"flight_data\"] = fdata\n",
    "                iter_span.log(metadata={\"fetched_flight\": fdata})\n",
    "\n",
    "            elif action == \"GENERATE_ITINERARY\":\n",
    "                itinerary = generate_final_itinerary(context)\n",
    "                context[\"itinerary\"] = itinerary or \"Failed to generate itinerary.\"\n",
    "                break\n",
    "\n",
    "            elif action == \"DONE\":\n",
    "                iter_span.log(metadata={\"status\": \"LLM indicated DONE\"})\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                iter_span.log(error=f\"Unknown action '{action}' => fix next iteration.\")\n",
    "                current_action_json = iteration_log[\"raw_llm_json\"]\n",
    "                fix_attempts += 1\n",
    "                if fix_attempts > 3:\n",
    "                    context[\"itinerary\"] = f\"Unknown action '{action}', gave up.\"\n",
    "                    break\n",
    "\n",
    "    final_data = {\n",
    "        \"final_itinerary\": context[\"itinerary\"],\n",
    "        \"iteration_logs\": context[\"iteration_logs\"],\n",
    "        \"input_data\": context[\"input_data\"],\n",
    "    }\n",
    "    return json.dumps(final_data, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation dataset\n",
    "\n",
    "Our workflow needs sample input data for testing. Below are 3 hardcoded test cases with different origins, destinations, budgets, and preferences. In a real application, you'll have a more extensive dataset with dozens if not hundreds of test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset() -> List[braintrust.EvalCase]:\n",
    "    return [\n",
    "        braintrust.EvalCase(\n",
    "            input={\n",
    "                \"origin\": \"NYC\",\n",
    "                \"destination\": \"Miami\",\n",
    "                \"start_date\": get_future_date(),\n",
    "                \"budget\": \"high\",\n",
    "                \"preferences\": {\"activity_level\": \"high\", \"foodie\": True},\n",
    "            },\n",
    "        ),\n",
    "        braintrust.EvalCase(\n",
    "            input={\n",
    "                \"origin\": \"SFO\",\n",
    "                \"destination\": \"Seattle\",\n",
    "                \"start_date\": get_future_date(),\n",
    "                \"budget\": \"medium\",\n",
    "                \"preferences\": {\"activity_level\": \"low\"},\n",
    "            },\n",
    "        ),\n",
    "        braintrust.EvalCase(\n",
    "            input={\n",
    "                \"origin\": \"IAH\",\n",
    "                \"destination\": \"Paris\",\n",
    "                \"start_date\": get_future_date(),\n",
    "                \"budget\": \"low\",\n",
    "                \"preferences\": {\"activity_level\": \"low\"},\n",
    "            },\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our scoring function\n",
    "\n",
    "We implement a custom LLM-based scorer that checks whether the final itinerary actually meets the user’s preferences. For instance, if the user wants a “high-activity trip,” but the final plan doesn’t suggest outdoor excursions or active elements, the scorer may judge that it’s missing key requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_itinerary = autoevals.LLMClassifier(\n",
    "    name=\"LLM Itinerary Judge\",\n",
    "    prompt_template=(\n",
    "        \"User preferences: {{input.preferences}}\\n\\n\"\n",
    "        \"Here is the final itinerary:\\n{{output}}\\n\\n\"\n",
    "        \"Does this itinerary meet the user preferences? (Y/N)\\n\"\n",
    "        \"Provide a short chain-of-thought, then say 'Final: Y' or 'Final: N'.\\n\"\n",
    "    ),\n",
    "    choice_scores={\"Y\": 1.0, \"N\": 0.0},\n",
    "    use_cot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the agent's end-to-end performance\n",
    "\n",
    "We define a `chain_task` that calls `agent_loop()`, then run an eval. Because the `agent_loop()` is wrapped with `@braintrust.traced`, each iteration and sub-step gets logged in the Braintrust UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_task(input_data: Dict[str, Any], hooks) -> str:\n",
    "    hooks.metadata[\"origin\"] = input_data[\"origin\"]\n",
    "    hooks.metadata[\"destination\"] = input_data[\"destination\"]\n",
    "    return agent_loop(client, input_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    braintrust.Eval(\n",
    "        name=\"TravelPlanner\",\n",
    "        data=dataset,\n",
    "        task=chain_task,\n",
    "        scores=[judge_itinerary],\n",
    "        experiment_name=\"end-to-end-eval\",\n",
    "        metadata={\"notes\": \"End to end evaluation of our travel planning agent\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![end-to-end](./assets/e2e.png)\n",
    "\n",
    "Starting with this top down approach is a generally recommended because it allows you spot where the chain might be breaking or not performing as expected. The Braintrust UI allows you to click intro any given component, view information such as the prompt or metadata. View each step can help decide which sub-component (weather fetch, flight fetch, judge) might need a closer look or some tuning. You would then run a separate evaluation on that component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the judge step in isolation\n",
    "\n",
    "After evaluating the end-to-end performance of an agent, you might want to take a closer look at a single component. For instance, if you notice that the agent frequently repeats certain actions when it shouldn’t, you might suspect the judge logic is misclassifying steps. To do this, we'll need to create a new experiement, a new dataset of test cases, and new scorers to evaluate specific components. \n",
    "\n",
    "<Callout type=\"info\">\n",
    "Depending on the complexity of your agent, or how you like to organize your work in Braintrust, you can choose to create a new project for this evaluation instead of adding it to the existing project like we do here.\n",
    "</Callout>\n",
    "\n",
    "For demonstration purposes, our approach is going to be simple. We create a judge-only dataset, along with a minimal `judge_eval_task` that passes the sample inputs though `judge_step_with_cot()` and then compares the response to our expected label using a heuristic scorer called `ExactMatch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_judge_eval() -> List[braintrust.EvalCase]:\n",
    "    \"\"\"\n",
    "    A small dataset focusing on testing judge_step_with_cot in isolation.\n",
    "    Each EvalCase includes a step_description and some minimal context.\n",
    "    We also specify an expected final decision (\"Y\" or \"N\") from the judge.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        braintrust.EvalCase(\n",
    "            input={\n",
    "                \"step_description\": \"Action: GET_WEATHER, Params: {'city': 'NYC', 'date': '2025-02-01'}\",\n",
    "                \"context_data\": {\n",
    "                    \"input_data\": {\n",
    "                        \"origin\": \"NYC\",\n",
    "                        \"destination\": \"Miami\",\n",
    "                        \"budget\": \"medium\",\n",
    "                        \"preferences\": {\"foodie\": True},\n",
    "                    },\n",
    "                    \"weather_data\": {},  # no existing weather => judge should say \"Y\"\n",
    "                    \"flight_data\": {},\n",
    "                },\n",
    "            },\n",
    "            expected=\"Y\",\n",
    "        ),\n",
    "        braintrust.EvalCase(\n",
    "            input={\n",
    "                \"step_description\": \"Action: GET_FLIGHTS, Params: {'origin': 'NYC', 'destination': 'Miami'}\",\n",
    "                \"context_data\": {\n",
    "                    \"input_data\": {\n",
    "                        \"origin\": \"NYC\",\n",
    "                        \"destination\": \"Miami\",\n",
    "                        \"budget\": \"low\",\n",
    "                        \"preferences\": {},\n",
    "                    },\n",
    "                    # Suppose we already have flight data => judge might say \"N\"\n",
    "                    \"weather_data\": {},\n",
    "                    \"flight_data\": {\n",
    "                        \"economy_price\": 300,\n",
    "                        \"business_price\": 1200,\n",
    "                        \"seats_left\": 10,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            expected=\"N\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def judge_eval_task(inputs: Dict[str, Any], hooks) -> str:\n",
    "    \"\"\"\n",
    "    A mini-task to evaluate judge_step_with_cot in isolation.\n",
    "    We pass the step_description and context_data to the function,\n",
    "    then return the judge's final decision (\"Y\" or \"N\").\n",
    "    \"\"\"\n",
    "    step_description = inputs[\"step_description\"]\n",
    "    context_data = inputs[\"context_data\"]\n",
    "\n",
    "    is_ok, _ = judge_step_with_cot(step_description, context_data)\n",
    "\n",
    "    return \"Y\" if is_ok else \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    braintrust.Eval(\n",
    "        name=\"TravelPlanner\",\n",
    "        data=dataset_judge_eval,\n",
    "        task=judge_eval_task,\n",
    "        scores=[autoevals.ExactMatch()],\n",
    "        experiment_name=\"judge-step-eval\",\n",
    "        metadata={\"notes\": \"Evaluating the judge_step_with_cot function in isolation.\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this evaluation, you can return to your orginal project in Braintrust. There you will see the new experiment for the judge step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![homepage](./assets/homepage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you click into the experiment, you can see all of the different evaluations and summaries. You can also click an individual row to view a full trace which includes the task function, metadata, and the scorers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![judge-eval](./assets/judge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s next:\n",
    "\n",
    "-[Read about](https://www.braintrust.dev/blog/evaluating-agents) on best practices for evaluating agents.\n",
    "\n",
    "-[Learn](https://www.braintrust.dev/blog/after-evals) what to do after you run an eval\n",
    "\n",
    "-Try out another [agent cookbook](https://www.braintrust.dev/docs/cookbook/recipes/APIAgent-Py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
