{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting robust LLM-as-a-judge scorers\n",
    "\n",
    "Let's say you're working on a customer service bot and trying to evaluate the quality of its outputs. Consider a question like \"What is your return policy?\". If the ground truth answer is \"You can return\n",
    "items within 30 days of purchase\", and your bot generates the answer \"You can return items within 30 days\", a direct comparison of the two strings would say they're not equal,\n",
    "but an LLM-as-a-judge scorer can understand that they are in fact the same.\n",
    "\n",
    "## Installing dependencies\n",
    "\n",
    "Let's install a few basic dependencies. We'll use the CoQA dataset (via DuckDB), Braintrust for evals, and OpenAI's LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autoevals in ./.venv/lib/python3.12/site-packages (0.0.95)\n",
      "Requirement already satisfied: chevron in ./.venv/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: duckdb in ./.venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: braintrust in ./.venv/lib/python3.12/site-packages (0.0.162)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: levenshtein in ./.venv/lib/python3.12/site-packages (from autoevals) (0.26.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from autoevals) (6.0.2)\n",
      "Requirement already satisfied: braintrust-core==0.0.54 in ./.venv/lib/python3.12/site-packages (from autoevals) (0.0.54)\n",
      "Requirement already satisfied: jsonschema in ./.venv/lib/python3.12/site-packages (from autoevals) (4.23.0)\n",
      "Requirement already satisfied: GitPython in ./.venv/lib/python3.12/site-packages (from braintrust) (3.1.43)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from braintrust) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from braintrust) (4.66.5)\n",
      "Requirement already satisfied: exceptiongroup==1.2.0 in ./.venv/lib/python3.12/site-packages (from braintrust) (1.2.0)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from braintrust) (1.0.1)\n",
      "Requirement already satisfied: sseclient-py in ./.venv/lib/python3.12/site-packages (from braintrust) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.12/site-packages (from GitPython->braintrust) (4.0.11)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema->autoevals) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema->autoevals) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema->autoevals) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema->autoevals) (0.20.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./.venv/lib/python3.12/site-packages (from levenshtein->autoevals) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->braintrust) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->braintrust) (2.2.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->GitPython->braintrust) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autoevals chevron duckdb braintrust openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur/projects/braintrust/cookbook/content/examples/Crafting-LLM-as-a-Judge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import braintrust\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    AsyncOpenAI(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",  # Caches requests to OpenAI\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Question:\n",
      "What color was Cotton?\n",
      "\n",
      "Answer:\n",
      "white\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\":memory:\")\n",
    "full_result = con.query(\"\"\"\n",
    "    SELECT * FROM 'hf://datasets/stanfordnlp/coqa/data/validation-00000-of-00001.parquet'\n",
    "        LIMIT 20\n",
    "\"\"\").fetchall()\n",
    "\n",
    "first_result = full_result[0]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(first_result[1])\n",
    "\n",
    "print(\"\\nQuestion:\")\n",
    "print(first_result[2][0])\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(first_result[3][\"input_text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains a series of passages, each with a number of questions and answers. Let's flatten this into a list of `(passage, question, answer)` tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnswer:\n",
    "    passage: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    generated_answer: str\n",
    "\n",
    "\n",
    "qa_pairs = [\n",
    "    QuestionAnswer(\n",
    "        passage=r[1], question=question, generated_answer=r[3][\"input_text\"][i], expected_answer=r[3][\"input_text\"][i]\n",
    "    )\n",
    "    for r in full_result\n",
    "    for (i, question) in enumerate(r[2])\n",
    "]\n",
    "\n",
    "print(len(qa_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hallucinations\n",
    "\n",
    "In addition to the reference answers, let's add some halucinated answers. We'll consider a halucination to be a random answer from another question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Question:\n",
      "What did the other cats do when Cotton emerged from the bucket of water?\n",
      "\n",
      "Expected Answer:\n",
      "licked her face\n",
      "\n",
      "Generated Answer:\n",
      "The other cats watched in surprise and amusement as Cotton emerged from the bucket of water.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "async def hallucinate_answer(qa):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "Answer the following question in 1 sentence. Make sure to always answer it confidently,\n",
    "even if you don't know the answer. Do not use words like \"perhaps\", \"likely\", \"maybe\", etc.\n",
    "Do not admit that you cannot or do not know the answer.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": qa.question},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "hallucinated_answers = await asyncio.gather(*[hallucinate_answer(qa) for qa in qa_pairs])\n",
    "\n",
    "\n",
    "hallucinations = [\n",
    "    QuestionAnswer(\n",
    "        passage=qa.passage,\n",
    "        question=qa.question,\n",
    "        expected_answer=qa.expected_answer,\n",
    "        generated_answer=hallucination,\n",
    "    )\n",
    "    for (qa, hallucination) in zip(qa_pairs, hallucinated_answers)\n",
    "]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(hallucinations[10].passage)\n",
    "print(\"\\nQuestion:\")\n",
    "print(hallucinations[10].question)\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(hallucinations[10].expected_answer)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(hallucinations[10].generated_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the evaluators\n",
    "\n",
    "Let's start by initializing the OpenAI client. We'll wrap it with Braintrust to enable logging and caching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #1: Numeric rater\n",
    "\n",
    "A common initial intuition when creating an LLM-as-a-judge is asking the LLM to rate the answer on a scale of 1 to 10. Let's see how this works for our task.\n",
    "\n",
    "We'll use a modified version of the [Factuality](https://github.com/braintrustdata/autoevals/blob/main/templates/factuality.yaml) template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What did the other cats do when Cotton emerged from the bucket of water? On a hallucinated answer: The other cats watched in surprise and amusement as Cotton emerged from the bucket of water.\n",
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import chevron\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {{{input}}}\n",
    "************\n",
    "[Expert]: {{{expected}}}\n",
    "************\n",
    "[Submission]: {{{output}}}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "Rate the submission on a scale of 1 to 10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": chevron.render(PROMPT, data=dict(input=input, output=output, expected=expected)),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(await numeric_rater(qa_pairs[10].question, qa_pairs[10].generated_answer, qa_pairs[10].expected_answer))\n",
    "\n",
    "print(hallucinations[10].question, \"On a hallucinated answer:\", hallucinations[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question, hallucinations[10].generated_answer, hallucinations[10].expected_answer\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (data): 297it [00:00, 88605.75it/s]\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (tasks): 100%|██████████| 297/297 [00:52<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "76.99% 'normalized_diff' score\n",
      "\n",
      "1.84s duration\n",
      "1.72s llm_duration\n",
      "177.31tok prompt_tokens\n",
      "5tok completion_tokens\n",
      "182.31tok total_tokens\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for Numeric rater at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from braintrust import Eval\n",
    "\n",
    "\n",
    "def data():\n",
    "    # for pair in qa_pairs[:20]:\n",
    "    #     yield dict(input=dict(asdict(pair)), expected=1, metadata=dict(hallucination=False))\n",
    "    for pair in hallucinations:\n",
    "        yield dict(input=dict(asdict(pair)), expected=0, metadata=dict(hallucination=True))\n",
    "\n",
    "\n",
    "async def task(input):\n",
    "    return await numeric_rater(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def normalized_diff(output, expected):\n",
    "    return 1 - abs(output - expected)\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding reasoning\n",
    "\n",
    "Next, let's tweak the prompt to get the LLM to also reason about its rating. This method is called [Chain of Thought Reasoning](https://en.wikipedia.org/wiki/Chain_of_thought_reasoning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What did the other cats do when Cotton emerged from the bucket of water? On a hallucinated answer: The other cats watched in surprise and amusement as Cotton emerged from the bucket of water.\n",
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": chevron.render(PROMPT, data=dict(input=input, output=output, expected=expected)),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"title\": \"Reasoning\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(await numeric_rater(qa_pairs[10].question, qa_pairs[10].generated_answer, qa_pairs[10].expected_answer))\n",
    "\n",
    "print(hallucinations[10].question, \"On a hallucinated answer:\", hallucinations[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question, hallucinations[10].generated_answer, hallucinations[10].expected_answer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater with reasoning is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (data): 297it [00:00, 60547.70it/s]\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (tasks): 100%|██████████| 297/297 [01:24<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Numeric rater with reasoning compared to Numeric rater:\n",
      "77.10% (+00.11%) 'normalized_diff' score\t(29 improvements, 33 regressions)\n",
      "\n",
      "2.96s (+111.88%) 'duration'         \t(10 improvements, 267 regressions)\n",
      "2.88s (+115.66%) 'llm_duration'     \t(9 improvements, 268 regressions)\n",
      "216.31tok (+3900.00%) 'prompt_tokens'    \t(0 improvements, 297 regressions)\n",
      "137.54tok (+13253.87%) 'completion_tokens'\t(0 improvements, 297 regressions)\n",
      "353.85tok (+17153.87%) 'total_tokens'     \t(0 improvements, 297 regressions)\n",
      "0.00$ (+00.14%) 'estimated_cost'   \t(0 improvements, 277 regressions)\n",
      "\n",
      "See results for Numeric rater with reasoning at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater with reasoning\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, it actually did worse!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #2: Classifier\n",
    "\n",
    "Next, let's try using classification. The intuition here is that by giving the model specific criteria to rate, it will generate a more accurate assessment. For illustration purposes,\n",
    "we've written out how this works, but you can access this scorer directly via the `Factuality` scorer in autoevals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1\n",
      "What did the other cats do when Cotton emerged from the bucket of water? On a hallucinated answer: The other cats watched in surprise and amusement as Cotton emerged from the bucket of water.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {{{input}}}\n",
    "************\n",
    "[Expert]: {{{expected}}}\n",
    "************\n",
    "[Submission]: {{{output}}}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "# Since we're testing for hallucinations, penalize (B) as much as (D).\n",
    "CHOICE_SCORES = {\n",
    "    \"A\": 0.5,\n",
    "    \"B\": 0,\n",
    "    \"C\": 1,\n",
    "    \"D\": 0,\n",
    "    \"E\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def classifier(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": chevron.render(PROMPT, data=dict(input=input, output=output, expected=expected)),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Call this function to select a choice.\",\n",
    "                    \"parameters\": {\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"choice\": {\n",
    "                                \"description\": \"The choice\",\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"reasons\", \"choice\"],\n",
    "                        \"type\": \"object\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    choice = arguments[\"choice\"]\n",
    "    return CHOICE_SCORES[choice] if choice in CHOICE_SCORES else None\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(await classifier(qa_pairs[10].question, qa_pairs[10].generated_answer, qa_pairs[10].expected_answer))\n",
    "\n",
    "print(hallucinations[10].question, \"On a hallucinated answer:\", hallucinations[10].generated_answer)\n",
    "print(\n",
    "    await classifier(\n",
    "        hallucinations[10].question, hallucinations[10].generated_answer, hallucinations[10].expected_answer\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Classifier is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n",
      "LLM-as-a-judge [experiment_name=Classifier] (data): 297it [00:00, 65377.78it/s]\n",
      "LLM-as-a-judge [experiment_name=Classifier] (tasks): 100%|██████████| 297/297 [01:44<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "92.59% 'normalized_diff' score\n",
      "\n",
      "3.71s duration\n",
      "3.57s llm_duration\n",
      "394.31tok prompt_tokens\n",
      "164.23tok completion_tokens\n",
      "558.54tok total_tokens\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for Classifier at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def task(input):\n",
    "    return await classifier(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Classifier\",\n",
    "    max_concurrency=10,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
