{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quNDJhiCJ-AI"
      },
      "source": [
        "# BrainTrust Text2SQL Tutorial\n",
        "\n",
        "This tutorial will teach you how to create an application that converts natural language questions into SQL queries, and then evaluating how well\n",
        "the queries work. We'll even make an improvement to the prompts, and evaluate the impact! By the time you finish this tutorial, you should be ready\n",
        "to run your own experiments.\n",
        "\n",
        "Before starting, please make sure that you have a BrainTrust account. If you do not, please [sign up](https://braintrustdata.com).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tMYg0jyKelb"
      },
      "source": [
        "## Setting up the environment\n",
        "\n",
        "Before we get started, please enter your OpenAI Key here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QlurLBK2LC"
      },
      "source": [
        "The next few commands will install some libraries and include some helper code for the text2sql application. Feel free to copy/paste/tweak/reuse this code in your own tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGuPB2SqUWdZ"
      },
      "outputs": [],
      "source": [
        "!pip install braintrust duckdb datasets openai pyarrow python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaKzmZuST7CU"
      },
      "outputs": [],
      "source": [
        "# Import libraries + define helper functions\n",
        "\n",
        "import duckdb\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from Levenshtein import distance\n",
        "import openai\n",
        "import os\n",
        "import pyarrow as pa\n",
        "import time\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "NUM_TEST_EXAMPLES = 10\n",
        "\n",
        "\n",
        "# Define some helper functions\n",
        "def get_table(table):\n",
        "    rows = [{h: row[i] for (i, h) in enumerate(table[\"header\"])} for row in table[\"rows\"]]\n",
        "\n",
        "    return pa.Table.from_pylist(rows)\n",
        "\n",
        "\n",
        "AGG_OPS = [None, \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\n",
        "COND_OPS = [\" ILIKE \", \">\", \"<\"]  # , \"OP\"]\n",
        "\n",
        "\n",
        "def esc_fn(s):\n",
        "    return f'''\"{s.replace('\"', '\"\"')}\"'''\n",
        "\n",
        "\n",
        "def esc_value(s):\n",
        "    if isinstance(s, str):\n",
        "        return s.replace(\"'\", \"''\")\n",
        "    else:\n",
        "        return s\n",
        "\n",
        "\n",
        "def codegen_query(query):\n",
        "    header = query[\"table\"][\"header\"]\n",
        "\n",
        "    projection = f\"{esc_fn(header[query['sql']['sel']])}\"\n",
        "\n",
        "    agg_op = AGG_OPS[query[\"sql\"][\"agg\"]]\n",
        "    if agg_op is not None:\n",
        "        projection = f\"{agg_op}({projection})\"\n",
        "\n",
        "    conds = query[\"sql\"][\"conds\"]\n",
        "\n",
        "    filters = \" and \".join(\n",
        "        [\n",
        "            f\"\"\"{esc_fn(header[field])}{COND_OPS[cond]}'{esc_value(value)}'\"\"\"\n",
        "            for (field, cond, value) in zip(conds[\"column_index\"], conds[\"operator_index\"], conds[\"condition\"])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if filters:\n",
        "        filters = f\" WHERE {filters}\"\n",
        "\n",
        "    return f'SELECT {projection} FROM \"table\"{filters}'\n",
        "\n",
        "\n",
        "OPENAI_CACHE = None\n",
        "\n",
        "\n",
        "def openai_req(Completion=openai.Completion, **kwargs):\n",
        "    global OPENAI_CACHE\n",
        "    if OPENAI_CACHE is None:\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        OPENAI_CACHE = duckdb.connect(database=\"data/oai_cache.duckdb\")\n",
        "        OPENAI_CACHE.query(\"CREATE TABLE IF NOT EXISTS cache (params text, response text)\")\n",
        "\n",
        "    param_key = json.dumps(kwargs)\n",
        "    resp = OPENAI_CACHE.execute(\"\"\"SELECT response FROM \"cache\" WHERE params=?\"\"\", [param_key]).fetchone()\n",
        "    if resp and resp[0]:\n",
        "        return json.loads(resp[0])\n",
        "\n",
        "    for i in range(5):\n",
        "        try:\n",
        "            resp = Completion.create(**kwargs).to_dict()\n",
        "            break\n",
        "        except openai.error.RateLimitError:\n",
        "            print(\"Rate limited... Sleeping for 30 seconds\")\n",
        "            time.sleep(30)\n",
        "\n",
        "    OPENAI_CACHE.execute(\"\"\"INSERT INTO \"cache\" VALUES (?, ?)\"\"\", [param_key, json.dumps(resp)])\n",
        "\n",
        "    return resp\n",
        "\n",
        "\n",
        "def green(s):\n",
        "    return \"\\x1b[32m\" + s + \"\\x1b[0m\"\n",
        "\n",
        "\n",
        "def run_query(sql, table_record):\n",
        "    table = get_table(table_record)  # noqa\n",
        "    rel_from_arrow = duckdb.arrow(table)\n",
        "\n",
        "    result = rel_from_arrow.query(\"table\", sql).fetchone()\n",
        "    if result and len(result) > 0:\n",
        "        return result[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "def score(r1, r2):\n",
        "    if r1 is None and r2 is None:\n",
        "        return 1\n",
        "    if r1 is None or r2 is None:\n",
        "        return 0\n",
        "\n",
        "    r1, r2 = str(r1), str(r2)\n",
        "\n",
        "    total_len = max(len(r1), len(r2))\n",
        "    return 1 - distance(r1, r2) / total_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fajc2XybK_gt"
      },
      "source": [
        "## Exploring the data\n",
        "\n",
        "In this section, we'll take a look at the dataset and ground truth text/sql pairs to better understand the problem and data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_OdDHOhUk0K"
      },
      "outputs": [],
      "source": [
        "# Initialize data from WikiSQL\n",
        "data = list(load_dataset(\"wikisql\")[\"test\"])\n",
        "idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqCA2b4lVNEo"
      },
      "outputs": [],
      "source": [
        "data[idx][\"question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCZ2D9_KWAuE"
      },
      "outputs": [],
      "source": [
        "table = get_table(data[idx][\"table\"])\n",
        "duckdb.arrow(table).query(\"table\", 'SELECT * FROM \"table\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GDsB2taWDol"
      },
      "outputs": [],
      "source": [
        "gt_sql = codegen_query(data[idx])\n",
        "print(gt_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkT_Ykt1WWvq"
      },
      "outputs": [],
      "source": [
        "duckdb.arrow(table).query(\"table\", gt_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKzPxqhNLMgV"
      },
      "source": [
        "## Running your first experiment\n",
        "\n",
        "In this section, we'll create our first experiment and analyze the results in BrainTrust.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv-jKnOGWa1T"
      },
      "outputs": [],
      "source": [
        "# First attempt: provide the question and columns\n",
        "def text2sql(query):\n",
        "    table = query[\"table\"]\n",
        "    meta = \"\\n\".join(f'\"{h}\"' for h in table[\"header\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Print a SQL query (over a table named \"table\" quoted with double quotes) that answers the question below.\n",
        "\n",
        "You have the following columns:\n",
        "{meta}\n",
        "\n",
        "The format should be\n",
        "Question: the question to ask\n",
        "SQL: the SQL to generate\n",
        "\n",
        "Question: {query['question']}\n",
        "SQL: \"\"\"\n",
        "    resp = openai_req(model=\"text-davinci-003\", prompt=prompt, max_tokens=1024)\n",
        "\n",
        "    return (\n",
        "        prompt,\n",
        "        resp,\n",
        "        resp[\"choices\"][0][\"text\"].rstrip(\";\") if len(resp[\"choices\"]) > 0 else None,\n",
        "    )\n",
        "\n",
        "\n",
        "prompt, resp, _ = text2sql(data[idx])\n",
        "print(prompt + green(resp[\"choices\"][0][\"text\"]))\n",
        "\n",
        "output_sql = resp[\"choices\"][0][\"text\"].rstrip(\";\")\n",
        "duckdb.arrow(table).query(\"table\", output_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1z_zEhOLYfy"
      },
      "source": [
        "Now, that we've tested the prompt on an example, let's run it on several test examples, and log the results in BrainTrust.\n",
        "\n",
        "First, we'll initialize a new experiment in a **project** named `text2sql-tutorial`. A project allows you to group together **experiments** that contain similar inputs and outputs, and compare results across them. In this tutorial, we'll ultimately create two experiments.\n",
        "\n",
        "When you run `braintrust.init`, BrainTrust will automatically create the project if it does not exist. If there is an experiment in that project with the name you selected, BrainTrust will automatically create a new one (e.g. `with-columns-001`). Experiments are meant to be short-lived, one-time analyses.\n",
        "\n",
        "_NOTE: If you did not specify a valid BRAINTRUST_API_KEY, you may be prompted to enter a token here for authentication._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fADUk1JpdEz5"
      },
      "outputs": [],
      "source": [
        "import braintrust\n",
        "\n",
        "bt = braintrust.init(project=\"text2sql-tutorial\", experiment=\"with-columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo_GjWwSuYr9"
      },
      "source": [
        "This function runs the experiment in a loop (over `NUM_TEST_EXAMPLES` test cases), and logs a bunch of data to BrainTrust with the `bt.log` command. Each of these arguments can contain arbitrary JSON data which you can later slice & dice to understand how changes in your prompts & models affect results for different subsets of data. Here's a quick explanation of each argument:\n",
        "\n",
        "- `inputs`: the arguments that uniquely define a test case. Later on, BrainTrust will use the `inputs` to know whether two test casess are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `inputs` should be identical.\n",
        "- `output`: the output of your application, including post-processing, that allows you to determine whether the result is correct or not. For example, in the text2sql app, the `output` is the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid SQL queries to answer a single question.\n",
        "- `expected`: the ground truth value that you'd compare to `output` to determine if your `output` value is correct or not. BrainTrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.\n",
        "- `scores`: one or more numeric values (between 0 and 1) that tell you how accurate the outputs are compared to what you expect. In this example, the `answer` score is the definitive measure of correctness, but the `query` score helps you measure how far off the queries are. For example, if there's a small syntax error, you might have an `answer` score of 0 but a high `query` score. You can use these scores to help you sort, filter, and compare experiments.\n",
        "- `metadata`: a JSON dictionary of additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. In this example, the `id` is particularly helpful because it allows us to quickly test examples in the notebook (later on below) by setting `idx`. `output_sql` allows us to look at failure cases and understand how far off the SQL was from what we expected. `prompt` makes it easy to compare the actual prompts between examples and experiments. And so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGf9KsC4dcsp"
      },
      "outputs": [],
      "source": [
        "def run_experiment(text2sql_fn):\n",
        "    for i in range(NUM_TEST_EXAMPLES):\n",
        "        print(f\"{i+1}/{NUM_TEST_EXAMPLES}\\r\")\n",
        "        query = data[i]\n",
        "        gt_query = codegen_query(query)\n",
        "        gt_answer = run_query(gt_query, query[\"table\"])\n",
        "\n",
        "        prompt, _, sql = text2sql_fn(query)\n",
        "        try:\n",
        "            answer = run_query(sql, query[\"table\"])\n",
        "        except Exception as e:\n",
        "            answer = f\"FAILED: {e}\"\n",
        "\n",
        "        bt.log(\n",
        "            inputs={\"question\": query[\"question\"]},\n",
        "            output=answer,\n",
        "            expected=gt_answer,\n",
        "            scores={\n",
        "                \"answer\": score(gt_answer, answer),\n",
        "                \"query\": score(gt_query, sql),\n",
        "            },\n",
        "            metadata={\n",
        "                \"prompt\": prompt,\n",
        "                \"gt_sql\": gt_query,\n",
        "                \"output_sql\": sql,\n",
        "                \"id\": i,\n",
        "            },\n",
        "        )\n",
        "    print(bt.summarize())\n",
        "\n",
        "\n",
        "run_experiment(text2sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IJNQtamRDH"
      },
      "source": [
        "Take a look at the failures. Feel free to explore individual examples, filter down to low `answer` scores, etc. You should notice that `id=4` is one of the failures. Let's debug it and see if we can improve the prompt.\n",
        "\n",
        "## Debugging a failure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skiosYHPmsP8"
      },
      "outputs": [],
      "source": [
        "idx = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlqC41AjnTHa"
      },
      "source": [
        "Let's start by looking at the ground truth:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN10oMOWnMaw"
      },
      "outputs": [],
      "source": [
        "print(data[idx][\"question\"])\n",
        "\n",
        "table = get_table(data[idx][\"table\"])\n",
        "print(duckdb.arrow(table).query(\"table\", 'SELECT * FROM \"table\"'))\n",
        "\n",
        "gt_sql = codegen_query(data[idx])\n",
        "print(gt_sql)\n",
        "\n",
        "print(duckdb.arrow(table).query(\"table\", gt_sql))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA-1WxsrnilJ"
      },
      "source": [
        "And then what the model spits out:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRmflA8lnkJy"
      },
      "outputs": [],
      "source": [
        "prompt, resp, _ = text2sql(data[idx])\n",
        "print(prompt + green(resp[\"choices\"][0][\"text\"]))\n",
        "\n",
        "output_sql = resp[\"choices\"][0][\"text\"].rstrip(\";\")\n",
        "duckdb.arrow(table).query(\"table\", output_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXta8DuJnnuC"
      },
      "source": [
        "Hmm, if only the model knew that `'Assen'` is a `Circuit`, not a `Round`. Let's provide some sample data for each column:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpvv-xPAmQDQ"
      },
      "outputs": [],
      "source": [
        "# Second attempt: provide the question, columns, and sample data\n",
        "def text2sql_data(query):\n",
        "    table = query[\"table\"]\n",
        "    rows = [{h: row[i] for (i, h) in enumerate(table[\"header\"])} for row in table[\"rows\"]]\n",
        "    meta = \"\\n\".join(f'\"{h}\": {[row[h] for row in rows[:10]]}' for h in table[\"header\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Print a SQL query (over a table named \"table\" quoted with double quotes) that answers the question.\n",
        "\n",
        "You have the following columns (each with some sample data). The column\n",
        "names may have spaces in them which you should escape with double quotes:\n",
        "\n",
        "{meta}\n",
        "\n",
        "The format should be\n",
        "Question: the question to ask\n",
        "SQL: the SQL to generate\n",
        "\n",
        "Question: {query['question']}\n",
        "SQL: \"\"\"\n",
        "    resp = openai_req(model=\"text-davinci-003\", prompt=prompt, max_tokens=1024)\n",
        "\n",
        "    return (\n",
        "        prompt,\n",
        "        resp,\n",
        "        resp[\"choices\"][0][\"text\"].rstrip(\";\") if len(resp[\"choices\"]) > 0 else None,\n",
        "    )\n",
        "\n",
        "\n",
        "prompt, resp, _ = text2sql_data(data[idx])\n",
        "print(prompt + green(resp[\"choices\"][0][\"text\"]))\n",
        "\n",
        "output_sql = resp[\"choices\"][0][\"text\"].rstrip(\";\")\n",
        "duckdb.arrow(table).query(\"table\", output_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6-HGKGWfOI"
      },
      "source": [
        "Ok great! Now let's re-run the loop with this new version of the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7o5OSZBWzjO"
      },
      "outputs": [],
      "source": [
        "bt = braintrust.init(project=\"text2sql-tutorial\", experiment=\"with-data\")\n",
        "run_experiment(text2sql_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zm138QEk-db"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "Congrats ðŸŽ‰. You've run your first couple of experiments. Now, return back to the tutorial docs to proceed to the next step where we'll analyze the experiments.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
