{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Braintrust to evaluate the precision and recall of an emotion classifier\n",
    "\n",
    "In this cookbook, we'll learn how to evaluate a custom LLM classifier in Braintrust using custom scoring functions that measure precision and recall. For this cookbook, We'll use the [go_emotions dataset](https://huggingface.co/datasets/google-research-datasets/go_emotions), which contains Reddit comments labeled with 28 different emotions. What makes this dataset particularly interesting is that each comment can be labeled with multiple emotions - for example, a single message might express both \"excitement\" and \"anger\".\n",
    "\n",
    "We'll build two classifiers - a random baseline and an LLM-based approach using OpenAI's GPT-4o. By comparing their performance using custom scorers, we'll demonstrate how to effectively measure then improve your LLM's accuracy on complex classification tasks.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "You will need to make sure you have a [Braintrust account](https://www.braintrust.dev/signup) and an [OpenAI account](https://platform.openai.com/). Once you have both accounts, get a a [BRAINTRUST_API_KEY](https://www.braintrust.dev/app/settings?subroute=api-keys) and an [OPENAI_API_KEY](https://platform.openai.com/account/api-keys). Lastly, add your `BRAINTRUST_API_KEY` along with `OPENAI_API_KEY` to your Python environment, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "export BRAINTRUST_API_KEY=\"YOUR_BRAINTRUST_API_KEY\"\n",
    "export OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install braintrust openai datasets autoevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import random\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from typing import List, Literal, Union, Set\n",
    "\n",
    "import openai\n",
    "import braintrust\n",
    "from datasets import load_dataset\n",
    "import autoevals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY UNCOMMENT IF YOU WANT TO HARDCODE YOUR API KEYS\n",
    "#os.environ[\"BRAINTRUST_API_KEY\"] = \"YOUR_BRAINTRUST_API_KEY\"\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "openai_client = braintrust.wrap_openai(\n",
    "    openai.OpenAI()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Callout type=\"info\">\n",
    " Best practice is to export your API key as an environment variable. However, to make it easier to follow along with this cookbook, you can also hardcode it into the code above.\n",
    "</Callout>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Type Definitions and Data Models\n",
    "\n",
    "In this cookbook, we're working with a fixed set of possible emotions, but any given comment might express multiple emotions simultaneously. We need to ensure our classifier only outputs valid emotions from our predefined set. To handle these requirements, we'll build a type-safe system using Python's type hints and Pydantic models. This approach catches invalid emotions at compile time, validates classifier outputs at runtime, and integrates seamlessly with OpenAI's structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\",\n",
    "    \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\",\n",
    "    \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "EmotionType = Literal[tuple(EMOTIONS)]\n",
    "\n",
    "EmotionClassification = create_model(\n",
    "    'EmotionClassification',\n",
    "    emotions=(List[EmotionType], ...)\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(limit: int = 100):\n",
    "    ds = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")\n",
    "    for i, item in list(enumerate(ds[\"train\"]))[:limit]:\n",
    "        actual_emotions = [emotion for emotion in EMOTIONS if item.get(emotion, 0) == 1]\n",
    "        yield {\n",
    "            \"input\": item[\"text\"],\n",
    "            \"expected\": actual_emotions,\n",
    "            \"metadata\": {\"subreddit\": item[\"subreddit\"], \"author\": item[\"author\"]},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Classifiers\n",
    "\n",
    "We implement two different approaches to emotion classification:\n",
    "\n",
    "1. An LLM-based classifier using GPT-4o that uses [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) to ensure valid emotion labels.\n",
    "\n",
    "2. A random classifier that assigns 1-3 emotions randomly from our predefined list. This random baseline helps us verify that our LLM classifier performs meaningfully better than chance predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_classifier(text: str) -> EmotionClassification:\n",
    "    prompt = (\n",
    "        f\"Analyze the emotional content in this text and STRICTLY classify it using ONLY the following emotion labels:\\n\"\n",
    "        f\"{', '.join(EMOTIONS)}\\n\\n\"\n",
    "        f\"IMPORTANT: You must ONLY use emotions from the above list. Do not use any other emotion labels and DO NOT repeat emotions.\\n\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Respond with a JSON object containing:\\n\"\n",
    "        f\"- emotions: array of emotions from the provided list only\\n\"\n",
    "        f\"Remember: Only use emotions from the provided list. If you see an emotion that isn't in the list, map it to the closest matching emotion from the list.\"\n",
    "    )\n",
    "\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        response_format= EmotionClassification\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    return EmotionClassification.model_validate_json(result)\n",
    "\n",
    "def random_classifier(text: str) -> EmotionClassification:\n",
    "    num_emotions = random.randint(1, 3)\n",
    "    selected_emotions = random.sample(EMOTIONS, num_emotions)\n",
    "    return EmotionClassification(\n",
    "        emotions=selected_emotions,\n",
    "        confidence=random.random(),\n",
    "        rationale=\"Random selection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing evaluation metrics \n",
    "\n",
    "For multi-label classification tasks like emotion detection, we need specialized metrics that can handle multiple correct answers. We implement three key metrics:\n",
    "\n",
    "`Precision`: Measures how many of the identified emotions are actually correct. For example, if our system predicts \"joy\" and \"anger\" for a comment that only expresses \"joy\", we achieve a precision of 0.5 because only \"joy\" is correct.\n",
    "   - High precision means we rarely predict emotions that aren't present\n",
    "   - Important for applications where false positives are costly\n",
    "\n",
    "`Recall`: Measures whether we captured all the emotions that were truly present in the Reddit comments. For instance, if a comment expresses both \"sadness\" and \"fear\" but we only identify \"sadness,\" we get a recall of 0.5. This is fundamentally different from precision because it tells us if we missed emotions that should have been identified.\n",
    "   - High recall means we rarely miss emotions that are present\n",
    "   - Important for applications where false negatives are costly\n",
    "\n",
    "`F1 Score`: The F1 Score balances precision and recall by calculating their harmonic mean, which is necessary because optimizing for one often comes at the expense of the other. If our LLM achieves high precision, it might be too conservative and miss emotions (low recall). Conversely, if it achieves high recall, it might overidentify emotions (low precision). The F1 score gives us a single metric to evaluate our LLM's overall performance in identifying emotions in Reddit comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_precision(output: EmotionClassification, expected: List[EmotionType]) -> float:\n",
    "    expected_set = set(expected)\n",
    "    output_set = set(output.emotions)\n",
    "    true_positives = len(output_set & expected_set)\n",
    "    false_positives = len(output_set - expected_set)\n",
    "    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 1.0\n",
    "\n",
    "def emotion_recall(output: EmotionClassification, expected: List[EmotionType]) -> float:\n",
    "    expected_set = set(expected)\n",
    "    output_set = set(output.emotions)\n",
    "    true_positives = len(output_set & expected_set)\n",
    "    false_negatives = len(expected_set - output_set)\n",
    "    return true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 1.0\n",
    "\n",
    "def emotion_f1(output: EmotionClassification, expected: List[EmotionType]) -> float:\n",
    "    prec = emotion_precision(output, expected)\n",
    "    rec = emotion_recall(output, expected)\n",
    "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "Finally, let's set up our evaluation pipeline using Braintrust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(num_samples: int = 100):\n",
    "    # Create a list of data to ensure it's not empty\n",
    "    data = list(load_data(limit=num_samples))\n",
    "    \n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook\",\n",
    "        data=lambda: data,  # Return the preloaded data\n",
    "        task=random_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"random\"},\n",
    "        experiment_name=\"random-classifier\",\n",
    "    )\n",
    "\n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook\",\n",
    "        data=lambda: data,  # Return the preloaded data\n",
    "        task=llm_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"llm\", \"model\": \"gpt-4o\"},\n",
    "        experiment_name=\"llm-classifier\",\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluations(num_samples=100) # Adjust the number of samples as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Once you run the evaluations, you'll see the results in your Braintrust dashboard. The LLM classifier should significantly outperform the random baseline across all metrics.\n",
    "\n",
    "![results.png](assets/results.png)\n",
    "\n",
    "Key features to examine:\n",
    "- Compare precision and recall scores between our runs\n",
    "- Look at specific examples where the LLM fails \n",
    "- Analyze cases where multiple emotions are present\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "There are several ways to improve this emotion classifier including:\n",
    "- Experimenting with different prompts and instructions. Maybe even a series of prompts?\n",
    "- Adding a `rationale` to the output for each emotion? This might help us identify the root cause of the classifier's failures and we can improve the prompts accordingly.\n",
    "- Trying other models like Grok 2 or OpenAI's o1.\n",
    "- Adding more sophisticated scoring functions to evaluate something in particular like \"anger\" recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
