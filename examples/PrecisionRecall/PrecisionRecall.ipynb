{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Braintrust to evaluate the precision and recall of an emotion classifier\n",
    "In this cookbook, we'll learn how to evaluate a custom LLM classifier in Braintrust using custom scoring functions that measure precision and recall. For this cookbook, We'll use the [go_emotions dataset](https://huggingface.co/datasets/google-research-datasets/go_emotions), which contains Reddit comments labeled with 28 different emotions. What makes this dataset particularly interesting is that each comment can be labeled with multiple emotions - for example, a single message might express both \"excitement\" and \"anger\".\n",
    "\n",
    "We'll build two classifiers - a random baseline and an LLM-based approach using OpenAI's GPT-4o. By comparing their performance using custom scorers, we'll demonstrate how to effectively measure then improve your LLM's accuracy on complex classification tasks.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "You will need to make sure you have a [Braintrust account](https://www.braintrust.dev/signup) and an API key for [OpenAI](https://platform.openai.com/). We'll use the [AI proxy](https://www.braintrust.dev/docs/guides/proxy) to access the GPT-4o, allowing us to quickly change the model or provider without having to write model-specific code. Make sure to plug the OpenAI key into your Braintrust account's [AI providers](https://www.braintrust.dev/app/settings?subroute=secrets) configuration and acquire a [BRAINTRUST_API_KEY](https://www.braintrust.dev/app/settings?subroute=api-keys). Lastly, add your `BRAINTRUST_API_KEY` to your Python environment, or just hardcode it into the code below.,\n",
    "   \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "![aiproviders.png](assets/aiproviders.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: braintrust in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.181)\n",
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.59.7)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: autoevals in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.115)\n",
      "Requirement already satisfied: GitPython in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (3.1.44)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (2.32.3)\n",
      "Requirement already satisfied: chevron in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (0.14.0)\n",
      "Requirement already satisfied: braintrust_core==0.0.57 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (0.0.57)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (1.2.2)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (1.0.1)\n",
      "Requirement already satisfied: sseclient-py in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (1.8.0)\n",
      "Requirement already satisfied: python-slugify in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from braintrust) (8.0.4)\n",
      "Requirement already satisfied: typing_extensions>=4.1.0 in /Users/adrian/Library/Python/3.11/lib/python/site-packages (from braintrust) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /Users/adrian/Library/Python/3.11/lib/python/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: levenshtein in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from autoevals) (0.26.1)\n",
      "Requirement already satisfied: jsonschema in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from autoevals) (4.23.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->braintrust) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->braintrust) (2.3.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from GitPython->braintrust) (4.0.12)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema->autoevals) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema->autoevals) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema->autoevals) (0.22.3)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from levenshtein->autoevals) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/adrian/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-slugify->braintrust) (1.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython->braintrust) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adrian/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install braintrust openai datasets autoevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Union\n",
    "\n",
    "import openai\n",
    "import braintrust\n",
    "from datasets import load_dataset\n",
    "from autoevals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure your Braintrust client:\n",
    "\n",
    "I you haven't already, export your API key as an environment variable:\n",
    "``bash\n",
    "export BRAINTRUST_API_KEY='YOUR_BRAINTRUST_API_KEY'\n",
    "``\n",
    "\n",
    "<Callout type=\"info\">\n",
    " Best practice is to export your API key as an environment variable. However, to make it easier to follow along with this cookbook, you can also hardcode it into the code below.\n",
    "</Callout>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"BRAINTRUST_API_KEY\"] = \"sk-2omSuFIj5arE85lA37l1AVwLzx8bgWoVUU0ugTqoZzmxN9JI\"\n",
    "\n",
    "openai_client = braintrust.wrap_openai(\n",
    "    openai.OpenAI(\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",\n",
    "        api_key=os.environ[\"BRAINTRUST_API_KEY\"], #Or hardcode your API key here\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Type Definitions and Data Models\n",
    "\n",
    " We'll to set up our core types and data structures. To do this, we use Python's `Literal` type to define a strict set of allowed emotions - this gives us strong type safety and IDE support. The `EmotionType` definition helps catch invalid emotions at compile time, while the `EMOTIONS` list lets us work with these values at runtime.\n",
    "Our Pydantic model, `EmotionClassification`, defines the structure for emotion analysis results. For each piece of text, it captures:\n",
    "\n",
    "   * The detected emotions (must be from our predefined set)\n",
    "   * A confidence score between 0 and 1\n",
    "   * A rationale explaining why these emotions were detected\n",
    "\n",
    "The `load_data` function processes our raw dataset, yielding one sample at a time to be memory efficient. It extracts both the labeled emotions and metadata, making it perfect for iteration and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmotionType = Literal[\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\",\n",
    "    \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\",\n",
    "    \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "# The list remains the same\n",
    "EMOTIONS: List[str] = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\",\n",
    "    \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\",\n",
    "    \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\",\n",
    "    \"neutral\"\n",
    "]\n",
    "\n",
    "class EmotionClassification(BaseModel):\n",
    "    emotions: List[EmotionType]\n",
    "    confidence: float = Field(default=1.0, ge=0.0, le=1.0)\n",
    "    rationale: str = \"\"\n",
    "\n",
    "\n",
    "def load_data(limit: int = 100):\n",
    "    ds = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")\n",
    "    for i, item in enumerate(ds[\"train\"]):\n",
    "        if i >= limit:\n",
    "            break\n",
    "\n",
    "        actual_emotions = [emotion for emotion in EMOTIONS if item.get(emotion, 0) == 1]\n",
    "\n",
    "        yield {\n",
    "            \"input\": item[\"text\"],\n",
    "            \"expected\": actual_emotions,\n",
    "            \"metadata\": {\"subreddit\": item[\"subreddit\"], \"author\": item[\"author\"]},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Classifiers\n",
    "\n",
    "We implement two different approaches to emotion classification:\n",
    "\n",
    "1. An LLM-based classifier using GPT-4o that:\n",
    "   * Uses structured prompts to ensure valid emotion labels\n",
    "   * Returns standardized JSON with emotions, confidence, and rationale\n",
    "   * Includes robust validation and error handling\n",
    "\n",
    "2. A random baseline classifier that assigns 1-3 emotions randomly from our predefined list. This helps establish a minimum performance threshold and validates our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_classifier(text: str) -> EmotionClassification:\n",
    "    prompt = (\n",
    "        f\"Analyze the emotional content in this text. Classify using ONLY the following emotion labels:\\n\"\n",
    "        f\"{', '.join(EMOTIONS)}\\n\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Respond with a JSON object containing:\\n\"\n",
    "        f\"- emotions: array of emotions (use only the exact labels listed above)\\n\"\n",
    "        f\"- confidence: number from 0 to 1\\n\"\n",
    "        f\"- rationale: brief explanation\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        if not result:\n",
    "            raise ValueError(\"Empty response from OpenAI\")\n",
    "            \n",
    "        # First validate that we got valid emotions\n",
    "        data = json.loads(result)\n",
    "        emotions = data.get('emotions', [])\n",
    "        \n",
    "        # Validate each emotion is in our allowed set\n",
    "        validated_emotions = [e for e in emotions if e in EMOTIONS]\n",
    "        \n",
    "        # Create new validated json\n",
    "        validated_data = {\n",
    "            \"emotions\": validated_emotions,\n",
    "            \"confidence\": min(max(float(data.get('confidence', 1.0)), 0), 1),\n",
    "            \"rationale\": str(data.get('rationale', \"\"))\n",
    "        }\n",
    "        \n",
    "        return EmotionClassification.model_validate(validated_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[llm_classifier] Exception: {e}\")\n",
    "        return EmotionClassification(\n",
    "            emotions=[],\n",
    "            confidence=0.0,\n",
    "            rationale=\"Error occurred during classification\"\n",
    "        )\n",
    "\n",
    "def random_classifier(text: str) -> EmotionClassification:\n",
    "    num_emotions = random.randint(1, 3)\n",
    "    selected_emotions = random.sample(EMOTIONS, num_emotions)\n",
    "    return EmotionClassification(\n",
    "        emotions=selected_emotions,\n",
    "        confidence=random.random(),\n",
    "        rationale=\"Random selection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing evaluation metrics \n",
    "\n",
    "For multi-label classification tasks like emotion detection, we need specialized metrics that can handle multiple correct answers. We implement three key metrics:\n",
    "\n",
    "`Precision`: Measures how many of the identified emotions are actually correct. For example, if our system predicts \"joy\" and \"anger\" for a comment that only expresses \"joy\", we achieve a precision of 0.5 because only \"joy\" is correct.\n",
    "   - High precision means we rarely predict emotions that aren't present\n",
    "   - Important for applications where false positives are costly\n",
    "\n",
    "`Recall`: Measures whether we captured all the emotions that were truly present in the Reddit comments. For instance, if a comment expresses both \"sadness\" and \"fear\" but we only identify \"sadness,\" we get a recall of 0.5. This is fundamentally different from precision because it tells us if we missed emotions that should have been identified.\n",
    "   - High recall means we rarely miss emotions that are present\n",
    "   - Important for applications where false negatives are costly\n",
    "\n",
    "`F1 Score`: The F1 Score balances precision and recall by calculating their harmonic mean, which is necessary because optimizing for one often comes at the expense of the other. If our LLM achieves high precision, it might be too conservative and miss emotions (low recall). Conversely, if it achieves high recall, it might overidentify emotions (low precision). The F1 score gives us a single metric to evaluate our LLM's overall performance in identifying emotions in Reddit comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_precision(_: str, expected: List[EmotionType], output: EmotionClassification) -> float:\n",
    "    expected_set = set(expected)\n",
    "    output_set = set(output.emotions)\n",
    "    true_positives = len(expected_set & output_set)\n",
    "    false_positives = len(output_set - expected_set)\n",
    "    denominator = true_positives + false_positives\n",
    "    return true_positives / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def emotion_recall(_: str, expected: List[EmotionType], output: EmotionClassification) -> float:\n",
    "    expected_set = set(expected)\n",
    "    output_set = set(output.emotions)\n",
    "    true_positives = len(expected_set & output_set)\n",
    "    false_negatives = len(expected_set - output_set)\n",
    "    denominator = true_positives + false_negatives\n",
    "    return true_positives / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def emotion_f1(_: str, expected: List[EmotionType], output: EmotionClassification) -> float:\n",
    "    prec = emotion_precision(\"\", expected, output)\n",
    "    rec = emotion_recall(\"\", expected, output)\n",
    "    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "Finally, let's set up our evaluation pipeline using Braintrust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Not a valid object name origin/main\n",
      "Experiment random-classifier-0518e57d is running at https://www.braintrust.dev/app/braintrustdata.com/p/emotion-classification-cookbook-pydantic/experiments/random-classifier-0518e57d\n",
      "fatal: Not a valid object name origin/main\n",
      "Experiment llm-classifier-23de7340 is running at https://www.braintrust.dev/app/braintrustdata.com/p/emotion-classification-cookbook-pydantic/experiments/llm-classifier-23de7340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emotion-classification-cookbook-pydantic [experiment_name=random-classifier] (data): 100it [00:01, 68.07it/s]\n",
      "emotion-classification-cookbook-pydantic [experiment_name=llm-classifier] (data): 100it [00:00, 114.54it/s]0 [00:00<?, ?it/s]\n",
      "emotion-classification-cookbook-pydantic [experiment_name=random-classifier] (tasks): 100%|██████████| 100/100 [00:07<00:00, 13.83it/s]\n",
      "emotion-classification-cookbook-pydantic [experiment_name=llm-classifier] (tasks): 100%|██████████| 100/100 [00:10<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "random-classifier-0518e57d compared to llm-classifier-01a6f55e:\n",
      "06.33% (-23.97%) 'emotion_f1'        score\t(8 improvements, 41 regressions)\n",
      "05.50% (-20.83%) 'emotion_precision' score\t(8 improvements, 41 regressions)\n",
      "08.33% (-31.17%) 'emotion_recall'    score\t(7 improvements, 40 regressions)\n",
      "\n",
      "1736828297.81s start\n",
      "1736828303.81s end\n",
      "0.99s (-1537.40%) 'duration'\t(100 improvements, 0 regressions)\n",
      "\n",
      "See results for random-classifier-0518e57d at https://www.braintrust.dev/app/braintrustdata.com/p/emotion-classification-cookbook-pydantic/experiments/random-classifier-0518e57d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "llm-classifier-23de7340 compared to random-classifier-0518e57d:\n",
      "26.33% (+20.83%) 'emotion_precision' score\t(41 improvements, 8 regressions)\n",
      "30.30% (+23.97%) 'emotion_f1'        score\t(41 improvements, 8 regressions)\n",
      "39.50% (+31.17%) 'emotion_recall'    score\t(40 improvements, 7 regressions)\n",
      "\n",
      "1736828298.48s start\n",
      "1736828305.68s end\n",
      "139.08tok prompt_tokens\n",
      "88.53tok completion_tokens\n",
      "227.61tok total_tokens\n",
      "\n",
      "See results for llm-classifier-23de7340 at https://www.braintrust.dev/app/braintrustdata.com/p/emotion-classification-cookbook-pydantic/experiments/llm-classifier-23de7340\n"
     ]
    }
   ],
   "source": [
    "def run_evaluations(num_samples: int = 100):\n",
    "    # Random classifier experiment\n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook-pydantic\",  # The name of your project in Braintrust\n",
    "        data=lambda: load_data(limit=num_samples),\n",
    "        task=random_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"random\"},\n",
    "        experiment_name=\"random-classifier\",\n",
    "    )\n",
    "\n",
    "    # LLM classifier experiment\n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook-pydantic\",\n",
    "        data=lambda: load_data(limit=num_samples),\n",
    "        task=llm_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"llm\", \"model\": \"gpt-4o\"},\n",
    "        experiment_name=\"llm-classifier\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluations(num_samples=100) # Adjust the number of samples as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Once you run the evaluations, you'll see the results in your Braintrust dashboard. The LLM classifier should significantly outperform the random baseline across all metrics.\n",
    "\n",
    "![results.png](assets/results.png)\n",
    "\n",
    "Key features to examine:\n",
    "- Compare precision and recall scores between our runs\n",
    "- Look at specific examples where the LLM fails \n",
    "- Analyze cases where multiple emotions are present\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "There are several ways to improve this emotion classifier:\n",
    "- Experiment with different prompts and instructions. Maybe even a series of prompts?\n",
    "- Try other models like Grok 2 or OpenAI's o1.\n",
    "- Add more sophisticated scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
