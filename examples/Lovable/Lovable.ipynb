{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Lovable to Lovable Evals\n",
    "\n",
    "This cookbook guides you through adding Braintrust observability and evaluations to a Lovable app. Lovable is a no-code tool that helps non-technical builders create real apps with AI features. The implementation runs on Supabase Edge Functions with Deno and sends telemetry data to Braintrust through the SDK.\n",
    "\n",
    "By the end of this cookbook, you'll learn how to:\n",
    "\n",
    "- Add Braintrust logging to a Lovable app running on Supabase Edge + Deno\n",
    "- Configure the Braintrust SDK to send traces for observability\n",
    "- Run evals to inspect AI behavior including prompts, tool calls, and responses\n",
    "- Set up remote evals to test changes in your lovable AI features before deploying\n",
    "\n",
    "## Key components\n",
    "\n",
    "- **Lovable**: No-code platform for building real applications with AI features\n",
    "- **Braintrust SDK**: Logging and evaluation framework for AI applications\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The Lovable app runs on Supabase Edge Functions, which are managed by Lovable. Telemetry data flows from the Edge Function through the Braintrust SDK to Braintrust for monitoring and debugging. The implementation uses a traced pattern to capture root and child spans for detailed observability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use case\n",
    "\n",
    "Imagine you run a local bakery and want to ship seasonal, healthy products with compelling product pages. A Lovable app can:\n",
    "\n",
    "- Recommend items based on ingredients you have on hand\n",
    "- Generate or select strong images for your website\n",
    "\n",
    "After building the app with Lovable, it's time to connect to Braintrust so you can see what the AI is doing and iterate confidently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To get started, make sure you have:\n",
    "\n",
    "- A Lovable account with an existing app\n",
    "- A [Braintrust account](/signup) and [API key](/app/settings?subroute=api-keys)\n",
    "- Access to your Lovable app's Edge Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Braintrust API key\n",
    "\n",
    "First, create an API key in Braintrust:\n",
    "\n",
    "1. Navigate to your Braintrust account settings\n",
    "2. Go to the API Keys section\n",
    "3. Create a new API key\n",
    "4. Copy the key for use in the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![API key screenshot 1](./assets/api-key-1.png)\n",
    "![API key screenshot 2](./assets/api-key-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add the API key to Lovable Secrets\n",
    "\n",
    "From your Lovable chat interface:\n",
    "\n",
    "1. Click the cloud icon to access secrets management\n",
    "2. Add a new secret named `BRAINTRUST_API_KEY`\n",
    "3. Paste your Braintrust API key as the value\n",
    "4. Save the secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Secrets UI screenshot 1](./assets/secrets-1.png)\n",
    "![Secrets UI screenshot 2](./assets/secrets-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Braintrust logging in your Edge Function\n",
    "\n",
    "Ask Lovable to configure Braintrust logging by pasting this prompt into the Lovable chat:\n",
    "\n",
    "```\n",
    "Add Braintrust logging to [project name]'s Edge Function following this pattern:\n",
    "\n",
    "1. Import the Braintrust SDK at the top of the Edge Function file.\n",
    "2. Initialize the logger in the request handler using env var BRAINTRUST_API_KEY, with projectName set to your Braintrust project. Use asyncFlush: false to send logs immediately.\n",
    "3. Create a root span named `request` and child spans for each major step (e.g., `ai_call`, `processing`).\n",
    "   - Wrap main logic with `braintrust.traced(..., { name: \"request\" })`.\n",
    "   - Create child spans with `rootSpan.startSpan(\"step_name\")` and always `await span.end()` in `finally`.\n",
    "   - Log input and output at each span for detailed tracing.\n",
    "   - Provide a safe fallback path if the logger is unavailable.\n",
    "4. Log inputs with clear fields (e.g., userPrompt, systemPrompt in metadata, not nested in messages).\n",
    "5. Log outputs with both preview and full response.\n",
    "6. If you later handle images, log full base64 data URLs: `data:image/[type];base64,[data]`.\n",
    "7. Handle all errors and end spans in finally blocks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Function implementation\n",
    "\n",
    "Here's the template for adding Braintrust logging to your Supabase Edge Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { serve } from \"https://deno.land/std@0.168.0/http/server.ts\";\n",
    "\n",
    "// Import Braintrust SDK\n",
    "let braintrust: any = null;\n",
    "try {\n",
    "  braintrust = await import(\"https://esm.sh/braintrust@0.4.8\");\n",
    "} catch (e) {\n",
    "  // Braintrust not available, continue without logging\n",
    "}\n",
    "\n",
    "const corsHeaders = {\n",
    "  \"Access-Control-Allow-Origin\": \"*\",\n",
    "  \"Access-Control-Allow-Headers\": \"authorization, x-client-info, apikey, content-type\",\n",
    "};\n",
    "\n",
    "serve(async (req) => {\n",
    "  if (req.method === \"OPTIONS\") {\n",
    "    return new Response(null, { headers: corsHeaders });\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    // Initialize logger\n",
    "    const BRAINTRUST_API_KEY = Deno.env.get(\"BRAINTRUST_API_KEY\");\n",
    "    const logger = braintrust && BRAINTRUST_API_KEY\n",
    "      ? braintrust.initLogger({\n",
    "        projectName: \"YOUR_PROJECT_NAME\", // Replace with your project name\n",
    "        apiKey: BRAINTRUST_API_KEY,\n",
    "        asyncFlush: false,\n",
    "      })\n",
    "      : null;\n",
    "\n",
    "    // Process request with or without Braintrust\n",
    "    if (logger) {\n",
    "      return await braintrust.traced(async (rootSpan: any) => {\n",
    "        try {\n",
    "          const body = await req.json();\n",
    "\n",
    "          // Log input at root span\n",
    "          await rootSpan?.log({ input: body });\n",
    "\n",
    "          // ============================================\n",
    "          // CHILD SPAN EXAMPLE\n",
    "          // ============================================\n",
    "          const childSpan = rootSpan.startSpan(\"example_step\");\n",
    "          let stepResult;\n",
    "          try {\n",
    "            // ← Add your logic here\n",
    "            // Example: stepResult = await yourFunction(body);\n",
    "            stepResult = body; // Placeholder - replace with your actual logic\n",
    "\n",
    "            await childSpan?.log({\n",
    "              input: body,\n",
    "              output: stepResult\n",
    "            });\n",
    "          } finally {\n",
    "            await childSpan?.end();\n",
    "          }\n",
    "\n",
    "          // Add more child spans as needed...\n",
    "\n",
    "          // Log output at root span\n",
    "          const finalResult = stepResult; // ← Replace with your actual result\n",
    "          await rootSpan?.log({ output: finalResult });\n",
    "          await rootSpan?.end();\n",
    "\n",
    "          return new Response(JSON.stringify(finalResult), {\n",
    "            headers: { ...corsHeaders, \"Content-Type\": \"application/json\" },\n",
    "          });\n",
    "        } catch (error: any) {\n",
    "          await rootSpan?.log({ error: error?.message });\n",
    "          await rootSpan?.end();\n",
    "          throw error;\n",
    "        }\n",
    "      }, { name: \"request\" });\n",
    "    } else {\n",
    "      // Fallback without Braintrust\n",
    "      const body = await req.json();\n",
    "      // ← Add your logic here (same as above, just without spans)\n",
    "      // Example: const result = await yourFunction(body);\n",
    "      const result = body; // Placeholder - replace with your actual logic\n",
    "      return new Response(JSON.stringify(result), {\n",
    "        headers: { ...corsHeaders, \"Content-Type\": \"application/json\" },\n",
    "      });\n",
    "    }\n",
    "  } catch (error: any) {\n",
    "    return new Response(JSON.stringify({ error: error?.message }), {\n",
    "      status: 500,\n",
    "      headers: corsHeaders,\n",
    "    });\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify logging\n",
    "\n",
    "After implementing the logging:\n",
    "\n",
    "1. Run your AI feature end-to-end\n",
    "2. Navigate to your Braintrust project\n",
    "3. Select the **Logs** tab to view traces\n",
    "4. Confirm that traces are streaming in real-time\n",
    "5. Inspect the `ai_call` child span to see system and user prompts\n",
    "\n",
    "The traces will include detailed information about:\n",
    "- Request inputs and outputs\n",
    "- AI model interactions with prompts\n",
    "- Processing steps with latency\n",
    "- Complete request/response payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Run flow](./assets/verify-logs-1.png)\n",
    "![Logs tab](./assets/verify-logs-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Braintrust for evaluation\n",
    "\n",
    "Once logging is live, you can use Braintrust Evals to compare prompt or agent changes and score results:\n",
    "\n",
    "1. Create a playground directly from Logs\n",
    "2. Ask Braintrust's AI assistant to add custom scorers\n",
    "3. Experiment with different models and prompts\n",
    "4. Compare results side-by-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Traces not showing up\n",
    "\n",
    "- Verify secret name in Supabase matches your code\n",
    "- Ensure Braintrust `projectName` is exact\n",
    "- Look for \"[Braintrust]\" console messages\n",
    "- Ensure every span calls `await span.end()`\n",
    "\n",
    "### Images not displaying\n",
    "\n",
    "- Log full base64 data URLs\n",
    "- Keep payloads under ~10 MB per trace\n",
    "- Use format: `data:image/png;base64,...`\n",
    "- Don't log booleans — include the actual data\n",
    "\n",
    "### Errors in logs\n",
    "\n",
    "- Verify SDK import succeeded\n",
    "- Check that API key is valid\n",
    "- Ensure `asyncFlush: false` is set\n",
    "- Confirm outbound network access is allowed from Supabase Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Remote Eval with Lovable\n",
    "\n",
    "Use Remote Eval to tweak prompts or tool calls locally, then test your cloud function as if it were deployed.\n",
    "\n",
    "1. Ask Lovable for the exact Supabase Edge Function URL\n",
    "2. Run a local dev Eval server\n",
    "3. Expose it via Cloudflare Tunnel\n",
    "4. Register the tunnel URL in Braintrust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Remote eval screenshot 1](./assets/remote-eval-1.png)\n",
    "![Remote eval screenshot 2](./assets/remote-eval-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Eval } from \"braintrust\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "export default Eval(\"My Function Remote Eval\", {\n",
    "  task: async (input, { parameters }) => {\n",
    "    const functionUrl = parameters?.functionUrl || input?.functionUrl;\n",
    "    const systemPrompt =\n",
    "      parameters?.systemPrompt || input?.systemPrompt || \"You are a helpful assistant.\";\n",
    "    const userPrompt = parameters?.userPrompt || input?.userPrompt;\n",
    "\n",
    "    if (!functionUrl) throw new Error(\"Missing functionUrl\");\n",
    "\n",
    "    const resp = await fetch(functionUrl, {\n",
    "      method: \"POST\",\n",
    "      headers: { \"Content-Type\": \"application/json\" },\n",
    "      body: JSON.stringify(input || {}),\n",
    "    });\n",
    "\n",
    "    if (!resp.ok) throw new Error(`Function error ${resp.status}: ${await resp.text()}`);\n",
    "\n",
    "    return await resp.json();\n",
    "  },\n",
    "\n",
    "  scores: [],\n",
    "\n",
    "  parameters: {\n",
    "    functionUrl: z.string().describe(\"Supabase Edge Function URL\").default(\"https://your-project.supabase.co/functions/v1/your-function\"),\n",
    "  },\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Remote Eval\n",
    "\n",
    "Start the dev server and tunnel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx braintrust eval my - function- eval.js--dev--dev - host 0.0.0.0 --dev - port 8400\n",
    "npx cloudflared tunnel--url http://localhost:8400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the tunnel URL\n",
    "\n",
    "From the Playground or Project Configuration:\n",
    "\n",
    "1. Add the tunnel URL (e.g., `https://xyz-abc-123.trycloudflare.com`)\n",
    "2. Run your Remote Eval\n",
    "\n",
    "### Manual setup required each time\n",
    "\n",
    "- Dev Eval server running\n",
    "- Cloudflare Tunnel active\n",
    "- Braintrust configured with the current tunnel URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you have a Lovable app with full observability and evaluation capabilities, you can:\n",
    "\n",
    "- Create [custom scorers](/docs/guides/functions/scorers) to evaluate AI quality against specific criteria\n",
    "- Build [evaluation datasets](/docs/guides/datasets) from production logs to continuously improve your app\n",
    "- Use the [playground](/docs/guides/playground) to experiment with prompts before deploying changes\n",
    "- Add more AI features to your Lovable app with confidence in their quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "typescript",
    "version": 5
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "nbconvert_exporter": "typescript",
   "pygments_lexer": "typescript",
   "version": "5.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
