{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vigvBmpNxHvb"
   },
   "source": [
    "# Braintrust Classification Tutorial (Article Titles)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/classify/py/Braintrust-Classify-Tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This is a quick tutorial on how to build and evaluate an AI app to classify news titles into categories with [Braintrust](https://www.braintrust.dev/).\n",
    "\n",
    "Before starting, make sure that you have a Braintrust account. If you do not, please [sign up](https://www.braintrust.dev) first. After this tutorial, learn more by visiting [the docs](http://www.braintrust.dev/docs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c0hvsPRZLCUz"
   },
   "source": [
    "First, we'll install some dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTIA4DVQw1K7"
   },
   "outputs": [],
   "source": [
    "%pip install -U braintrust openai datasets autoevals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the [ag_news dataset](https://huggingface.co/datasets/ag_news) from Huggingface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8UDe2_sAw1K7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Huggingface.\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "\n",
    "# Extract category names from the dataset and build a map from index to\n",
    "# category name. We will use this to compare the expected categories to\n",
    "# those produced by the model.\n",
    "category_names = dataset.features[\"label\"].names\n",
    "category_map = dict([name for name in enumerate(category_names)])\n",
    "\n",
    "# Shuffle and trim to 20 datapoints. Restructure our dataset\n",
    "# slightly so that each item in the list contains an input\n",
    "# being the title and the expected category index label.\n",
    "trimmed_dataset = dataset.shuffle(seed=42)[:20]\n",
    "articles = [\n",
    "    {\n",
    "        \"input\": trimmed_dataset[\"text\"][i],\n",
    "        \"expected\": category_map[trimmed_dataset[\"label\"][i]],\n",
    "    }\n",
    "    for i in range(len(trimmed_dataset[\"text\"]))\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3z5e5UG3w1K8"
   },
   "source": [
    "## Writing the initial prompts\n",
    "\n",
    "Let's first write a prompt for categorizing a title for just one article. With BrainTrust, you can use any library you'd like â€” OpenAI, OSS models, LangChain, Guidance, or even just direct calls to an LLM.\n",
    "\n",
    "The prompt provides the article's title to the model, and asks it to generate a category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Bangladesh to a halt, the day after 18 people died in explosions at a political rally.\n",
      "Article Label: World\n"
     ]
    }
   ],
   "source": [
    "# Here's the input and expected output for the first article in our dataset.\n",
    "test_article = articles[0]\n",
    "test_text = test_article[\"input\"]\n",
    "expected_text = test_article[\"expected\"]\n",
    "\n",
    "print(\"Article Title:\", test_text)\n",
    "print(\"Article Label:\", expected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize an OpenAI client with your API key. We'll use `wrap_openai` from the `braintrust` library to automatically instrument the client to track useful metrics for you. When Braintrust is not initialized, `wrap_openai` is a no-op.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintrust\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"Your OPENAI_API_KEY here\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to try writing a prompt and classifying a title! We'll define a `classify_article` function that takes an input title and returns a category. The `@braintrust.traced` decorator, like `wrap_openai` above, will help us trace inputs, outputs, and timing and is a no-op when Braintrust is not active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3yK7ZSQnw1K8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Bangladesh to a halt, the day after 18 people died in explosions at a political rally.\n",
      "Classified as: World\n",
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "SEED = 123\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "def classify_article(input):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business or Sci-Tech. Reply with one word corresponding to the category.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Article title: {article_title} Category:\".format(\n",
    "                article_title=input\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    category = result.choices[0].message.content\n",
    "    return category\n",
    "\n",
    "\n",
    "test_classify = classify_article(test_text)\n",
    "print(\"Input:\", test_text)\n",
    "print(\"Classified as:\", test_classify)\n",
    "print(\"Score:\", 1 if test_classify == expected_text else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Running across the dataset\n",
    "\n",
    "Now that we have automated classifying titles, we can test the full set of articles using Braintrust's `Eval` function. Behind the scenes, `Eval` will in parallel run the `classify_article` function on each article in the dataset, and then compare the results to the ground truth labels using a simple `Levenshtein` scorer. When it finishes running, it will print out the results with a link to Braintrust to dig deeper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment main-1706735925 is running at https://www.braintrust.dev/app/braintrust.dev/p/Classifying%20News%20Articles%20Cookbook/main-1706735925\n",
      "Classifying News Articles Cookbook (data): 20it [00:00, 46065.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e36c739fe734733a41671612ed3286c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying News Articles Cookbook (tasks):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "See results for main-1706735925 at https://www.braintrust.dev/app/braintrust.dev/p/Classifying%20News%20Articles%20Cookbook/main-1706735925\n"
     ]
    }
   ],
   "source": [
    "from autoevals import Levenshtein\n",
    "\n",
    "braintrust.login(\n",
    "    api_key=os.environ.get(\"BRAINTRUST_API_KEY\", \"Your BRAINTRUST_API_KEY here\")\n",
    ")\n",
    "\n",
    "await braintrust.Eval(\n",
    "    \"Classifying News Articles Cookbook\",\n",
    "    data=articles,\n",
    "    task=classify_article,\n",
    "    scores=[Levenshtein],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "90TGWYWihDP7"
   },
   "source": [
    "## Pause and analyze the results in Braintrust!\n",
    "\n",
    "The cell above will print a link to the Braintrust experiment. Click on it to investigate where we can improve our AI app.\n",
    "\n",
    "Looking at our results table (in the screenshot below), we incorrectly output `Sci-Tech` instead of `Sci/Tech` which results in a failed eval test case. Let's fix it.\n",
    "\n",
    "![Sci/Tech issue](./assets/table.png)\n",
    "\n",
    "## Reproducing an example\n",
    "\n",
    "First, let's see if we can reproduce this issue locally. We can test an article corresponding to the `Sci/Tech` category and reproduce the evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Cosmic Storm: When Galaxy Clusters Collide Astronomers have found what they are calling the perfect cosmic storm, a galaxy cluster pile-up so powerful its energy output is second only to the Big Bang.\n",
      "Sci/Tech\n",
      "Sci-Tech\n"
     ]
    }
   ],
   "source": [
    "sci_tech_article = [a for a in articles if \"Galaxy Clusters\" in a[\"input\"]][0]\n",
    "print(sci_tech_article[\"input\"])\n",
    "print(sci_tech_article[\"expected\"])\n",
    "\n",
    "out = classify_article(sci_tech_article[\"expected\"])\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Fixing the prompt\n",
    "\n",
    "Have you spotted the issue? It looks like we misspelled one of the categories in our prompt. The dataset's categories are `World`, `Sports`, `Business` and `Sci/Tech` - but we are using `Sci-Tech` in our prompt. Let's fix it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "def classify_article(input):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business or Sci/Tech. Reply with one word corresponding to the category.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Article title: {input} Category:\".format(input=input),\n",
    "        },\n",
    "    ]\n",
    "    result = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    category = result.choices[0].message.content\n",
    "    return category\n",
    "\n",
    "\n",
    "result = classify_article(sci_tech_article[\"input\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Evaluate the new prompt\n",
    "\n",
    "The model classified the correct category `Sci/Tech` for this example. But, how do we know it works for the rest of the dataset? Let's run a new experiment to evaluate our new prompt using BrainTrust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment main-1706736165 is running at https://www.braintrust.dev/app/braintrust.dev/p/Classifying%20News%20Articles%20Cookbook/main-1706736165\n",
      "Classifying News Articles Cookbook (data): 20it [00:00, 108100.62it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29522325239146d9b6c33db655a2340b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying News Articles Cookbook (tasks):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "main-1706736165 compared to main-1706735925:\n",
      "78.12% (+01.88%) 'Levenshtein' score\t(3 improvements, 0 regressions)\n",
      "\n",
      "0.63s (-13.44%) 'duration'\t(13 improvements, 7 regressions)\n",
      "\n",
      "See results for main-1706736165 at https://www.braintrust.dev/app/braintrust.dev/p/Classifying%20News%20Articles%20Cookbook/main-1706736165\n"
     ]
    }
   ],
   "source": [
    "await braintrust.Eval(\n",
    "    \"Classifying News Articles Cookbook\",\n",
    "    data=articles,\n",
    "    task=classify_article,\n",
    "    scores=[Levenshtein],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Click into the new experiment, and check it out! You should notice a few things:\n",
    "\n",
    "![Compare](assets/comparison.png)\n",
    "\n",
    "- BrainTrust will automatically compare the new experiment to your previous one.\n",
    "- You should see the eval scores increase and you can see which test cases improved.\n",
    "- You can also filter the test cases that have a low score and work on improving the prompt for those.\n",
    "\n",
    "Now, you are on your journey of building reliable AI apps with BrainTrust!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
