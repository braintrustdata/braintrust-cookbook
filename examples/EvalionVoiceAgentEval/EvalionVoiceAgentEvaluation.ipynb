{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Evaluating voice AI agents with Evalion\n",
        "\n",
        "[Evalion](https://www.evalion.ai) is a voice-agent evaluation platform that simulates real user interactions and normalizes results across scenarios, enabling teams to detect regressions, compare runs over time, and validate an agentâ€™s readiness for production. Their platform enables teams to test voice agents by creating autonomous testing agents that conduct realistic conversations: interrupting mid-sentence, changing their mind, and expressing frustration just like real customers.\n",
        "\n",
        "This cookbook demonstrates how to evaluate voice agents by combining Evalion's simulation capabilities with Braintrust. Voice agents require assessment beyond simple text accuracy: they must handle real-time latency constraints (< 500ms responses), manage interruptions gracefully, maintain context across multi-turn conversations, and deliver natural-sounding interactions.\n",
        "\n",
        "By the end of this guide, you'll learn how to:\n",
        "\n",
        "- Create test scenarios in Braintrust datasets\n",
        "- Orchestrate automated voice simulations with Evalion's API\n",
        "- Extract and normalize voice-specific metrics (latency, CSAT, goal completion)\n",
        "- Track evaluation results across iterations\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- A [Braintrust account](https://www.braintrust.dev/signup) and [API key](https://www.braintrust.dev/app/settings?subroute=api-keyss)\n",
        "- Evalion backend access with API credentials\n",
        "- Python 3.8+"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "\n",
        "Export your API keys to your environment:\n",
        "\n",
        "```bash\n",
        "export BRAINTRUST_API_KEY=\"YOUR_BRAINTRUST_API_KEY\"\n",
        "export EVALION_API_TOKEN=\"YOUR_EVALION_API_TOKEN\"\n",
        "export EVALION_PROJECT_ID=\"YOUR_EVALION_PROJECT_ID\"\n",
        "export EVALION_PERSONA_ID=\"YOUR_EVALION_PERSONA_ID\"\n",
        "```\n",
        "\n",
        "Install the required packages:\n",
        "\n",
        "```bash\n",
        "pip install braintrust httpx pydantic\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "import-cell",
      "metadata": {},
      "source": [
        "<Callout type=\"info\">\n",
        "Best practice is to export your API key as an environment variable. However, to make it easier to follow along with this cookbook, you can also hardcode it into the code below.\n",
        "</Callout>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "Import the required libraries and set up your API credentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "from typing import Any, Dict, List, Optional\n",
        "import httpx\n",
        "import nest_asyncio\n",
        "\n",
        "from braintrust import init_dataset, EvalAsync, Score\n",
        "\n",
        "# Uncomment to hardcode your API keys\n",
        "# os.environ[\"BRAINTRUST_API_KEY\"] = \"YOUR_BRAINTRUST_API_KEY\"\n",
        "# os.environ[\"EVALION_API_TOKEN\"] = \"YOUR_EVALION_API_TOKEN\"\n",
        "# os.environ[\"EVALION_PROJECT_ID\"] = \"YOUR_EVALION_PROJECT_ID\"\n",
        "# os.environ[\"EVALION_PERSONA_ID\"] = \"YOUR_EVALION_PERSONA_ID\"\n",
        "\n",
        "BRAINTRUST_API_KEY = os.getenv(\"BRAINTRUST_API_KEY\", \"\")\n",
        "EVALION_API_TOKEN = os.getenv(\"EVALION_API_TOKEN\", \"\")\n",
        "EVALION_PROJECT_ID = os.getenv(\"EVALION_PROJECT_ID\", \"\")\n",
        "EVALION_PERSONA_ID = os.getenv(\"EVALION_PERSONA_ID\", \"\")\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-header",
      "metadata": {},
      "source": [
        "## Creating test scenarios\n",
        "\n",
        "We'll create test scenarios for an airline customer service agent. Each scenario includes the customer's situation (input) and success criteria (expected outcome). These range from straightforward bookings to high-stress cancellation handling. We'll add all the scenarios to a dataset in Braintrust. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Braintrust\n",
        "project_name = \"Voice Agent Evaluation\"\n",
        "dataset_name = \"Customer Service Scenarios\"\n",
        "\n",
        "# Create test scenarios\n",
        "test_scenarios = [\n",
        "    {\n",
        "        \"input\": \"Customer calling to book a flight from New York to Los Angeles for next Tuesday. They want a morning flight and have a budget of $400.\",\n",
        "        \"expected\": [\n",
        "            \"Agent introduces themselves professionally\",\n",
        "            \"Agent confirms the departure city (New York) and destination (Los Angeles)\",\n",
        "            \"Agent confirms the date (next Tuesday)\",\n",
        "            \"Agent asks about preferred time of day (morning)\",\n",
        "            \"Agent presents available flight options within budget\",\n",
        "            \"Agent confirms the booking details before finalizing\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Frustrated customer calling because their flight was cancelled. They need to get to Chicago for an important business meeting tomorrow morning.\",\n",
        "        \"expected\": [\n",
        "            \"Agent shows empathy for the situation\",\n",
        "            \"Agent apologizes for the inconvenience\",\n",
        "            \"Agent asks for booking reference number\",\n",
        "            \"Agent proactively searches for alternative flights\",\n",
        "            \"Agent offers multiple rebooking options\",\n",
        "            \"Agent provides compensation information if applicable\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Customer wants to change their existing reservation to add extra baggage and select a window seat.\",\n",
        "        \"expected\": [\n",
        "            \"Agent asks for booking confirmation number\",\n",
        "            \"Agent retrieves existing reservation details\",\n",
        "            \"Agent explains baggage fees and options\",\n",
        "            \"Agent checks seat availability\",\n",
        "            \"Agent confirms changes and new total cost\",\n",
        "            \"Agent sends confirmation of modifications\",\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "dataset = init_dataset(project_name, dataset_name)\n",
        "\n",
        "# Insert test scenarios\n",
        "for scenario in test_scenarios:\n",
        "    dataset.insert(**scenario)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scorers-header",
      "metadata": {},
      "source": [
        "## Creating scorers\n",
        "\n",
        "Evalion provides objective metrics (latency, duration) and subjective assessments (CSAT, clarity). We'll normalize all scores to 0-1 for consistent tracking in Braintrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scorers",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_score(\n",
        "    score_value: Optional[float], has_succeeded: Optional[bool] = None\n",
        ") -> Optional[float]:\n",
        "    \"\"\"Normalize scores to 0-1 range.\"\"\"\n",
        "    if has_succeeded is not None:\n",
        "        return 1.0 if has_succeeded else 0.0\n",
        "\n",
        "    if score_value is None:\n",
        "        return None\n",
        "\n",
        "    # Normalize 1-10 scale to 0-1\n",
        "    return max(0.0, min(1.0, score_value / 10.0))\n",
        "\n",
        "\n",
        "def extract_custom_metrics(output: Dict[str, Any]) -> List[Score]:\n",
        "    \"\"\"Extract custom metric scores from simulation results.\"\"\"\n",
        "    scores = []\n",
        "    simulations = output.get(\"simulations\", [])\n",
        "    if not simulations:\n",
        "        return scores\n",
        "\n",
        "    simulation = simulations[0]\n",
        "    evaluations = simulation.get(\"evaluations\", [])\n",
        "\n",
        "    for evaluation in evaluations:\n",
        "        if not evaluation.get(\"is_applicable\", True):\n",
        "            continue\n",
        "\n",
        "        metric = evaluation.get(\"metric\", {})\n",
        "        metric_name = metric.get(\"name\", \"unknown\")\n",
        "        measurement_type = metric.get(\"measurement_type\")\n",
        "\n",
        "        if measurement_type == \"boolean\":\n",
        "            score_value = normalize_score(None, evaluation.get(\"has_succeeded\"))\n",
        "        else:\n",
        "            score_value = normalize_score(evaluation.get(\"score\"))\n",
        "\n",
        "        if score_value is not None:\n",
        "            scores.append(\n",
        "                Score(\n",
        "                    name=metric_name,\n",
        "                    score=score_value,\n",
        "                    metadata={\n",
        "                        \"reasoning\": evaluation.get(\"reasoning\"),\n",
        "                        \"improvement_suggestions\": evaluation.get(\n",
        "                            \"improvement_suggestions\"\n",
        "                        ),\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def extract_builtin_metrics(output: Dict[str, Any]) -> List[Score]:\n",
        "    \"\"\"Extract builtin metric scores from simulation results.\"\"\"\n",
        "    scores = []\n",
        "    simulations = output.get(\"simulations\", [])\n",
        "    if not simulations:\n",
        "        return scores\n",
        "\n",
        "    simulation = simulations[0]\n",
        "    builtin_evaluations = simulation.get(\"builtin_evaluations\", [])\n",
        "\n",
        "    for evaluation in builtin_evaluations:\n",
        "        if not evaluation.get(\"is_applicable\", True):\n",
        "            continue\n",
        "\n",
        "        builtin_metric = evaluation.get(\"builtin_metric\", {})\n",
        "        metric_name = builtin_metric.get(\"name\", \"unknown\")\n",
        "        measurement_type = builtin_metric.get(\"measurement_type\")\n",
        "\n",
        "        # Handle latency specially\n",
        "        if metric_name == \"avg_latency\":\n",
        "            latency_ms = evaluation.get(\"score\")\n",
        "            if latency_ms is None:\n",
        "                continue\n",
        "\n",
        "            # Score based on distance from 1500ms target\n",
        "            target_latency = 1500\n",
        "            if latency_ms <= target_latency:\n",
        "                normalized_score = 1.0\n",
        "            else:\n",
        "                normalized_score = max(\n",
        "                    0.0, 1.0 - (latency_ms - target_latency) / target_latency\n",
        "                )\n",
        "\n",
        "            scores.append(\n",
        "                Score(\n",
        "                    name=\"avg_latency_ms\",\n",
        "                    score=normalized_score,\n",
        "                    metadata={\n",
        "                        \"actual_latency_ms\": latency_ms,\n",
        "                        \"target_latency_ms\": target_latency,\n",
        "                        \"is_within_target\": latency_ms <= target_latency,\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        if measurement_type == \"boolean\":\n",
        "            score_value = normalize_score(None, evaluation.get(\"has_succeeded\"))\n",
        "        else:\n",
        "            score_value = normalize_score(evaluation.get(\"score\"))\n",
        "\n",
        "        if score_value is not None:\n",
        "            scores.append(\n",
        "                Score(\n",
        "                    name=metric_name,\n",
        "                    score=score_value,\n",
        "                    metadata={\"reasoning\": evaluation.get(\"reasoning\")},\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "api-service-header",
      "metadata": {},
      "source": [
        "## Evalion API integration\n",
        "\n",
        "The `EvalionAPIService` class handles all interactions with Evalion's API for creating agents, test setups, and running simulations. The task function orchestrates the workflow: creating agents in Evalion, running simulations, and extracting results. This enables reproducible evaluation across iterations.\n",
        "\n",
        "The function performs the following steps:\n",
        "1. Creates a hosted agent in Evalion with your prompt\n",
        "2. Sets up test scenarios and personas\n",
        "3. Runs the voice simulation\n",
        "4. Polls for completion and retrieves results\n",
        "5. Cleans up temporary resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-service",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvalionAPIService:\n",
        "    \"\"\"Service class for interacting with the Evalion API.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, base_url: str = \"https://api.evalion.ai/api/v1\", api_token: str = \"\"\n",
        "    ):\n",
        "        self.base_url = base_url\n",
        "        self.headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "\n",
        "    async def create_hosted_agent(\n",
        "        self, prompt: str, name: Optional[str] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create a hosted agent with the given prompt.\"\"\"\n",
        "        if not name:\n",
        "            name = f\"Voice Agent - {uuid.uuid4()}\"\n",
        "\n",
        "        payload = {\n",
        "            \"name\": name,\n",
        "            \"description\": \"Agent created for evaluation\",\n",
        "            \"agent_type\": \"outbound\",\n",
        "            \"prompt\": prompt,\n",
        "            \"is_active\": True,\n",
        "            \"speaks_first\": False,\n",
        "            \"llm_provider\": \"openai\",\n",
        "            \"llm_model\": \"gpt-4o-mini\",\n",
        "            \"llm_temperature\": 0.7,\n",
        "            \"tts_provider\": \"elevenlabs\",\n",
        "            \"tts_model\": \"eleven_turbo_v2_5\",\n",
        "            \"tts_voice\": \"5IDdqnXnlsZ1FCxoOFYg\",\n",
        "            \"stt_provider\": \"openai\",\n",
        "            \"stt_model\": \"gpt-4o-mini-transcribe\",\n",
        "            \"language\": \"en\",\n",
        "            \"max_conversation_time_in_minutes\": 5,\n",
        "            \"llm_max_tokens\": 800,\n",
        "        }\n",
        "\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/hosted-agents\",\n",
        "                headers=self.headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "    async def delete_hosted_agent(self, hosted_agent_id: str) -> None:\n",
        "        \"\"\"Delete a hosted agent.\"\"\"\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.delete(\n",
        "                f\"{self.base_url}/hosted-agents/{hosted_agent_id}\",\n",
        "                headers=self.headers,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "    async def create_agent(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        hosted_agent_id: str,\n",
        "        prompt: str,\n",
        "        name: Optional[str] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create an agent that references a hosted agent.\"\"\"\n",
        "        if not name:\n",
        "            name = f\"Test Agent {int(time.time())}\"\n",
        "\n",
        "        payload = {\n",
        "            \"name\": name,\n",
        "            \"description\": \"Agent for evaluation testing\",\n",
        "            \"agent_type\": \"inbound\",\n",
        "            \"interaction_mode\": \"voice\",\n",
        "            \"integration_type\": \"phone\",\n",
        "            \"language\": \"en\",\n",
        "            \"speaks_first\": False,\n",
        "            \"prompt\": prompt,\n",
        "            \"is_active\": True,\n",
        "            \"hosted_agent_id\": hosted_agent_id,\n",
        "            \"project_id\": project_id,\n",
        "        }\n",
        "\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/projects/{project_id}/agents\",\n",
        "                headers=self.headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "    async def delete_agent(self, project_id: str, agent_id: str) -> None:\n",
        "        \"\"\"Delete an agent.\"\"\"\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.delete(\n",
        "                f\"{self.base_url}/projects/{project_id}/agents/{agent_id}\",\n",
        "                headers=self.headers,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "    async def create_test_set(\n",
        "        self, project_id: str, name: Optional[str] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create a test set.\"\"\"\n",
        "        if not name:\n",
        "            name = f\"Test Set {int(time.time())}\"\n",
        "\n",
        "        payload = {\n",
        "            \"name\": name,\n",
        "            \"description\": \"Test set for evaluation\",\n",
        "            \"project_id\": project_id,\n",
        "        }\n",
        "\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/projects/{project_id}/test-sets\",\n",
        "                headers=self.headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "    async def delete_test_set(self, project_id: str, test_set_id: str) -> None:\n",
        "        \"\"\"Delete a test set.\"\"\"\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.delete(\n",
        "                f\"{self.base_url}/projects/{project_id}/test-sets/{test_set_id}\",\n",
        "                headers=self.headers,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "    async def create_test_case(\n",
        "        self, project_id: str, test_set_id: str, scenario: str, expected_outcome: str\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create a test case.\"\"\"\n",
        "        payload = {\n",
        "            \"name\": f\"Test Case {int(time.time())}\",\n",
        "            \"description\": \"Test case for evaluation\",\n",
        "            \"scenario\": scenario,\n",
        "            \"expected_outcome\": expected_outcome,\n",
        "            \"test_set_id\": test_set_id,\n",
        "        }\n",
        "\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/projects/{project_id}/test-cases\",\n",
        "                headers=self.headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "    async def create_test_setup(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        agent_id: str,\n",
        "        persona_id: str,\n",
        "        test_set_id: str,\n",
        "        metrics: Optional[List[str]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create a test setup.\"\"\"\n",
        "        payload = {\n",
        "            \"name\": f\"Test Setup {int(time.time())}\",\n",
        "            \"description\": \"Test setup for evaluation\",\n",
        "            \"project_id\": project_id,\n",
        "            \"agents\": [agent_id],\n",
        "            \"personas\": [persona_id],\n",
        "            \"test_sets\": [test_set_id],\n",
        "            \"metrics\": metrics or [],\n",
        "            \"testing_mode\": \"voice\",\n",
        "        }\n",
        "\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/test-setups\",\n",
        "                headers=self.headers,\n",
        "                json=payload,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "\n",
        "    async def delete_test_setup(self, project_id: str, test_setup_id: str) -> None:\n",
        "        \"\"\"Delete a test setup.\"\"\"\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.delete(\n",
        "                f\"{self.base_url}/test-setups/{test_setup_id}?project_id={project_id}\",\n",
        "                headers=self.headers,\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "    async def run_test_setup(self, project_id: str, test_setup_id: str) -> str:\n",
        "        \"\"\"Prepare and run a test setup.\"\"\"\n",
        "        # First prepare\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/test-setup-runs/prepare\",\n",
        "                headers=self.headers,\n",
        "                json={\"project_id\": project_id, \"test_setup_id\": test_setup_id},\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            test_setup_run_id = response.json()[\"test_setup_run_id\"]\n",
        "\n",
        "        # Then run\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(\n",
        "                f\"{self.base_url}/test-setup-runs/{test_setup_run_id}/run\",\n",
        "                headers=self.headers,\n",
        "                json={\"project_id\": project_id},\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "        return test_setup_run_id\n",
        "\n",
        "    async def poll_for_completion(\n",
        "        self, project_id: str, test_setup_run_id: str, max_wait: int = 600\n",
        "    ) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Poll for simulation completion.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        while time.time() - start_time < max_wait:\n",
        "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "                response = await client.get(\n",
        "                    f\"{self.base_url}/test-setup-runs/{test_setup_run_id}/simulations\",\n",
        "                    headers=self.headers,\n",
        "                    params={\"project_id\": project_id},\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    simulations = data.get(\"data\", [])\n",
        "\n",
        "                    if simulations:\n",
        "                        sim = simulations[0]\n",
        "                        status = sim.get(\"run_status\")\n",
        "\n",
        "                        if status in [\"COMPLETED\", \"FAILED\"]:\n",
        "                            return sim\n",
        "\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agent-prompt-header",
      "metadata": {},
      "source": [
        "Then, we'll define the agent prompt that will be evaluated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agent-prompt",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the agent prompt to evaluate\n",
        "AGENT_PROMPT = \"\"\"\n",
        "You are a professional travel agent assistant. Your role is to help customers with:\n",
        "- Booking flights\n",
        "- Modifying existing reservations\n",
        "- Handling cancellations and rebooking\n",
        "- Answering questions about flights and policies\n",
        "\n",
        "Guidelines:\n",
        "- Always introduce yourself at the beginning of the call\n",
        "- Be empathetic, especially with frustrated customers\n",
        "- Confirm all details before making changes\n",
        "- Provide clear pricing information\n",
        "- Thank the customer at the end of the call\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "task-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_evaluation_task(input: Dict[str, Any] | str) -> Dict[str, Any]:\n",
        "    \"\"\"Main task function that orchestrates the evaluation workflow.\"\"\"\n",
        "\n",
        "    # Extract scenario and expected outcome from input\n",
        "    if isinstance(input, dict):\n",
        "        scenario = input.get(\"scenario\", \"\")\n",
        "        expected_list = input.get(\"expected\", [])\n",
        "        expected_outcome = (\n",
        "            \"\\n\".join(expected_list)\n",
        "            if isinstance(expected_list, list)\n",
        "            else str(expected_list)\n",
        "        )\n",
        "    elif isinstance(input, str):\n",
        "        scenario = input\n",
        "        expected_outcome = \"\"\n",
        "\n",
        "    # Initialize Evalion API service\n",
        "    api_service = EvalionAPIService(\n",
        "        base_url=\"https://api.evalion.ai/api/v1\", api_token=EVALION_API_TOKEN\n",
        "    )\n",
        "\n",
        "    # Store resource IDs for cleanup\n",
        "    hosted_agent_id = None\n",
        "    agent_id = None\n",
        "    test_set_id = None\n",
        "    test_setup_id = None\n",
        "\n",
        "    try:\n",
        "        # Create hosted agent\n",
        "        hosted_agent = await api_service.create_hosted_agent(\n",
        "            prompt=AGENT_PROMPT, name=\"Travel Agent Eval\"\n",
        "        )\n",
        "        hosted_agent_id = hosted_agent[\"id\"]\n",
        "\n",
        "        # Create agent\n",
        "        agent = await api_service.create_agent(\n",
        "            project_id=EVALION_PROJECT_ID,\n",
        "            hosted_agent_id=hosted_agent_id,\n",
        "            prompt=AGENT_PROMPT,\n",
        "        )\n",
        "        agent_id = agent[\"id\"]\n",
        "\n",
        "        # Create test set\n",
        "        test_set = await api_service.create_test_set(project_id=EVALION_PROJECT_ID)\n",
        "        test_set_id = test_set[\"id\"]\n",
        "\n",
        "        # Create test case\n",
        "        await api_service.create_test_case(\n",
        "            project_id=EVALION_PROJECT_ID,\n",
        "            test_set_id=test_set_id,\n",
        "            scenario=scenario,\n",
        "            expected_outcome=expected_outcome,\n",
        "        )\n",
        "\n",
        "        # Create test setup\n",
        "        test_setup = await api_service.create_test_setup(\n",
        "            project_id=EVALION_PROJECT_ID,\n",
        "            agent_id=agent_id,\n",
        "            persona_id=EVALION_PERSONA_ID,\n",
        "            test_set_id=test_set_id,\n",
        "            metrics=None,\n",
        "        )\n",
        "        test_setup_id = test_setup[\"id\"]\n",
        "\n",
        "        # Run test setup\n",
        "        test_setup_run_id = await api_service.run_test_setup(\n",
        "            project_id=EVALION_PROJECT_ID, test_setup_id=test_setup_id\n",
        "        )\n",
        "\n",
        "        # Poll for completion\n",
        "        simulation = await api_service.poll_for_completion(\n",
        "            project_id=EVALION_PROJECT_ID, test_setup_run_id=test_setup_run_id\n",
        "        )\n",
        "\n",
        "        # Clean up Evalion resources\n",
        "        if test_setup_id:\n",
        "            await api_service.delete_test_setup(EVALION_PROJECT_ID, test_setup_id)\n",
        "        if agent_id:\n",
        "            await api_service.delete_agent(EVALION_PROJECT_ID, agent_id)\n",
        "        if test_set_id:\n",
        "            await api_service.delete_test_set(EVALION_PROJECT_ID, test_set_id)\n",
        "        if hosted_agent_id:\n",
        "            await api_service.delete_hosted_agent(hosted_agent_id)\n",
        "\n",
        "        if not simulation:\n",
        "            return {\"success\": False, \"error\": \"Simulation timed out\", \"transcript\": \"\"}\n",
        "\n",
        "        # Return results\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"transcript\": simulation.get(\"transcript\", \"\"),\n",
        "            \"simulations\": [simulation],\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"success\": False, \"error\": str(e), \"transcript\": \"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-eval-header",
      "metadata": {},
      "source": [
        "Finally, we'll run the evaluation with Braintrust:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-eval",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the evaluation\n",
        "await EvalAsync(\n",
        "    \"Voice Agent Evaluation\",\n",
        "    data=dataset,\n",
        "    task=run_evaluation_task,\n",
        "    scores=[\n",
        "        extract_custom_metrics,\n",
        "        extract_builtin_metrics,\n",
        "    ],\n",
        "    parameters={\n",
        "        \"main\": {\n",
        "            \"type\": \"prompt\",\n",
        "            \"description\": \"Prompt to be tested by Evalion simulations\",\n",
        "            \"default\": {\n",
        "                \"prompt\": {\n",
        "                    \"type\": \"chat\",\n",
        "                    \"messages\": [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": AGENT_PROMPT,\n",
        "                        }\n",
        "                    ],\n",
        "                },\n",
        "                \"options\": {\"model\": \"gpt-4o\"},\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "results-header",
      "metadata": {},
      "source": [
        "## Analyzing results\n",
        "\n",
        "After running evaluations, navigate to **Experiments** in Braintrust to analyze your results. You'll see metrics like average latency, CSAT scores, and goal completion rates across all test scenarios.\n",
        "\n",
        "![braintrust-results.png](./assets/braintrust-results.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of what the results look like\n",
        "example_results = {\n",
        "    \"scenario\": \"Customer calling to book a flight from New York to Los Angeles\",\n",
        "    \"scores\": {\n",
        "        \"Expected Outcome\": 0.9,\n",
        "        \"conversation_flow\": 0.85,\n",
        "        \"empathy\": 0.92,\n",
        "        \"clarity\": 0.88,\n",
        "        \"avg_latency_ms\": 0.95,  # 1450ms actual, target 1500ms\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"transcript_length\": 450,\n",
        "        \"duration_seconds\": 180,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(json.dumps(example_results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "Now that you have a working evaluation pipeline, you can:\n",
        "\n",
        "1. **Expand test coverage**: Add more scenarios covering edge cases\n",
        "2. **Iterate on prompts**: Adjust your agent's prompt and compare results\n",
        "3. **Monitor production**: Set up online evaluation for live traffic\n",
        "4. **Track trends**: Use Braintrust's experiment comparison to identify improvements\n",
        "\n",
        "For more agent cookbooks, check out:\n",
        "- [Evaluating a voice agent](/cookbook/recipes/VoiceAgent) with OpenAI Realtime API\n",
        "- [Building reliable AI agents](/cookbook/recipes/AgentWhileLoop) with tool calling"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
