{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fcfba1",
   "metadata": {},
   "source": [
    "# Evaluating voice AI agents with Evalion and Braintrust\n",
    "\n",
    "Evaluating voice AI agents presents unique challenges compared to traditional text-based systems. Voice conversations require assessing real-time performance (response latency under 500ms, interruption handling within 200ms), multi-turn dialogue across dozens of exchanges, and subjective qualities like customer satisfaction and naturalness.\n",
    "\n",
    "Consider a customer service agent handling a flight cancellation. Success depends not just on providing correct information, but on maintaining context across the conversation, showing empathy under pressure, handling mid-sentence interruptions, and adapting to the customer's emotional state—all while processing background noise and accents in real-time.\n",
    "\n",
    "This cookbook demonstrates how to build a systematic evaluation pipeline for voice AI agents by combining **Braintrust** (an evaluation and observability platform) with **Evalion** (a voice AI testing platform that simulates realistic customer interactions). Together, they enable you to evaluate complex voice conversations at scale.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- How to set up end-to-end voice agent evaluation pipelines\n",
    "- How to use Braintrust datasets to define test scenarios\n",
    "- How to orchestrate automated voice simulations with Evalion\n",
    "- How to extract and track metrics across multiple evaluation runs\n",
    "- How to build your own integration between evaluation platforms\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Braintrust Account**: Sign up at [braintrust.dev](https://www.braintrust.dev)\n",
    "2. **Evalion Access**: Running Evalion backend with API access\n",
    "3. **API Credentials**: Both Braintrust and Evalion API tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffbbf2",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "## Braintrust platform setup\n",
    "\n",
    "Before running the evaluation programmatically, let's see how to set up the key components in the Braintrust platform UI.\n",
    "\n",
    "### 1. Creating a dataset in Braintrust\n",
    "\n",
    "In the Braintrust platform, you can create and manage datasets through the web interface. Here's what the dataset creation looks like:\n",
    "\n",
    "![braintrust-dataset.png](./assets/braintrust-dataset.png)\n",
    "\n",
    "*The dataset view shows your test scenarios with input/expected pairs that will be used for evaluation.*\n",
    "\n",
    "### 2. Setting up a playground\n",
    "\n",
    "The Playground allows you to test and iterate on your prompts before running full evaluations:\n",
    "\n",
    "![braintrust-playground.png](./assets/braintrust-playground.png)\n",
    "\n",
    "*The Playground interface lets you experiment with different agent configurations and see immediate results.*\n",
    "\n",
    "### 3. Running experiments\n",
    "\n",
    "Once your dataset and prompts are configured, you can run experiments to evaluate performance:\n",
    "\n",
    "![braintrust-experiment.png](./assets/braintrust-experiment.png)\n",
    "\n",
    "*The experiment view displays comprehensive metrics, scores, and comparisons across multiple runs.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732c126",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install braintrust httpx pydantic asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a68b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# Set your API keys as environment variables\n",
    "# os.environ[\"BRAINTRUST_API_KEY\"] = \"<YOUR_BRAINTRUST_API_KEY>\"\n",
    "# os.environ[\"EVALION_API_TOKEN\"] = \"<YOUR_EVALION_API_TOKEN>\"\n",
    "# os.environ[\"EVALION_PROJECT_ID\"] = \"<YOUR_EVALION_PROJECT_ID>\"\n",
    "# os.environ[\"EVALION_PERSONA_ID\"] = \"<YOUR_EVALION_PERSONA_ID>\"\n",
    "\n",
    "# For demo purposes, we'll use placeholders\n",
    "BRAINTRUST_API_KEY = os.getenv(\"BRAINTRUST_API_KEY\", \"\")\n",
    "EVALION_API_TOKEN = os.getenv(\"EVALION_API_TOKEN\", \"\")\n",
    "EVALION_PROJECT_ID = os.getenv(\"EVALION_PROJECT_ID\", \"\")\n",
    "EVALION_PERSONA_ID = os.getenv(\"EVALION_PERSONA_ID\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dde88f",
   "metadata": {},
   "source": [
    "## Understanding the workflow\n",
    "\n",
    "Braintrust manages the evaluation pipeline—organizing datasets, launching evaluations, and tracking metrics over time. Evalion provides voice-specific capabilities: simulating realistic customer interactions and measuring voice metrics.\n",
    "\n",
    "The workflow uses Braintrust's evaluation primitives:\n",
    "1. **Datasets**: Define test scenarios for your voice agent\n",
    "2. **Tasks**: Configure Evalion to simulate realistic customer conversations with your voice agent\n",
    "3. **Scorers**: Measure voice-specific metrics (latency, customer satisfaction, goal completion)\n",
    "4. **Experiments**: Run evaluations across multiple scenarios and track results over time\n",
    "5. **Analysis**: Review results in Braintrust dashboard to identify improvements and regressions\n",
    "\n",
    "Let's build each component step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f775e06",
   "metadata": {},
   "source": [
    "## 1: Creating test scenarios in Braintrust\n",
    "\n",
    "Braintrust datasets define test scenarios for your voice agent. Each scenario specifies three components: the customer's situation (input), their behavioral characteristics (persona), and what constitutes successful handling (expected outcome).\n",
    "\n",
    "For our airline customer service agent, scenarios range from straightforward bookings to high-stress cancellation handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94740a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import init_dataset, init\n",
    "\n",
    "# Initialize Braintrust\n",
    "project_name = \"Voice Agent Evaluation Demo\"\n",
    "dataset_name = \"Customer Service Scenarios\"\n",
    "\n",
    "# Create test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"input\": \"Customer calling to book a flight from New York to Los Angeles for next Tuesday. They want a morning flight and have a budget of $400.\",\n",
    "        \"expected\": [\n",
    "            \"Agent introduces themselves professionally\",\n",
    "            \"Agent confirms the departure city (New York) and destination (Los Angeles)\",\n",
    "            \"Agent confirms the date (next Tuesday)\",\n",
    "            \"Agent asks about preferred time of day (morning)\",\n",
    "            \"Agent presents available flight options within budget\",\n",
    "            \"Agent confirms the booking details before finalizing\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Frustrated customer calling because their flight was cancelled. They need to get to Chicago for an important business meeting tomorrow morning.\",\n",
    "        \"expected\": [\n",
    "            \"Agent shows empathy for the situation\",\n",
    "            \"Agent apologizes for the inconvenience\",\n",
    "            \"Agent asks for booking reference number\",\n",
    "            \"Agent proactively searches for alternative flights\",\n",
    "            \"Agent offers multiple rebooking options\",\n",
    "            \"Agent provides compensation information if applicable\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Customer wants to change their existing reservation to add extra baggage and select a window seat.\",\n",
    "        \"expected\": [\n",
    "            \"Agent asks for booking confirmation number\",\n",
    "            \"Agent retrieves existing reservation details\",\n",
    "            \"Agent explains baggage fees and options\",\n",
    "            \"Agent checks seat availability\",\n",
    "            \"Agent confirms changes and new total cost\",\n",
    "            \"Agent sends confirmation of modifications\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create or update dataset in Braintrust\n",
    "dataset = init_dataset(project_name, dataset_name)\n",
    "\n",
    "# Insert test scenarios\n",
    "for scenario in test_scenarios:\n",
    "    dataset.insert(**scenario)\n",
    "\n",
    "print(f\"Created dataset '{dataset_name}' with {len(test_scenarios)} scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1957503",
   "metadata": {},
   "source": [
    "These scenarios capture the spectrum of real customer interactions—from first-time nervous travelers to experienced flyers with complex itineraries. The persona details (interrupts when confused, gets frustrated if interrupted mid-sentence) ensure simulations behave like actual customers, not scripted test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd5745",
   "metadata": {},
   "source": [
    "## 2: Creating Scorers\n",
    "\n",
    "Voice scorers must evaluate what traditional scorers can't. Beyond \"Did the agent book the flight?\" you need to assess:\n",
    "\n",
    "- **Latency**: Did the agent find alternatives within 800ms?\n",
    "- **Customer satisfaction (CSAT)**: Did the agent acknowledge the customer's frustration?\n",
    "- **Goal completion**: Was the cancelled flight successfully rebooked?\n",
    "\n",
    "Evalion provides both objective metrics (latency, duration) and subjective assessments (CSAT, clarity). All scores normalize to 0-1 for Braintrust tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb989aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from braintrust import Score\n",
    "\n",
    "\n",
    "def normalize_score(\n",
    "    score_value: Optional[float], has_succeeded: Optional[bool] = None\n",
    ") -> Optional[float]:\n",
    "    \"\"\"Normalize scores to 0-1 range for Braintrust.\"\"\"\n",
    "    if has_succeeded is not None:\n",
    "        return 1.0 if has_succeeded else 0.0\n",
    "\n",
    "    if score_value is None:\n",
    "        return None\n",
    "\n",
    "    # Normalize 1-10 scale to 0-1\n",
    "    return max(0.0, min(1.0, score_value / 10.0))\n",
    "\n",
    "\n",
    "def extract_custom_metrics(output: Dict[str, Any]) -> List[Score]:\n",
    "    \"\"\"Extract custom metric scores from simulation results.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    simulations = output.get(\"simulations\", [])\n",
    "    if not simulations:\n",
    "        return scores\n",
    "\n",
    "    simulation = simulations[0]\n",
    "    evaluations = simulation.get(\"evaluations\", [])\n",
    "\n",
    "    for evaluation in evaluations:\n",
    "        metric = evaluation.get(\"metric\", {})\n",
    "        metric_name = metric.get(\"name\", \"unknown\")\n",
    "        measurement_type = metric.get(\"measurement_type\")\n",
    "\n",
    "        if not evaluation.get(\"is_applicable\", True):\n",
    "            continue\n",
    "\n",
    "        if measurement_type == \"boolean\":\n",
    "            score_value = normalize_score(None, evaluation.get(\"has_succeeded\"))\n",
    "        else:\n",
    "            score_value = normalize_score(evaluation.get(\"score\"))\n",
    "\n",
    "        if score_value is not None:\n",
    "            scores.append(\n",
    "                Score(\n",
    "                    name=metric_name,\n",
    "                    score=score_value,\n",
    "                    metadata={\n",
    "                        \"reasoning\": evaluation.get(\"reasoning\"),\n",
    "                        \"improvement_suggestions\": evaluation.get(\n",
    "                            \"improvement_suggestions\"\n",
    "                        ),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def extract_builtin_metrics(output: Dict[str, Any]) -> List[Score]:\n",
    "    \"\"\"Extract builtin metric scores from simulation results.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    simulations = output.get(\"simulations\", [])\n",
    "    if not simulations:\n",
    "        return scores\n",
    "\n",
    "    simulation = simulations[0]\n",
    "    builtin_evaluations = simulation.get(\"builtin_evaluations\", [])\n",
    "\n",
    "    for evaluation in builtin_evaluations:\n",
    "        builtin_metric = evaluation.get(\"builtin_metric\", {})\n",
    "        metric_name = builtin_metric.get(\"name\", \"unknown\")\n",
    "        measurement_type = builtin_metric.get(\"measurement_type\")\n",
    "\n",
    "        if not evaluation.get(\"is_applicable\", True):\n",
    "            continue\n",
    "\n",
    "        # Skip avg_latency (handled separately)\n",
    "        print(f\"metric_name: {metric_name}\")\n",
    "        if metric_name == \"avg_latency\":\n",
    "            latency_ms = evaluation.get(\"score\")\n",
    "\n",
    "            print(f\"latency_ms: {latency_ms} ms\")\n",
    "            if latency_ms is None:\n",
    "                continue\n",
    "\n",
    "            # Score based on distance from 1500ms target\n",
    "            target_latency = 1500\n",
    "            if latency_ms <= target_latency:\n",
    "                normalized_score = 1.0\n",
    "            else:\n",
    "                normalized_score = max(\n",
    "                    0.0, 1.0 - (latency_ms - target_latency) / target_latency\n",
    "                )\n",
    "\n",
    "            print(f\"Latency: {latency_ms} ms, Score: {normalized_score}\")\n",
    "\n",
    "            scores.append(\n",
    "                Score(\n",
    "                    name=\"avg_latency_ms\",\n",
    "                    score=normalized_score,\n",
    "                    metadata={\n",
    "                        \"actual_latency_ms\": latency_ms,\n",
    "                        \"target_latency_ms\": target_latency,\n",
    "                        \"is_within_target\": latency_ms <= target_latency,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if measurement_type == \"boolean\":\n",
    "            score_value = normalize_score(None, evaluation.get(\"has_succeeded\"))\n",
    "        else:\n",
    "            score_value = normalize_score(evaluation.get(\"score\"))\n",
    "\n",
    "        if score_value is not None:\n",
    "            scores.append(\n",
    "                Score(\n",
    "                    name=metric_name,\n",
    "                    score=score_value,\n",
    "                    metadata={\n",
    "                        \"reasoning\": evaluation.get(\"reasoning\"),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"✅ Scorer functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc725a0",
   "metadata": {},
   "source": [
    "## 3: Defining your tasks through Evalion\n",
    "\n",
    "For voice agent evaluation, the \"task\" is the actual conversation between a simulated customer and your agent. Unlike traditional functions with simple inputs/outputs, Evalion creates autonomous testing agents that call your voice agent's phone number and conduct realistic conversations—interrupting mid-sentence, changing their mind, expressing frustration just like real customers.\n",
    "\n",
    "Now let's create a service class to interact with Evalion's API. This handles all the HTTP requests for creating agents, test setups, and running simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import time\n",
    "import uuid\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "class EvalionAPIService:\n",
    "    \"\"\"Service class for interacting with the Evalion API.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_url: str = \"https://api.evalion.ai/api/v1\", api_token: str = \"\"\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "    async def create_hosted_agent(\n",
    "        self, prompt: str, name: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create a subject agent to be testedwith the given prompt. This agent will handle incoming calls from test scenarios.\"\"\"\n",
    "        if not name:\n",
    "            name = f\"Voice Agent - {uuid.uuid4()}\"\n",
    "        else:\n",
    "            name = f\"{name} - {uuid.uuid4()}\"\n",
    "\n",
    "        payload = {\n",
    "            \"name\": name,\n",
    "            \"description\": \"Agent created for evaluation\",\n",
    "            \"agent_type\": \"outbound\",\n",
    "            \"prompt\": prompt,\n",
    "            \"is_active\": True,\n",
    "            \"speaks_first\": False,\n",
    "            \"llm_provider\": \"openai\",\n",
    "            \"llm_model\": \"gpt-4o-mini\",\n",
    "            \"llm_temperature\": 0.7,\n",
    "            \"tts_provider\": \"elevenlabs\",\n",
    "            \"tts_model\": \"eleven_turbo_v2_5\",\n",
    "            \"tts_voice\": \"5IDdqnXnlsZ1FCxoOFYg\",\n",
    "            \"stt_provider\": \"openai\",\n",
    "            \"stt_model\": \"gpt-4o-mini-transcribe\",\n",
    "            \"language\": \"en\",\n",
    "            \"max_conversation_time_in_minutes\": 5,\n",
    "            \"llm_max_tokens\": 800,\n",
    "        }\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/hosted-agents\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    async def delete_hosted_agent(self, hosted_agent_id: str) -> None:\n",
    "        \"\"\"Delete a hosted agent.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.delete(\n",
    "                f\"{self.base_url}/hosted-agents/{hosted_agent_id}\",\n",
    "                headers=self.headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return None\n",
    "\n",
    "    async def create_agent(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        hosted_agent_id: str,\n",
    "        prompt: str,\n",
    "        name: Optional[str] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create an agent that references a hosted agent.\"\"\"\n",
    "        if not name:\n",
    "            name = f\"Test Agent {int(time.time())}\"\n",
    "\n",
    "        payload = {\n",
    "            \"name\": name,\n",
    "            \"description\": \"Agent for evaluation testing\",\n",
    "            \"agent_type\": \"inbound\",\n",
    "            \"interaction_mode\": \"voice\",\n",
    "            \"integration_type\": \"phone\",\n",
    "            \"language\": \"en\",\n",
    "            \"speaks_first\": False,\n",
    "            \"prompt\": prompt,\n",
    "            \"is_active\": True,\n",
    "            \"hosted_agent_id\": hosted_agent_id,\n",
    "            \"project_id\": project_id,\n",
    "        }\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/projects/{project_id}/agents\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    async def delete_agent(self, project_id: str, agent_id: str) -> None:\n",
    "        \"\"\"Delete an agent.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.delete(\n",
    "                f\"{self.base_url}/projects/{project_id}/agents/{agent_id}\",\n",
    "                headers=self.headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return None\n",
    "\n",
    "    async def create_test_set(\n",
    "        self, project_id: str, name: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create a test set.\"\"\"\n",
    "        if not name:\n",
    "            name = f\"Test Set {int(time.time())}\"\n",
    "\n",
    "        payload = {\n",
    "            \"name\": name,\n",
    "            \"description\": \"Test set for evaluation\",\n",
    "            \"project_id\": project_id,\n",
    "        }\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/projects/{project_id}/test-sets\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    async def delete_test_set(self, project_id: str, test_set_id: str) -> None:\n",
    "        \"\"\"Delete a test set.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.delete(\n",
    "                f\"{self.base_url}/projects/{project_id}/test-sets/{test_set_id}\",\n",
    "                headers=self.headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return None\n",
    "\n",
    "    async def create_test_case(\n",
    "        self, project_id: str, test_set_id: str, scenario: str, expected_outcome: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create a test case.\"\"\"\n",
    "        payload = {\n",
    "            \"name\": f\"Test Case {int(time.time())}\",\n",
    "            \"description\": \"Test case for evaluation\",\n",
    "            \"scenario\": scenario,\n",
    "            \"expected_outcome\": expected_outcome,\n",
    "            \"test_set_id\": test_set_id,\n",
    "        }\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/projects/{project_id}/test-cases\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    async def delete_test_case(self, project_id: str, test_case_id: str) -> None:\n",
    "        \"\"\"Delete a test case.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.delete(\n",
    "                f\"{self.base_url}/projects/{project_id}/test-cases/{test_case_id}\",\n",
    "                headers=self.headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return None\n",
    "\n",
    "    async def create_test_setup(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        agent_id: str,\n",
    "        persona_id: str,\n",
    "        test_set_id: str,\n",
    "        metrics: Optional[List[str]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Create a test setup.\"\"\"\n",
    "        payload = {\n",
    "            \"name\": f\"Test Setup {int(time.time())}\",\n",
    "            \"description\": \"Test setup for evaluation\",\n",
    "            \"project_id\": project_id,\n",
    "            \"agents\": [agent_id],\n",
    "            \"personas\": [persona_id],\n",
    "            \"test_sets\": [test_set_id],\n",
    "            \"metrics\": metrics or [],\n",
    "            \"testing_mode\": \"voice\",\n",
    "        }\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/test-setups\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    async def delete_test_setup(self, project_id: str, test_setup_id: str) -> None:\n",
    "        \"\"\"Delete a test setup.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.delete(\n",
    "                f\"{self.base_url}/test-setups/{test_setup_id}?project_id={project_id}\",\n",
    "                headers=self.headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return None\n",
    "\n",
    "    async def run_test_setup(self, project_id: str, test_setup_id: str) -> str:\n",
    "        \"\"\"Prepare and run a test setup.\"\"\"\n",
    "        # First prepare\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/test-setup-runs/prepare\",\n",
    "                headers=self.headers,\n",
    "                json={\"project_id\": project_id, \"test_setup_id\": test_setup_id},\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            test_setup_run_id = response.json()[\"test_setup_run_id\"]\n",
    "\n",
    "        # Then run\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{self.base_url}/test-setup-runs/{test_setup_run_id}/run\",\n",
    "                headers=self.headers,\n",
    "                json={\"project_id\": project_id},\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "        return test_setup_run_id\n",
    "\n",
    "    async def poll_for_completion(\n",
    "        self, project_id: str, test_setup_run_id: str, max_wait: int = 600\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Poll for simulation completion.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        while time.time() - start_time < max_wait:\n",
    "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "                response = await client.get(\n",
    "                    f\"{self.base_url}/test-setup-runs/{test_setup_run_id}/simulations\",\n",
    "                    headers=self.headers,\n",
    "                    params={\"project_id\": project_id},\n",
    "                )\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    simulations = data.get(\"data\", [])\n",
    "\n",
    "                    if simulations:\n",
    "                        sim = simulations[0]\n",
    "                        status = sim.get(\"run_status\")\n",
    "\n",
    "                        if status in [\"COMPLETED\", \"FAILED\"]:\n",
    "                            return sim\n",
    "\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"EvalionAPIService class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f0101",
   "metadata": {},
   "source": [
    "This service class provides methods for the complete evaluation lifecycle:\n",
    "- Creating voice agents with your prompt\n",
    "- Setting up scenarios sets and test suites\n",
    "- Running simulations (agents call each other)\n",
    "- Polling for completion and retrieving results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab12e5d",
   "metadata": {},
   "source": [
    "## 4: Running your experiment\n",
    "\n",
    "Now we orchestrate the complete workflow: create agents, define scenarios, run simulations, and measure performance. The power is **reproducibility**—run the same evaluation after each prompt change to track improvements systematically.\n",
    "\n",
    "Here's the main evaluation function that ties everything together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import EvalAsync\n",
    "\n",
    "# Define the agent prompt to evaluate\n",
    "AGENT_PROMPT = \"\"\"\n",
    "You are a professional travel agent assistant. Your role is to help customers with:\n",
    "- Booking flights\n",
    "- Modifying existing reservations\n",
    "- Handling cancellations and rebooking\n",
    "- Answering questions about flights and policies\n",
    "\n",
    "Guidelines:\n",
    "- Always introduce yourself at the beginning of the call\n",
    "- Be empathetic, especially with frustrated customers\n",
    "- Confirm all details before making changes\n",
    "- Provide clear pricing information\n",
    "- Thank the customer at the end of the call\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def run_evaluation_task(input: Dict[str, Any] | str) -> Dict[str, Any]:\n",
    "    \"\"\"Main task function that orchestrates the evaluation workflow.\"\"\"\n",
    "\n",
    "    # Extract scenario and expected outcome from input\n",
    "    # Handle both old format (input is a string) and new format (input is a dict)\n",
    "    if isinstance(input, dict):\n",
    "        # New format: input is {\"scenario\": \"...\", \"expected\": [...]}\n",
    "        scenario = input.get(\"scenario\", \"\")\n",
    "        expected_list = input.get(\"expected\", [])\n",
    "        # Convert expected list to string format for Evalion API\n",
    "        expected_outcome = (\n",
    "            \"\\n\".join(expected_list)\n",
    "            if isinstance(expected_list, list)\n",
    "            else str(expected_list)\n",
    "        )\n",
    "    elif isinstance(input, str):\n",
    "        # Old format: input is just the scenario string\n",
    "        scenario = input\n",
    "        expected_outcome = \"\"\n",
    "\n",
    "    # Initialize Evalion API service\n",
    "    api_service = EvalionAPIService(\n",
    "        base_url=\"https://api.evalion.ai/api/v1\", api_token=EVALION_API_TOKEN\n",
    "    )\n",
    "\n",
    "    # Store resource IDs for cleanup\n",
    "    hosted_agent_id = None\n",
    "    agent_id = None\n",
    "    test_set_id = None\n",
    "    test_setup_id = None\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create hosted agent\n",
    "        print(\"Creating hosted agent...\")\n",
    "        hosted_agent = await api_service.create_hosted_agent(\n",
    "            prompt=AGENT_PROMPT, name=\"Travel Agent Eval\"\n",
    "        )\n",
    "        hosted_agent_id = hosted_agent[\"id\"]\n",
    "\n",
    "        # Step 2: Create agent\n",
    "        print(\"Creating agent...\")\n",
    "        agent = await api_service.create_agent(\n",
    "            project_id=EVALION_PROJECT_ID,\n",
    "            hosted_agent_id=hosted_agent_id,\n",
    "            prompt=AGENT_PROMPT,\n",
    "        )\n",
    "        agent_id = agent[\"id\"]\n",
    "\n",
    "        # Step 3: Create test set\n",
    "        print(\"Creating test set...\")\n",
    "        test_set = await api_service.create_test_set(project_id=EVALION_PROJECT_ID)\n",
    "        test_set_id = test_set[\"id\"]\n",
    "\n",
    "        # Step 4: Create test case\n",
    "        print(\"Creating test case...\")\n",
    "        await api_service.create_test_case(\n",
    "            project_id=EVALION_PROJECT_ID,\n",
    "            test_set_id=test_set_id,\n",
    "            scenario=scenario,\n",
    "            expected_outcome=expected_outcome,\n",
    "        )\n",
    "\n",
    "        # Step 5: Create test setup\n",
    "        print(\"Creating test setup...\")\n",
    "        test_setup = await api_service.create_test_setup(\n",
    "            project_id=EVALION_PROJECT_ID,\n",
    "            agent_id=agent_id,\n",
    "            persona_id=EVALION_PERSONA_ID,\n",
    "            test_set_id=test_set_id,\n",
    "            metrics=None,  # Example metric ID\n",
    "        )\n",
    "        test_setup_id = test_setup[\"id\"]\n",
    "\n",
    "        # Step 6: Run test setup\n",
    "        print(\"Running test setup...\")\n",
    "        test_setup_run_id = await api_service.run_test_setup(\n",
    "            project_id=EVALION_PROJECT_ID, test_setup_id=test_setup_id\n",
    "        )\n",
    "\n",
    "        # Step 7: Poll for completion\n",
    "        print(\"Waiting for simulation to complete...\")\n",
    "        simulation = await api_service.poll_for_completion(\n",
    "            project_id=EVALION_PROJECT_ID, test_setup_run_id=test_setup_run_id\n",
    "        )\n",
    "\n",
    "        # Step 8: Clean up Evalion resources\n",
    "        print(\"Deleting test setup...\")\n",
    "        await api_service.delete_test_setup(EVALION_PROJECT_ID, test_setup_id)\n",
    "        print(\"Deleting agent...\")\n",
    "        await api_service.delete_agent(EVALION_PROJECT_ID, agent_id)\n",
    "        print(\"Deleting test set...\")\n",
    "        await api_service.delete_test_set(EVALION_PROJECT_ID, test_set_id)\n",
    "        print(\"Deleting hosted agent...\")\n",
    "        await api_service.delete_hosted_agent(hosted_agent_id)\n",
    "\n",
    "        if not simulation:\n",
    "            return {\"success\": False, \"error\": \"Simulation timed out\", \"transcript\": \"\"}\n",
    "\n",
    "        # Return results\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"transcript\": simulation.get(\"transcript\", \"\"),\n",
    "            \"simulations\": [simulation],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"transcript\": \"\"}\n",
    "\n",
    "\n",
    "print(\"Evaluation task function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1406283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation (this would typically be run via Braintrust CLI)\n",
    "# For demonstration, we show how it would be structured\n",
    "\n",
    "# Note: In production, you would run this with:\n",
    "# braintrust eval path/to/eval_script.py --braintrust_project \"Your Project\" --braintrust_dataset \"Your Dataset\"\n",
    "await EvalAsync(\n",
    "    \"Voice Agent Evaluation Demo\",\n",
    "    data=dataset,\n",
    "    task=run_evaluation_task,\n",
    "    scores=[\n",
    "        extract_custom_metrics,\n",
    "        extract_builtin_metrics,\n",
    "    ],\n",
    "    parameters={\n",
    "        \"main\": {\n",
    "            \"type\": \"prompt\",\n",
    "            \"description\": \"Prompt to be tested by Evalion simulations\",\n",
    "            \"default\": {\n",
    "                \"prompt\": {\n",
    "                    \"type\": \"chat\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": AGENT_PROMPT,\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                \"options\": {\"model\": \"gpt-4o\"},\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29719162",
   "metadata": {},
   "source": [
    "## 5: Analyzing results\n",
    "\n",
    "After running evaluations, you can analyze results in the Braintrust dashboard. Here's what you'll see:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbb0fc",
   "metadata": {},
   "source": [
    "After running the evaluation, navigate to Evaluations > Experiments in the Braintrust UI to see your results.\n",
    "\n",
    "Here you will see metrics like average latency, CSAT scores, and goal completion rates across all test scenarios. You can drill down into individual scenarios to identify specific strengths and weaknesses of your voice agent.\n",
    "\n",
    "![braintrust-results.png](./assets/braintrust-results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of what the results look like\n",
    "example_results = {\n",
    "    \"scenario\": \"Customer calling to book a flight from New York to Los Angeles\",\n",
    "    \"scores\": {\n",
    "        \"Expected Outcome\": 0.9,  # Custom metric: Did agent meet expectations?\n",
    "        \"conversation_flow\": 0.85,  # Builtin: Was conversation natural?\n",
    "        \"empathy\": 0.92,  # Builtin: Did agent show empathy?\n",
    "        \"clarity\": 0.88,  # Builtin: Was agent clear?\n",
    "        \"avg_latency_ms\": 0.95,  # Builtin: Response time (1450ms actual, target 1500ms)\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"transcript_length\": 450,\n",
    "        \"duration_seconds\": 180,\n",
    "        \"reasoning\": \"Agent performed well overall, successfully gathered all required information...\",\n",
    "        \"improvement_suggestions\": \"Could be more proactive in offering seat selection options\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Example Results Structure:\")\n",
    "print(json.dumps(example_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db530c",
   "metadata": {},
   "source": [
    "The combination of scores reveals the full picture:\n",
    "\n",
    "- **High empathy (0.92) + Lower clarity (0.88)**: Agent is warm but needs clearer fee explanations\n",
    "- **Great latency (0.95) + Lower flow (0.85)**: Fast searches but awkward booking transitions\n",
    "\n",
    "**Compare iterations**: Run v1 → Adjust prompt → Run v2 → See what improved (and what regressed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5dabee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Evaluating voice agents requires three shifts from traditional AI evaluation:\n",
    "\n",
    "**Full conversations**: Assess entire interactions (goal achievement, engagement, flow), not isolated responses.\n",
    "\n",
    "**Dynamic simulations**: Test with realistic behaviors (interruptions, frustration, topic changes), not fixed prompts.\n",
    "\n",
    "**Interaction quality**: Measure technical performance (latency, interruption handling) and subjective experience (customer satisfaction, clarity).\n",
    "\n",
    "This integration enables systematic evaluation at scale—reproducible testing across iterations with multidimensional metrics combining technical and qualitative assessment.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've learned how to:\n",
    "\n",
    "- Set up an end-to-end voice agent evaluation pipeline  \n",
    "- Integrate Evalion's simulation testing with Braintrust's evaluation platform  \n",
    "- Create custom scorers and metrics  \n",
    "- Orchestrate automated testing workflows  \n",
    "- Extract and analyze evaluation results  \n",
    "\n",
    "### Next steps\n",
    "1. **Start small**: Create 5-10 core scenarios (bookings, cancellations, common questions)\n",
    "2. **Establish baselines**: Run first evaluation to benchmark current performance\n",
    "3. **Iterate systematically**: Adjust prompt → Run eval → Compare → Deploy\n",
    "4. **Automate**: Integrate with CI/CD to test every prompt change\n",
    "5. **Track trends**: Monitor improvement over time via Braintrust dashboard\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Braintrust Documentation](https://www.braintrust.dev/docs)\n",
    "- [Evalion API Documentation](http://docs.evalion.ai)\n",
    "- [Contact Evalion](mailto:support@evalion.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38f3ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
