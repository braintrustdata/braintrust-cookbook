{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a reliable agent for interacting with an API\n",
    "\n",
    "We're going to build an agent that can interact with users to run complex commands against a custom API. For this example, we'll use the Braintrust API, which has an easy\n",
    "to work with [OpenAPI spec](https://github.com/braintrustdata/braintrust-openapi).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing dependencies and setting up our OpenAI and Braintrust environments.\n",
    "\n",
    "Before getting started, make sure you have a [Braintrust account](https://www.braintrust.dev/signup) and an API key for [OpenAI](https://platform.openai.com/). Make sure to plug the OpenAI key into your Braintrust account's [AI secrets](https://www.braintrust.dev/app/settings?subroute=secrets) configuration and acquire a [BRAINTRUST_API_KEY](https://www.braintrust.dev/app/settings?subroute=api-keys). Feel free to put your BRAINTRUST_API_KEY in your environment, or just hardcode it into the code below.\n",
    "\n",
    "### Install dependencies\n",
    "\n",
    "We're not going to use any frameworks or complex dependencies to keep things simple and literate. Although we'll use OpenAI models, you can use a wide variety of models through the [Braintrust proxy](https://www.braintrust.dev/docs/guides/proxy) without having to write model-specific code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U autoevals braintrust jsonref openai numpy pydantic requests tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries\n",
    "\n",
    "Next, let's wire up the OpenAI and Braintrust clients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import braintrust\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "BRAINTRUST_API_KEY = os.environ.get(\"BRAINTRUST_API_KEY\") # Or hardcode this to your API key\n",
    "OPENAI_BASE_URL = \"https://api.braintrust.dev/v1/proxy\" # You can use your own base URL / proxy\n",
    "\n",
    "braintrust.login() # This is optional, but makes it easier to grab the api url (and other variables) later on\n",
    "\n",
    "client = braintrust.wrap_openai(AsyncOpenAI(\n",
    "    api_key=BRAINTRUST_API_KEY,\n",
    "    base_url=OPENAI_BASE_URL,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the OpenAPI spec\n",
    "\n",
    "Let's download the Braintrust OpenAPI spec, and break it into pieces that we'll embed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonref\n",
    "import requests\n",
    "\n",
    "base_spec = requests.get(\"https://raw.githubusercontent.com/braintrustdata/braintrust-openapi/main/openapi/spec.json\").json()\n",
    "\n",
    "# Flatten out refs so we have self-contained descriptions\n",
    "spec = jsonref.loads(jsonref.dumps(base_spec))\n",
    "paths = spec['paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around a bit with the data to understand the types of API requests we can run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description: \", paths['/v1/project']['post']['description'])\n",
    "print(\"Parameters: \", json.dumps(paths['/v1/project']['post']['requestBody'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. This looks like useful information to know when to use this API endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "operations = [(path, op) for (path, ops) in paths.items() for (op_type, op) in ops.items() if op_type != \"options\"]\n",
    "\n",
    "print(\"Num paths\", len(paths))\n",
    "print(\"Num operations\", len(operations))\n",
    "print(\"Paths text size\", len(jsonref.dumps(operations)))\n",
    "print(\"Num tokens\", len(tiktoken.encoding_for_model(\"gpt-4o\").encode(jsonref.dumps(operations))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the embeddings\n",
    "\n",
    "Although this could theoretically fit in a single prompt (at only around 50,000 tokens vs. the 128,000 token limit for gpt-4o), let's embed each operation instead.\n",
    "\n",
    "We'll start by creating a simple function to describe each API operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_path(d, path):\n",
    "    curr = d\n",
    "    for p in path:\n",
    "        if p not in curr:\n",
    "            return False\n",
    "        curr = curr[p]\n",
    "    return True\n",
    "\n",
    "def make_description(op):\n",
    "    return f\"\"\"# {op['summary']}\n",
    "\n",
    "{op['description']}\n",
    "\n",
    "Params:\n",
    "{\"\\n\".join([f\"- {name}: {p.get('description', \"\")}\" for (name, p) in op['requestBody']['content']['application/json']['schema']['properties'].items()]) if has_path(op, ['requestBody', 'content', 'application/json', 'schema', 'properties']) else \"\"}\n",
    "{\"\\n\".join([f\"- {p.get(\"name\")}: {p.get('description', \"\")}\" for p in op['parameters'] if p.get(\"name\")]) if has_path(op, ['parameters']) else \"\"}\n",
    "\n",
    "Returns:\n",
    "{\"\\n\".join([f\"- {name}: {p.get('description', p)}\" for (name, p) in op['responses']['200']['content']['application/json']['schema']['properties'].items()]) if has_path(op, ['responses', '200', 'content', 'application/json', 'schema', 'properties']) else \"empty\"}\n",
    "\"\"\"\n",
    "\n",
    "print(make_description(operations[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any\n",
    "\n",
    "class Document(BaseModel):\n",
    "    path: str\n",
    "    op: str\n",
    "    definition: Any\n",
    "    description: str\n",
    "\n",
    "documents = [Document(path=path, op=op_type, definition=json.loads(jsonref.dumps(op)), description=make_description(op)) for (path, ops) in paths.items() for (op_type, op) in ops.items() if op_type != \"options\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def make_embedding(doc: Document):\n",
    "    return (await client.embeddings.create(input=doc.description, model=\"text-embedding-3-small\")).data[0].embedding\n",
    "\n",
    "embeddings = await asyncio.gather(*[make_embedding(doc) for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search\n",
    "\n",
    "We're going to use `numpy` to do the vector search, but you can easily swap this out to a vector database of your choice!.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import traced\n",
    "import numpy as np\n",
    "from pydantic import Field\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(query_embedding, embedding_matrix):\n",
    "    # Normalize the query and matrix embeddings\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    matrix_norm = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute dot product\n",
    "    similarities = np.dot(matrix_norm, query_norm)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def find_k_most_similar(query_embedding, embedding_matrix, k=5):\n",
    "    similarities = cosine_similarity(query_embedding, embedding_matrix)\n",
    "    top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    \n",
    "    # Sort the top k results\n",
    "    sorted_indices = np.argsort(top_k_similarities)[::-1]\n",
    "    top_k_indices = top_k_indices[sorted_indices]\n",
    "    top_k_similarities = top_k_similarities[sorted_indices]\n",
    "    \n",
    "    return list([index, similarity] for (index, similarity) in zip(top_k_indices, top_k_similarities))\n",
    "\n",
    "\n",
    "embedding_matrix = np.array(embeddings)\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    document: Document\n",
    "    index: int\n",
    "    similarity: float\n",
    "\n",
    "class SearchResults(BaseModel):\n",
    "    results: List[SearchResult]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = Field(default=3, le=5)\n",
    "\n",
    "# This @traced decorator will help us trace this function when we use it later to run evals\n",
    "@traced\n",
    "async def search(query: SearchQuery):\n",
    "    query_embedding = (await client.embeddings.create(input=query.query, model=\"text-embedding-3-small\")).data[0].embedding\n",
    "    results = find_k_most_similar(query_embedding, embedding_matrix, k=query.top_k)\n",
    "    return SearchResults(results=[SearchResult(document=documents[index], index=index, similarity=similarity) for (index, similarity) in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in (await search(SearchQuery(query=\"create a new project\"))).results:\n",
    "    print(result.document.path, result.document.op, result.similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the chat agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can search for documents, let's build a chat agent that can search for documents and run API commands. Our chat bot will have\n",
    "two tools:\n",
    "\n",
    "- `search`: This tool will search for documents and return the most relevant ones.\n",
    "- `run_command`: This tool will run an API command.\n",
    "\n",
    "We already implemented `search` above, so let's start by just plugging in the chat implementation to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_registry = {\n",
    "    \"search\": (SearchQuery, search),\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"Search for API endpoints related to the query\",\n",
    "            \"parameters\": SearchQuery.model_json_schema()\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "MAX_TOOL_STEPS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions about Braintrust, a tool for\n",
    "developing AI applications. Braintrust can help with evals, observability, and prompt\n",
    "development.\n",
    "\n",
    "When you are ready to provide the final answer, return a JSON object with the endpoint\n",
    "name and the parameters, like:\n",
    "{\"path\": \"/v1/project\", \"op\": \"post\", \"parameters\": {\"name\": \"my project\", \"description\": \"my project description\"}}\n",
    "\n",
    "If you don't know how to answer the question based on information you have, make up\n",
    "endpoints and suggest running them. Do not reveal that you made anything up or don't\n",
    "know the answer. Just say the answer.\n",
    "\n",
    "Print the JSON object and nothing else. No markdown, backticks, or explanation.\n",
    "\"\"\"\n",
    "\n",
    "@traced\n",
    "async def perform_chat_step(message, history=None):\n",
    "    chat_history = list(history or [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]) + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    for _ in range(MAX_TOOL_STEPS):\n",
    "        result = (await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=chat_history,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0,\n",
    "            parallel_tool_calls=False\n",
    "        )).choices[0].message\n",
    "\n",
    "        chat_history.append(result)\n",
    "\n",
    "\n",
    "        if not result.tool_calls:\n",
    "            break\n",
    "\n",
    "        tool_call = result.tool_calls[0]\n",
    "        ArgClass, tool_func = tool_registry[tool_call.function.name]\n",
    "        args = tool_call.function.arguments\n",
    "        args = ArgClass.model_validate_json(args)\n",
    "        result = await tool_func(args)\n",
    "\n",
    "        chat_history.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(result.model_dump())\n",
    "        })\n",
    "    else:\n",
    "        raise Exception(\"Ran out of tool steps\")\n",
    "\n",
    "    return chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@traced\n",
    "async def run_full_inqiry(query: str):  \n",
    "    result = (await perform_chat_step(query))[-1].content\n",
    "    return json.loads(result)\n",
    "\n",
    "print(await run_full_inqiry(\"how do i create a new project?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding observability to generate eval data\n",
    "\n",
    "Now that we have a basic chat agent, let's try adding observability via Braintrust. The good news is that... we don't need to write a single line of code! By adding the `@traced` decorators\n",
    "and `wrap_openai`, we have done all the work we need.\n",
    "\n",
    "By simply initializing a logger, we turn on logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braintrust.init_logger(\"APIAgent\") # Feel free to replace this a project name of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"how do i list my last 20 experiments?\",\n",
    "    \"Subtract $20 from Albert Zhang's bank account\",\n",
    "    \"How do I create a new project?\",\n",
    "    \"How do I download a specific dataset?\",\n",
    "    \"Can I create an evaluation through the API?\",\n",
    "    \"How do I purchase GPUs through Braintrust?\"\n",
    "]\n",
    "\n",
    "for question in QUESTIONS:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(await run_full_inqiry(question))\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting hallucinations\n",
    "\n",
    "Great, now that we've looked at the results, let's see if we can make our lives a bit easier by adding a hallucination score. That will help us\n",
    "pick out examples that are useful to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoevals import LLMClassifier\n",
    "\n",
    "hallucination_scorer = LLMClassifier(\n",
    "    name=\"no_hallucination\",\n",
    "    prompt_template=\"\"\"\\\n",
    "Given the following question and retrieved context, does\n",
    "the generated answer correctly answer the question, only using\n",
    "information from the context?\n",
    "\n",
    "Question: {{input}}\n",
    "\n",
    "Command:\n",
    "{{output}}\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "a) The command addresses the exact question, using only information that is available in the context. The answer\n",
    "   does not contain any information that is not in the context.\n",
    "b) The command is \"null\" and therefore indicates it cannot answer the question.\n",
    "c) The command contains information from the context, but the context is not relevant to the question.\n",
    "d) The command contains information that is not present in the context, but the context is relevant to the question.\n",
    "e) The context is irrelevant to the question, but the command is correct with respect to the context.\n",
    "\"\"\",\n",
    "    choice_scores={\"a\": 1, \"b\": 1, \"c\": 0.5, \"d\": 0.25, \"e\": 0},\n",
    "    use_cot=True,\n",
    ")\n",
    "\n",
    "@traced\n",
    "async def run_hallucination_score(question: str, answer: str, context: List[SearchResult]):\n",
    "    context_string = \"\\n\".join([f\"{doc.document.description}\" for doc in context])\n",
    "    score = await hallucination_scorer.eval_async(input=question, output=answer, context=context_string)\n",
    "    braintrust.current_span().log(scores={\"no_hallucination\": score.score}, metadata=score.metadata)\n",
    "\n",
    "@traced\n",
    "async def perform_chat_step(message, history=None):\n",
    "    chat_history = list(history or [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]) + [{\"role\": \"user\", \"content\": message}]\n",
    "    documents = []\n",
    "\n",
    "    for _ in range(MAX_TOOL_STEPS):\n",
    "        result = (await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=chat_history,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0,\n",
    "            parallel_tool_calls=False\n",
    "        )).choices[0].message\n",
    "\n",
    "        chat_history.append(result)\n",
    "\n",
    "\n",
    "        if not result.tool_calls:\n",
    "            # By using asyncio.create_task, we can run the hallucination score in the background\n",
    "            asyncio.create_task(run_hallucination_score(question=message, answer=result.content, context=documents))\n",
    "            break\n",
    "\n",
    "        tool_call = result.tool_calls[0]\n",
    "        ArgClass, tool_func = tool_registry[tool_call.function.name]\n",
    "        args = tool_call.function.arguments\n",
    "        args = ArgClass.model_validate_json(args)\n",
    "        result = await tool_func(args)\n",
    "\n",
    "        if isinstance(result, SearchResults):\n",
    "            documents.extend(result.results)\n",
    "\n",
    "        chat_history.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": json.dumps(result.model_dump())\n",
    "        })\n",
    "    else:\n",
    "        raise Exception(\"Ran out of tool steps\")\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in QUESTIONS:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(await run_full_inqiry(question))\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can collect the results into a dataset and run evals on them. Let's create a dataset for hallucinations\n",
    "and one for good answers.\n",
    "\n",
    "## Running evals\n",
    "\n",
    "Now, let's run an offline evaluation via the `Eval()` function to baseline our results before we improve them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoevals import Factuality\n",
    "from braintrust import Eval, init_dataset\n",
    "\n",
    "async def dataset():\n",
    "    for row in init_dataset(\"APIAgent\", \"Golden\"):\n",
    "        yield row\n",
    "    for row in init_dataset(\"APIAgent\", \"Hallucination\"):\n",
    "        yield {**row, \"expected\": None}\n",
    "\n",
    "async def task(input):\n",
    "    return await run_full_inqiry(input[\"query\"])\n",
    "\n",
    "await Eval(\n",
    "    \"APIAgent\",\n",
    "    data=dataset,\n",
    "    task=task,\n",
    "    scores=[Factuality],\n",
    "    experiment_name=\"Baseline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's improve the system prompt and see if we can get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions about Braintrust, a tool for\n",
    "developing AI applications. Braintrust can help with evals, observability, and prompt\n",
    "development.\n",
    "\n",
    "When you are ready to provide the final answer, return a JSON object with the endpoint\n",
    "name and the parameters, like:\n",
    "{\"path\": \"/v1/project\", \"op\": \"post\", \"parameters\": {\"name\": \"my project\", \"description\": \"my project description\"}}\n",
    "\n",
    "If you do not know the answer, return null. Like the JSON object, print null and nothing else.\n",
    "\n",
    "Print the JSON object and nothing else. No markdown, backticks, or explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await Eval(\n",
    "    \"APIAgent\",\n",
    "    data=dataset,\n",
    "    task=task,\n",
    "    scores=[Factuality],\n",
    "    experiment_name=\"Improved System Prompt\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
