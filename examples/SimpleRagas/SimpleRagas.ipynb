{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ragas metrics in Braintrust to measure and improve RAG performance\n",
    "\n",
    "Ragas is a popular framework for evaluating Retrieval Augmented Generation (RAG) applications, and were recently included in Braintrust's `autoevals` package, with improvements. You can read more about our deep dive into Ragas metrics [here](https://braintrust-git-Ragas-braintrustdata.vercel.app/docs/cookbook/Ragas-Retrieval). For this cookbook example, we're going to build a simple RAG application and show how to compute Ragas metrics.\n",
    "\n",
    "For data, we're going to use the Q&A for Coda that we built on in our [previous notebook](https://www.braintrustdata.com/docs/cookbook/CodaHelpDesk), but you don't need to have completed it - we'll build everything up from scratch in the first few lines!\n",
    "\n",
    "First, install dependancies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U autoevals braintrust openai scipy lancedb markdownify --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the RAG application\n",
    "\n",
    "We'll quickly set up a full end-to-end RAG application, based on our earlier [cookbook](https://www.braintrustdata.com/docs/cookbook/CodaHelpDesk). We use the Coda Q&A dataset, LanceDB for our vector database, and OpenAI's textual embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODA_QA_FILE_LOC = \"https://gist.githubusercontent.com/wong-codaio/b8ea0e087f800971ca5ec9eef617273e/raw/39f8bd2ebdecee485021e20f2c1d40fd649a4c77/articles.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import lancedb\n",
    "\n",
    "import braintrust\n",
    "import markdownify\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "NUM_SECTIONS = 20\n",
    "\n",
    "braintrust.login(\n",
    "    api_key=os.environ.get(\"BRAINTRUST_API_KEY\", \"Your BRAINTRUST_API_KEY here\")\n",
    ")\n",
    "\n",
    "openai_client = braintrust.wrap_openai(\n",
    "    openai.AsyncOpenAI(\n",
    "        base_url=\"https://braintrustproxy.com/v1\",\n",
    "        default_headers={\"x-bt-use-cache\": \"always\"},\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\", \"Your OPENAI_API_KEY here\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "coda_qa_content_data = requests.get(CODA_QA_FILE_LOC).json()\n",
    "\n",
    "markdown_sections = [\n",
    "    {\"doc_id\": row[\"id\"], \"markdown\": section.strip()}\n",
    "    for row in coda_qa_content_data\n",
    "    for section in re.split(r\"(.*\\n=+\\n)\", markdownify.markdownify(row[\"body\"]))\n",
    "    if section.strip() and not re.match(r\".*\\n=+\\n\", section)\n",
    "]\n",
    "\n",
    "\n",
    "LANCE_DB_PATH = os.path.join(tempfile.TemporaryDirectory().name, \"docs-lancedb\")\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def embed_text(text: str):\n",
    "    params = dict(input=text, model=\"text-embedding-3-small\")\n",
    "    response = await openai_client.embeddings.create(**params)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "\n",
    "embeddings = await asyncio.gather(\n",
    "    *(embed_text(section[\"markdown\"]) for section in markdown_sections)\n",
    ")\n",
    "\n",
    "db = lancedb.connect(LANCE_DB_PATH)\n",
    "table = db.create_table(\n",
    "    \"sections\",\n",
    "    data=[\n",
    "        {\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"section_id\": i,\n",
    "            \"markdown\": row[\"markdown\"],\n",
    "            \"vector\": embedding,\n",
    "        }\n",
    "        for i, (row, embedding) in enumerate(\n",
    "            zip(markdown_sections[:NUM_SECTIONS], embeddings)\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "table.count_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "Now we will write some simple, framework-free code to quickly retrieve relevant documents and answer questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "from textwrap import dedent\n",
    "\n",
    "QA_ANSWER_MODEL = \"gpt-3.5-turbo\"\n",
    "TOP_K = 2\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def fetch_top_k_relevant_sections(input: str) -> List[str]:\n",
    "    embedding = await embed_text(input)\n",
    "    results = table.search(embedding).limit(TOP_K).to_arrow().to_pylist()\n",
    "    return [result[\"markdown\"] for result in results]\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def generate_answer_from_docs(question: str, relevant_sections: Iterable[str]):\n",
    "    context = \"\\n\\n\".join(relevant_sections)\n",
    "    completion = await openai_client.chat.completions.create(\n",
    "        model=QA_ANSWER_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": dedent(\n",
    "                    f\"\"\"\\\n",
    "            Given the following context\n",
    "            {context}\n",
    "            Please answer the following question:\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can answer a question through our RAG pipeline, first by querying our vector DB for the relevant documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Not all Coda docs are used in the same way. You'll inevitably have a few that you use every week, and some that you'll only use once. This is where starred docs can help you stay organized.\\n\\n\\n\\nStarring docs is a great way to mark docs of personal importance. After you star a doc, it will live in a section on your doc list called **[My Shortcuts](https://coda.io/shortcuts)**. All starred docs, even from multiple different workspaces, will live in this section.\\n\\n\\n\\nStarring docs only saves them to your personal My Shortcuts. It doesn’t affect the view for others in your workspace. If you’re wanting to shortcut docs not just for yourself but also for others in your team or workspace, you’ll [use pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) instead.\",\n",
       " 'When should I star a doc and when should I pin it?\\n--------------------------------------------------\\n\\n\\n\\nStarring docs is best for docs of *personal* importance. Starred docs appear in your **My Shortcuts**, but they aren’t starred for anyone else in your workspace. For instance, you may want to star your personal to-do list doc or any docs you use on a daily basis.\\n\\n\\n\\n[Pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) is recommended when you want to flag or shortcut a doc for *everyone* in your workspace or folder. For instance, you likely want to pin your company wiki doc to your workspace. And you may want to pin your team task tracker doc to your team’s folder.\\n\\n\\n\\nCan I star docs for everyone?\\n-----------------------------\\n\\n\\n\\nStarring docs only applies to your own view and your own My Shortcuts. To pin docs (or templates) to your workspace or folder, [refer to this article](https://help.coda.io/en/articles/2865511-starred-pinned-docs).\\n\\n\\n\\n\\n\\n---']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = (\n",
    "    \"What impact does starring a document have on other workspace members in Coda?\"\n",
    ")\n",
    "\n",
    "relevant_sections = await fetch_top_k_relevant_sections(question)\n",
    "\n",
    "relevant_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then passing the relevant documents to the LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starring a document in Coda only affects the individual who starred it. It does not impact other workspace members, as the star only saves the document to the individual's personal My Shortcuts section. Other workspace members will not see the document as starred in their own views.\n"
     ]
    }
   ],
   "source": [
    "answer = await generate_answer_from_docs(question, relevant_sections)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a quick convenience function to combine these two steps, and return both the final answer and the retrieved documents so we can observe if we picked useful documents! (Later, returning documents will come in useful for evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starring a document in Coda only affects the individual who starred it. It does not impact other workspace members, as the star only saves the document to the individual's personal My Shortcuts section. Other workspace members will not see the document as starred in their own views.\n"
     ]
    }
   ],
   "source": [
    "from braintrust import EvalHooks\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def generate_answer_e2e(question: str):\n",
    "    retrieved_content = await fetch_top_k_relevant_sections(question)\n",
    "    answer = await generate_answer_from_docs(question, retrieved_content)\n",
    "    return dict(answer=answer, retrieved_docs=retrieved_content)\n",
    "\n",
    "\n",
    "e2e_answer = await generate_answer_e2e(question)\n",
    "print(e2e_answer.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now that we have the whole system working, we can compute Ragas metrics and try a couple improvements\n",
    "\n",
    "## Baseline Ragas metrics with autoeval\n",
    "\n",
    "To get a large enough sample size for evaluations, we're going to use the synthetic test questions we generated in [our earlier cookbook](https://www.braintrust.dev/docs/cookbook/CodaHelpDesk). Feel free to check out that cookbook if you're interested in synthetic data generation, otherwise, you can just load them from this file location:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODA_QA_PAIRS_LOC = \"https://gist.githubusercontent.com/nelsonauner/2ef4d38948b78a9ec2cff4aa265cff3f/raw/c47306b4469c68e8e495f4dc050f05aff9f997e1/qa_pairs_coda_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the purpose of starred docs in Coda?',\n",
       " 'expected': 'Starring docs in Coda helps to mark documents of personal importance and organizes them in a specific section called My Shortcuts for easy access.',\n",
       " 'metadata': {'document_id': '8179780',\n",
       "  'section_id': 0,\n",
       "  'question_idx': 0,\n",
       "  'answer_idx': 0,\n",
       "  'id': 0,\n",
       "  'split': 'train'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "coda_qa_pairs = requests.get(CODA_QA_PAIRS_LOC)\n",
    "\n",
    "qa_pairs = [json.loads(line) for line in coda_qa_pairs.text.split(\"\\n\") if line]\n",
    "\n",
    "qa_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas provides a variety of metrics - you can read an overview of each metric at the [Ragas metrics overview](https://docs.ragas.io/en/stable/concepts/metrics/index.html), but for the purposes of this guide, we'll show you how to calculate two scores we've found to be useful: Answer Correctness, which compares your system's answer to provided golden answer, and Context Recall, which compares the retrieved context to the information in the provided golden answer.\n",
    "\n",
    "Before we calculate metrics, we'll write a short wrapper class that splits the returned output and context into two arguments that our Ragas evaluator classes can easily ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoevals import ContextRecall, AnswerCorrectness\n",
    "\n",
    "\n",
    "class RAGScorerWrapper:\n",
    "    \"\"\"This wrapper passes on retrieved_docs to the scorer's eval_async method via the context arg\"\"\"\n",
    "\n",
    "    def __new__(cls, scorer_class):\n",
    "        async def eval_async(self, output, **kwargs):\n",
    "            return await super(RAGWrappedScorer, self).eval_async(\n",
    "                output=output[\"answer\"],\n",
    "                context=output[\"retrieved_docs\"],\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        RAGWrappedScorer = type(\n",
    "            scorer_class.__name__, (scorer_class,), {\"eval_async\": eval_async}\n",
    "        )\n",
    "        return RAGWrappedScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run our evaluation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Using gpt-3.5-turbo is running at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-3.5-turbo\n",
      "Coda RAG with ragas [experiment_name=Using gpt-3.5-turbo] (data): 20it [00:00, 53025.34it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956aa08b6805436b9724b571ec60e54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Coda RAG with ragas [experiment_name=Using gpt-3.5-turbo] (tasks):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "90.45% 'ContextRecall'     score\n",
      "68.23% 'AnswerCorrectness' score\n",
      "\n",
      "2.22s duration\n",
      "\n",
      "See results for Using gpt-3.5-turbo at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Eval\n",
    "\n",
    "eval_result = await Eval(\n",
    "    name=\"Coda RAG with ragas\",\n",
    "    experiment_name=f\"Using {QA_ANSWER_MODEL}\",\n",
    "    data=qa_pairs[:20],\n",
    "    task=generate_answer_e2e,\n",
    "    scores=[RAGScorerWrapper(AnswerCorrectness), RAGScorerWrapper(ContextRecall)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty of ragas is that it can be complex to understand how metrics were generated for a given datapoint, but with Braintrust we see each each constituent metric, all submetrics, and go down to the actual LLM calls used to generate the metric components.\n",
    "\n",
    "For example, here, we can see that `AnswerCorrectness` is computed by taking both a factuality and answer similarity score, and can examine the subscore computations - in this case, `factuality_score` and `answer_similarity_score`\n",
    "\n",
    "![ ragas metric computation ](assets/ragas_metric_computation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating changes\n",
    "\n",
    "Now that we have our end-to-end system, and know how to use Braintrust to examine the results, let's make some small changes and evaluate if they helped.\n",
    "\n",
    "#### Swapping LLMs\n",
    "\n",
    "Up to this point, we've used GPT-3.5 to answer questions, but do we observe performance improvements when using GPT-4 instead?\n",
    "Let's name a new experiment and rerun the evaluations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Using gpt-4-turbo is running at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-4-turbo\n",
      "Coda RAG with ragas [experiment_name=Using gpt-4-turbo] (data): 20it [00:00, 56527.01it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44eb59b84694cb19af9b8869d592741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Coda RAG with ragas [experiment_name=Using gpt-4-turbo] (tasks):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Using gpt-4-turbo compared to Using gpt-3.5-turbo:\n",
      "74.10% (+05.87%) 'AnswerCorrectness' score\t(13 improvements, 7 regressions)\n",
      "90.45% (-) 'ContextRecall'     score\t(0 improvements, 0 regressions)\n",
      "\n",
      "1.71s (-51.83%) 'duration'\t(20 improvements, 0 regressions)\n",
      "\n",
      "See results for Using gpt-4-turbo at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-4-turbo\n"
     ]
    }
   ],
   "source": [
    "QA_ANSWER_MODEL = \"gpt-4-turbo\"\n",
    "\n",
    "\n",
    "eval_result = await Eval(\n",
    "    name=\"Coda RAG with ragas\",\n",
    "    experiment_name=f\"Using {QA_ANSWER_MODEL}\",\n",
    "    data=qa_pairs[:20],\n",
    "    task=generate_answer_e2e,\n",
    "    scores=[RAGScorerWrapper(AnswerCorrectness), RAGScorerWrapper(ContextRecall)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like changing our LLM model increased AnswerCorrectness while maintaining the same ContextRecall. Both of these results make intuitive sense: We'd expect AnswerCorrectness to increase with a better model, and ContextRecall doesn't use an LLM at all, so it shouldn't be affected (Interested in LLM-powered document selection? See our other RAG [notebook](https://www.braintrustdata.com/docs/cookbook/CodaHelpDesk))\n",
    "\n",
    "#### Reducing document retrieval\n",
    "\n",
    "Now, let's see if we can get away with only pulling a single document per question, instead of the two we've been fetching up to this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Using gpt-4-turbo, TOP_K=1 is running at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-4-turbo%2C%20TOP_K%3D1\n",
      "Coda RAG with ragas [experiment_name=Using gpt-4-turbo, TOP_K=1] (data): 20it [00:00, 46269.21it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ce266594ea47a584473ba7ce33f4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Coda RAG with ragas [experiment_name=Using gpt-4-turbo, TOP_K=1] (tasks):   0%|          | 0/20 [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Using gpt-4-turbo, TOP_K=1 compared to Using gpt-4-turbo:\n",
      "97.29% (+06.84%) 'ContextRecall'     score\t(2 improvements, 3 regressions)\n",
      "59.19% (-14.91%) 'AnswerCorrectness' score\t(6 improvements, 14 regressions)\n",
      "\n",
      "1.70s (-00.95%) 'duration'\t(12 improvements, 8 regressions)\n",
      "\n",
      "See results for Using gpt-4-turbo, TOP_K=1 at https://www.braintrust.dev/app/braintrustdata.com/p/Coda%20RAG%20with%20ragas/experiments/Using%20gpt-4-turbo%2C%20TOP_K%3D1\n"
     ]
    }
   ],
   "source": [
    "TOP_K = 1\n",
    "\n",
    "\n",
    "eval_result = await Eval(\n",
    "    name=\"Coda RAG with ragas\",\n",
    "    experiment_name=f\"Using {QA_ANSWER_MODEL}, TOP_K={TOP_K}\",\n",
    "    data=qa_pairs[:20],\n",
    "    task=generate_answer_e2e,\n",
    "    scores=[RAGScorerWrapper(AnswerCorrectness), RAGScorerWrapper(ContextRecall)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing our retrieval step from two to one documents per question decreased our overall answer quality.\n",
    "Jumping into braintrust's UI, we can see the comparison here:\n",
    "\n",
    "![ ragas metric computation ](assets/ragas_triple_comparison.png)\n",
    "\n",
    "We can also easily drill down on examples. Here we identify a question that had a drastic score difference when we used two vs. one documents per answer. Looking at the diff view between runs, we see that the second document pulled provide context to completely change the answer from a \"No\" to a \"Yes\", increasing the AnswerCorrectness metric from .15 to .58.\n",
    "\n",
    "![example diff](assets/example_qa_different.png)\n",
    "\n",
    "And there you have it! We hope you found this cookbook useful for quickly getting ragas metrics implemented in your LLM app\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
