{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Chat Bot\n",
    "\n",
    "This tutorial will walk through using Braintrust to evaluate a conversational, multi-turn chat assistant.\n",
    "\n",
    "These types of chat bots have become very important parts of applications, acting as customer service agents, sales representatives, or travel agents, to name a few. As an owner of such an application, it's important to be sure the bot provides value to the user.\n",
    "\n",
    "We will expand on this below, but the history and context of a conversation is usually crucial in being able to produce a good response. If you received a request to \"Make a dinner reservation at 7pm\" and you knew the context of where, on what date, and for how many people, you could provide some assistance; otherwise, you'd need to ask for more information. \n",
    "\n",
    "Before starting, please make sure that you have a Braintrust account. If you do not have one, you can [sign up here](https://www.braintrustdata.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "Begin by installing the necessary dependencies if you have not done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "npm install autoevals braintrust openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the data\n",
    "\n",
    "Let's take a look at the small dataset prepared for this cookbook. You can find the full dataset in the accompanying [dataset.ts](https://github.com/braintrustdata/braintrust-cookbook/tree/main/examples/EvaluatingCustomerSupportAgent/assets/dataset.ts) file.\n",
    "\n",
    "Below is an example of a data point.\n",
    "- `chat_history` contains the history of the conversation between the user and the assistant\n",
    "- `input` is the last `user` turn that will be sent to the completion\n",
    "- `expected` is the output expected given the input\n",
    "\n",
    "From looking at this one example, it is clear why we need the history to be able to provide a helpful response.\n",
    "\n",
    "If you were asked \"who won the men's trophy that year?\", you would wonder *What trophy? Which year?*. But if you read the `chat_history`, you'd be able to answer the question (maybe after some quick research)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const dataset = [\n",
    "  {\n",
    "    chat_history: [\n",
    "      { role: \"user\", content: \"when was the ballon d'or first awarded for female players?\" },\n",
    "      {\n",
    "        role: \"assistant\",\n",
    "        content:\n",
    "          `The Ballon d'Or Féminin, awarded to the best female football player in the world, was first introduced in 2018.\n",
    "          The inaugural winner was Ada Hegerberg of Norway, who played for Olympique Lyonnais at the time.`,\n",
    "      },\n",
    "    ],\n",
    "    input: \"who won the men's trophy that year?\",\n",
    "    expected: \"The men's Ballon d'Or trophy in 2018 was won by Luka Modrić of Croatia, who played for Real Madrid.\"\n",
    "  },\n",
    "  // ... rest of dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evals\n",
    "\n",
    "The key to running evals on a multi-turn conversation is to include the history of the chat in the chat completion request.\n",
    "\n",
    "To do so, we simply add the history between the `system` message and the final `user` message in the `messages` argument as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { wrapOpenAI } from \"braintrust\";\n",
    "import { OpenAI } from \"openai\";\n",
    "\n",
    "type ChatHistory = {\n",
    "  role: \"user\" | \"assistant\",\n",
    "  content: string\n",
    "}\n",
    "\n",
    "async function runTask({\n",
    "  input,\n",
    "  chat_history\n",
    "}: {\n",
    "  input: string,\n",
    "  chat_history: ChatHistory[]\n",
    "}) {\n",
    "  const client = wrapOpenAI(\n",
    "    new OpenAI({\n",
    "      baseURL: \"https://braintrustproxy.com/v1\",\n",
    "      apiKey: process.env.OPENAI_API_KEY ?? \"\", // Can use OpenAI, Anthropic, Mistral etc. API keys here\n",
    "    })\n",
    "  )\n",
    "\n",
    "  const response = await client.chat.completions.create({\n",
    "    model: \"gpt-4o\",\n",
    "    messages: [\n",
    "      {\n",
    "        role: \"system\",\n",
    "        content: \"You are a helpful and polite assistant who knows about sports.\"\n",
    "      },\n",
    "      ...chat_history,\n",
    "      {\n",
    "        role: \"user\",\n",
    "        content: input,\n",
    "      }\n",
    "    ]\n",
    "  });\n",
    "  return response.choices[0].message.content || \"\";\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the eval\n",
    "\n",
    "We'll use a `Factuality` scoring function to check how the output of the completion compares to the expected value.\n",
    "\n",
    "For this cookbook, we will alter the [built in factuality prompt](https://github.com/braintrustdata/autoevals/blob/main/templates/factuality.yaml) slightly.\n",
    "\n",
    "(I have used the built-in Factuality scorer and it does not significantly reduce the score compared to running the eval with no chat history. If the model does not have enough context to factually answer the question and responds with a request for the user to be more specific, the Factuality scorer evaluates this response as having no factual inconsistencies between the expert and submitted answers. Because this cookbook is to illustrate how to incorporate chat history to produce a good response, we will tweak the built-in prompt slightly to account for this.)\n",
    "\n",
    "In the [altered factuality spec](https://github.com/braintrustdata/braintrust-cookbook/tree/main/examples/EvaluatingCustomerSupportAgent/assets/factuality.ts), you can see that we have added two more score choices:\n",
    "- (F) The submitted answer asks for more context, specifics or clarification but provides factual information consistent with the expert answer.\n",
    "- (G) The submitted answer asks for more context, specifics or clarification but does not provide factual information consistent with the expert answer.\n",
    "\n",
    "These will score (F) = 0.2 and (G) = 0 so the model gets some credit if there was any context it was able to gather from the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Eval } from \"braintrust\";\n",
    "import dataset from \"./assets/dataset\"\n",
    "import factualitySpec from \"./assets/factuality\";\n",
    "import { LLMClassifierFromSpec } from \"autoevals\";\n",
    "\n",
    "function Factuality(args: {\n",
    "  input: {\n",
    "    input: string,\n",
    "    chat_history: ChatHistory[]\n",
    "  },\n",
    "  output: string,\n",
    "  expected: string\n",
    "}) {\n",
    "  const scorer = LLMClassifierFromSpec(\n",
    "    \"Factuality\",\n",
    "    {\n",
    "      ...factualitySpec,\n",
    "      model: \"gpt-4o\"\n",
    "    }\n",
    "  );\n",
    "  return scorer(args);\n",
    "}\n",
    "\n",
    "Eval(\"Chat assistant\", {\n",
    "  experimentName: \"gpt-4o assistant\",\n",
    "  data: () => dataset,\n",
    "  task: runTask,\n",
    "  scores: [Factuality],\n",
    "  metadata: {\n",
    "    model: \"gpt-4o\",\n",
    "    prompt: \"You are a helpful, polite assistant who knows about sports.\"\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment gpt - 4o assistant is running at http://www.braintrust.dev/app/braintrustdata.com/p/Chat%20assistant/experiments/gpt-4o%20assistant\n",
    " ████████████████████████████████████████ | Chat assistant[experimentName = gpt - 4o... | 100 % | 5 / 5 datapoints\n",
    "\n",
    "=========================SUMMARY=========================\n",
    "64.00% 'Factuality' score       (0 improvements, 0 regressions)\n",
    "\n",
    "4.28s 'duration'        (0 improvements, 0 regressions)\n",
    "\n",
    "See results for gpt-4o assistant at http://www.braintrust.dev/app/braintrustdata.com/p/Chat%20assistant/experiments/gpt-4o%20assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "\n",
    "Now that we've run the eval, we can look at the results on [braintrust.dev](https://www.braintrust.dev).\n",
    "\n",
    "Open the newly created `Chat assistant` project and you can see the experiment we just ran:\n",
    "\n",
    "![dist-chart](./assets/assistant-dist.png)\n",
    "\n",
    "The model scored 64% on Factuality, but we can see from the chart that the model responses got a lot of partial credit (as opposed to a few perfect responses and a few completely incorrect responses). The responses were either exactly the same as the expected answer or a subset/superset but still fully factually consistent with the expected answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to a model with no context\n",
    "\n",
    "To illustrate how the model needs the chat_history to produce a good response, let's remove that argument from the completions request.\n",
    "\n",
    "Comment out or delete the line `...chat_history` from the `client.chat.completions.create(...)` request in the task function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const response = await client.chat.completions.create({\n",
    "  model: \"gpt-4o\",\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: \"You are a helpful and polite assistant who knows about sports.\"\n",
    "    },\n",
    "    // ...chat_history,\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: input,\n",
    "    }\n",
    "  ]\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the eval again in a new `gpt-4o assistant - no history` experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval(\"Chat assistant\", {\n",
    "  experimentName: \"gpt-4o assistant - no history\",\n",
    "  data: () => dataset,\n",
    "  task: runTask,\n",
    "  scores: [Factuality],\n",
    "  metadata: {\n",
    "    model: \"gpt-4o\",\n",
    "    prompt: \"You are a helpful, polite assistant who knows about sports.\"\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment gpt - 4o assistant - no history is running at http://www.braintrust.dev/app/braintrustdata.com/p/Chat%20assistant/experiments/gpt-4o%20assistant%20-%20no%20history\n",
    " ████████████████████████████████████████ | Chat assistant[experimentName = gpt - 4o... | 100 % | 5 / 5 datapoints\n",
    "\n",
    "=========================SUMMARY=========================\n",
    "gpt-4o assistant - no history compared to gpt-4o assistant:\n",
    "4.00% (-60.00%) 'Factuality' score      (0 improvements, 5 regressions)\n",
    "\n",
    "4.14s 'duration'        (2 improvements, 3 regressions)\n",
    "\n",
    "See results for gpt-4o assistant - no history at http://www.braintrust.dev/app/braintrustdata.com/p/Chat%20assistant/experiments/gpt-4o%20assistant%20-%20no%20history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we were expecting, the Factuality score went way down, from 64% to just 4%.\n",
    "\n",
    "If we open [the dashboard](https://www.braintrust.dev), we can compare the results of the two experiments\n",
    "\n",
    "![dist-chart](./assets/assistant-dist-comp.png)\n",
    "\n",
    "For one datapoint the model was able to glean some context from the user's input and provide some information that was factually consistent with the expected answer, but for the majority the model needed the context and specificity that the `chat_history` provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hopefully it's clear at this point why a model needs context to produce helpful responses and why providing the chat history to the completions request is the strategy for running evals on chat assistant models.\n",
    "\n",
    "The experiment still only scores 64% on Factuality, so see what you can do with the model parameters or prompt to improve the results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
