{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an emotion classifier with OpenAI and Braintrust\n",
    "\n",
    "In this cookbook, we'll learn how to evaluate the precision and recall of a custom LLM classifier in Braintrust using custom scoring functions. For this cookbook, We'll use the [go_emotions dataset](https://huggingface.co/datasets/google-research-datasets/go_emotions), which contains Reddit comments labeled with 28 different emotions. What makes this dataset particularly interesting is that each comment can be labeled with multiple emotions - for example, a single message might express both \"excitement\" and \"anger\".\n",
    "\n",
    "We'll build two classifiers - a random baseline and an LLM-based approach using OpenAI's GPT-4o. By comparing their performance using custom scorers, we'll demonstrate how to effectively measure then improve your LLM's accuracy on complex classification tasks.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First, you'll need a two accounts:\n",
    "- [Braintrust](https://www.braintrust.dev/signup) - for running the LLM evaluations\n",
    "- [OpenAI](https://platform.openai.com/signup) - to use the GPT-4o model. Save your API key in Braintrust because we will be using the Braintrust proxy for this cookbook.\n",
    "\n",
    "\n",
    "![aiproviders.png](assets/aiproviders.png)\n",
    "\n",
    "Next, install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install braintrust openai datasets autoevals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Any, Set\n",
    "\n",
    "import openai\n",
    "import braintrust\n",
    "from datasets import load_dataset\n",
    "from autoevals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your Braintrust client:\n",
    "\n",
    "Replace `YOUR_API_KEY` with your Braintrust API key that you create in the Braintrust dashboard.\n",
    "\n",
    "<Callout type=\"info\">\n",
    " Best practice is to export your API key as an environment variable. However, to make it easier to follow along with this cookbook, we've included it directly in the code.\n",
    "</Callout>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BRAINTRUST_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "openai_client = braintrust.wrap_openai(\n",
    "    openai.OpenAI(\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",\n",
    "        api_key=os.environ[\"BRAINTRUST_API_KEY\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "First, we'll create helper functions to load the emotion labels and dataset. The `get_emotion_labels` function extracts the emotion column names from the dataset by filtering out metadata columns. This gives us our complete set of possible emotions that we can detect.\n",
    "\n",
    "The `load_data` function then processes the raw dataset into a format suitable for evaluation. For each comment, it:\n",
    "1. Extracts the text content\n",
    "2. Identifies which emotions are present (where the value is 1)\n",
    "3. Includes relevant metadata like the subreddit and author - Not necessary but included to see how meta data might appear in Braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 211225/211225 [00:00<00:00, 911049.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def get_emotion_labels() -> List[str]:\n",
    "    ds = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")\n",
    "\n",
    "    metadata_columns = {\n",
    "        \"text\",\n",
    "        \"id\",\n",
    "        \"author\",\n",
    "        \"subreddit\",\n",
    "        \"link_id\",\n",
    "        \"parent_id\",\n",
    "        \"created_utc\",\n",
    "        \"rater_id\",\n",
    "        \"example_very_unclear\",\n",
    "    }\n",
    "    return sorted(\n",
    "        col for col in ds[\"train\"].features.keys() if col not in metadata_columns\n",
    "    )\n",
    "\n",
    "\n",
    "EMOTIONS: List[str] = get_emotion_labels()\n",
    "\n",
    "\n",
    "def load_data(limit: int = 100):\n",
    "    ds = load_dataset(\"google-research-datasets/go_emotions\", \"raw\")\n",
    "    for i, item in enumerate(ds[\"train\"]):\n",
    "        if i >= limit:\n",
    "            break\n",
    "\n",
    "        actual_emotions = [emotion for emotion in EMOTIONS if item.get(emotion, 0) == 1]\n",
    "\n",
    "        yield {\n",
    "            \"input\": item[\"text\"],\n",
    "            \"expected\": actual_emotions,\n",
    "            \"metadata\": {\"subreddit\": item[\"subreddit\"], \"author\": item[\"author\"]},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the classifiers\n",
    "\n",
    "We'll implement two different approaches to emotion classification:\n",
    "\n",
    "1. A random baseline classifier that assigns 1-3 emotions randomly from our predefined list, which helps establish a minimum performance threshold and validates our custom scoring mechanisms.\n",
    "\n",
    "2. An LLM-based classifier using GPT-4o that analyzes the Reddit comments with a prompt containing possible emotions. This classifier returns emotions in a comma-separated format, and includes robust error handling for edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classifier(text: str) -> List[str]:\n",
    "    num_emotions = random.randint(1, 3)\n",
    "    return random.sample(EMOTIONS, num_emotions)\n",
    "\n",
    "\n",
    "def parse_llm_emotions(raw_response: str) -> List[str]:\n",
    "    raw_response = raw_response.strip().lower()\n",
    "\n",
    "    if \",\" in raw_response:\n",
    "        candidates = [e.strip() for e in raw_response.split(\",\")]\n",
    "    else:\n",
    "        candidates = [raw_response]\n",
    "\n",
    "    valid_emotions = [e for e in candidates if e in EMOTIONS and e != \"\"]\n",
    "    return valid_emotions\n",
    "\n",
    "\n",
    "def llm_classifier(text: str) -> List[str]:\n",
    "    prompt = (\n",
    "        f\"Please identify the emotions present in the following text.\\n\"\n",
    "        f\"Choose from these emotions: {', '.join(EMOTIONS)}\\n\"\n",
    "        f\"Multiple emotions may be present.\\n\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Return only the emotions list with no additional text or explanation.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        raw_response = resp.choices[0].message.content\n",
    "        return parse_llm_emotions(raw_response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[llm_classifier] Exception: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing evaluation metrics \n",
    "\n",
    "For multi-label classification tasks like emotion detection, we need specialized metrics that can handle multiple correct answers. We implement three key metrics:\n",
    "\n",
    "`Precision`: Measures how many of the identified emotions are actually correct. For example, if our system predicts \"joy\" and \"anger\" for a comment that only expresses \"joy\", we achieve a precision of 0.5 because only \"joy\" is correct.\n",
    "   - High precision means we rarely predict emotions that aren't present\n",
    "   - Important for applications where false positives are costly\n",
    "\n",
    "`Recall`: Measures whether we captured all the emotions that were truly present in the Reddit comments. For instance, if a comment expresses both \"sadness\" and \"fear\" but we only identify \"sadness,\" we get a recall of 0.5. This is fundamentally different from precision because it tells us if we missed emotions that should have been identified.\n",
    "   - High recall means we rarely miss emotions that are present\n",
    "   - Important for applications where false negatives are costly\n",
    "\n",
    "`F1 Score`: The F1 Score balances precision and recall by calculating their harmonic mean, which is necessary because optimizing for one often comes at the expense of the other. If our LLM achieves high precision, it might be too conservative and miss emotions (low recall). Conversely, if it achieves high recall, it might overidentify emotions (low precision). The F1 score gives us a single metric to evaluate our LLM's overall performance in identifying emotions in Reddit comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_precision(_: str, expected: List[str], output: List[str]) -> float:\n",
    "    expected_set: Set[str] = set(expected)\n",
    "    output_set: Set[str] = set(output)\n",
    "\n",
    "    true_positives: int = len(expected_set & output_set)\n",
    "    false_positives: int = len(output_set - expected_set)\n",
    "\n",
    "    denominator = true_positives + false_positives\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return true_positives / denominator\n",
    "\n",
    "\n",
    "def emotion_recall(_: str, expected: List[str], output: List[str]) -> float:\n",
    "    expected_set: Set[str] = set(expected)\n",
    "    output_set: Set[str] = set(output)\n",
    "\n",
    "    true_positives: int = len(expected_set & output_set)\n",
    "    false_negatives: int = len(expected_set - output_set)\n",
    "\n",
    "    denominator = true_positives + false_negatives\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return true_positives / denominator\n",
    "\n",
    "\n",
    "def emotion_f1(_: str, expected: List[str], output: List[str]) -> float:\n",
    "    prec = emotion_precision(\"\", expected, output)\n",
    "    rec = emotion_recall(\"\", expected, output)\n",
    "\n",
    "    if (prec + rec) == 0:\n",
    "        return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "Finally, let's set up our evaluation pipeline using Braintrust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(num_samples: int = 100):\n",
    "    # Random classifier experiment\n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook-jp\",  # The name of your project in Braintrust\n",
    "        data=lambda: load_data(limit=num_samples),\n",
    "        task=random_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"random\"},\n",
    "        experiment_name=\"random-classifier\",\n",
    "    )\n",
    "\n",
    "    # LLM classifier experiment\n",
    "    braintrust.Eval(\n",
    "        \"emotion-classification-cookbook-jp\",\n",
    "        data=lambda: load_data(limit=num_samples),\n",
    "        task=llm_classifier,\n",
    "        scores=[emotion_precision, emotion_recall, emotion_f1],\n",
    "        metadata={\"classifier_type\": \"llm\", \"model\": \"gpt-4o\"},\n",
    "        experiment_name=\"llm-classifier\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluations(num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Once you run the evaluations, you'll see the results in your Braintrust dashboard. The LLM classifier should significantly outperform the random baseline across all metrics.\n",
    "\n",
    "![results.png](assets/results.png)\n",
    "\n",
    "Key features to examine:\n",
    "- Compare precision and recall scores between our runs\n",
    "- Look at specific examples where the LLM fails \n",
    "- Analyze cases where multiple emotions are present\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "There are several ways to improve this emotion classifier:\n",
    "- Experiment with different prompts and instructions. Maybe even a series of prompts?\n",
    "- Try other models like Grok 2 or OpenAI's o1.\n",
    "- Add more sophisticated scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
