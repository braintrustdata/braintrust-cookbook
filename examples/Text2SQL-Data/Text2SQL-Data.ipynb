{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quNDJhiCJ-AI"
   },
   "source": [
    "# LLM Eval For Text2SQL\n",
    "\n",
    "![eval framework](./assets/eval_framework.png)\n",
    "\n",
    "Before starting, please make sure that you have a Braintrust account. If you do not, please [sign up](https://braintrust.dev).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tMYg0jyKelb"
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "The next few commands will install some libraries and include some helper code for the text2sql application. Feel free to copy/paste/tweak/reuse this code in your own tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGuPB2SqUWdZ"
   },
   "outputs": [],
   "source": [
    "%pip install -U autoevals braintrust duckdb datasets openai pyarrow pydantic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fajc2XybK_gt"
   },
   "source": [
    "### Downloading the data\n",
    "\n",
    "We're going to use an NBA dataset that includes information about games from 2014-2018. Let's start by downloading it and poking around.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load it in DuckDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Team</th>\n",
       "      <th>Game</th>\n",
       "      <th>Date</th>\n",
       "      <th>Home</th>\n",
       "      <th>Opponent</th>\n",
       "      <th>WINorLOSS</th>\n",
       "      <th>TeamPoints</th>\n",
       "      <th>OpponentPoints</th>\n",
       "      <th>FieldGoals</th>\n",
       "      <th>...</th>\n",
       "      <th>Opp.FreeThrows</th>\n",
       "      <th>Opp.FreeThrowsAttempted</th>\n",
       "      <th>Opp.FreeThrows.</th>\n",
       "      <th>Opp.OffRebounds</th>\n",
       "      <th>Opp.TotalRebounds</th>\n",
       "      <th>Opp.Assists</th>\n",
       "      <th>Opp.Steals</th>\n",
       "      <th>Opp.Blocks</th>\n",
       "      <th>Opp.Turnovers</th>\n",
       "      <th>Opp.TotalFouls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>10/29/14</td>\n",
       "      <td>Away</td>\n",
       "      <td>TOR</td>\n",
       "      <td>L</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>0.818</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2</td>\n",
       "      <td>11/1/14</td>\n",
       "      <td>Home</td>\n",
       "      <td>IND</td>\n",
       "      <td>W</td>\n",
       "      <td>102</td>\n",
       "      <td>92</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>0.857</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "      <td>3</td>\n",
       "      <td>11/5/14</td>\n",
       "      <td>Away</td>\n",
       "      <td>SAS</td>\n",
       "      <td>L</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>0.711</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "      <td>4</td>\n",
       "      <td>11/7/14</td>\n",
       "      <td>Away</td>\n",
       "      <td>CHO</td>\n",
       "      <td>L</td>\n",
       "      <td>119</td>\n",
       "      <td>122</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>0.741</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ATL</td>\n",
       "      <td>5</td>\n",
       "      <td>11/8/14</td>\n",
       "      <td>Home</td>\n",
       "      <td>NYK</td>\n",
       "      <td>W</td>\n",
       "      <td>103</td>\n",
       "      <td>96</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.727</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Team  Game      Date  Home Opponent WINorLOSS  TeamPoints  \\\n",
       "0           1  ATL     1  10/29/14  Away      TOR         L         102   \n",
       "1           2  ATL     2   11/1/14  Home      IND         W         102   \n",
       "2           3  ATL     3   11/5/14  Away      SAS         L          92   \n",
       "3           4  ATL     4   11/7/14  Away      CHO         L         119   \n",
       "4           5  ATL     5   11/8/14  Home      NYK         W         103   \n",
       "\n",
       "   OpponentPoints  FieldGoals  ...  Opp.FreeThrows  Opp.FreeThrowsAttempted  \\\n",
       "0             109          40  ...              27                       33   \n",
       "1              92          35  ...              18                       21   \n",
       "2              94          38  ...              27                       38   \n",
       "3             122          43  ...              20                       27   \n",
       "4              96          33  ...               8                       11   \n",
       "\n",
       "   Opp.FreeThrows.  Opp.OffRebounds  Opp.TotalRebounds  Opp.Assists  \\\n",
       "0            0.818               16                 48           26   \n",
       "1            0.857               11                 44           25   \n",
       "2            0.711               11                 50           25   \n",
       "3            0.741               11                 51           31   \n",
       "4            0.727               13                 44           26   \n",
       "\n",
       "   Opp.Steals  Opp.Blocks  Opp.Turnovers  Opp.TotalFouls  \n",
       "0          13           9              9              22  \n",
       "1           5           5             18              26  \n",
       "2           7           9             19              15  \n",
       "3           6           7             19              30  \n",
       "4           2           6             15              29  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"nba\", data.to_pandas())\n",
    "\n",
    "conn.query(\"SELECT * FROM nba LIMIT 5\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping text2sql\n",
    "\n",
    "Now that we have the basic data in place, let's implement the text2sql logic. Don't overcomplicate it at the start. We can always improve its implementation later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "import braintrust\n",
    "import openai\n",
    "\n",
    "client = braintrust.wrap_openai(\n",
    "    openai.AsyncClient(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=\"https://braintrustproxy.com/v1\",  # This is optional and allows us to cache responses\n",
    "    )\n",
    ")\n",
    "\n",
    "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": dedent(f\"\"\"\\\n",
    "        You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "        {\", \".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\n",
    "\n",
    "        Write a SQL query corresponding to the user's request. Return just the query text, with no\n",
    "        formatting (backticks, markdown, etc.).\n",
    "\"\"\"),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "query = await generate_query(\"Who won the most games?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's try running it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Team': 'GSW', 'Wins': 265}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial evals\n",
    "\n",
    "An `Eval()` consists of three parts — data, task, and scores. We'll start with **data**.\n",
    "\n",
    "### Creating an initial dataset\n",
    "\n",
    "Let's handwrite a few examples to bootstrap the dataset. It'll be a real pain, and probably brittle, to try and handwrite both questions and SQL queries/outputs. Instead,\n",
    "we'll just write some questions, and try to evaluate the outputs without an _expected_ answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task function\n",
    "\n",
    "Now let's write a task function. The function should take input (the question) and return output (the SQL query and results).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@braintrust.traced\n",
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n",
    "\n",
    "At this point, there's not a lot we can score, but we can at least check if the SQL query is valid. If we generate an invalid query, the `error` field will be non-empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def no_error(output):\n",
    "    return output[\"error\"] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "\n",
    "And that's it! Now let's plug these things together and run an eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Initial dataset is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Initial%20dataset\n",
      "LLM Eval for Text2SQL [experiment_name=Initial dataset] (data): 5it [00:00, 33078.11it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4a25917a77401dbba65bffaa1161c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM Eval for Text2SQL [experiment_name=Initial dataset] (tasks):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "60.00% 'no_error' score\n",
      "\n",
      "See results for Initial dataset at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Initial%20dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Eval\n",
    "\n",
    "PROJECT_NAME = \"LLM Eval for Text2SQL\"\n",
    "\n",
    "await Eval(\n",
    "    PROJECT_NAME,\n",
    "    experiment_name=\"Initial dataset\",\n",
    "    data=[{\"input\": q} for q in questions],\n",
    "    task=text2sql,\n",
    "    scores=[no_error],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! It looks like 3/5 of our queries are valid. Let's take a look at them in the UI to see whether they're good.\n",
    "\n",
    "### Capturing good data\n",
    "\n",
    "Now that we ran the initial eval, it looks like two of the results are valid, two errored, and one is incorrect. A few key observations:\n",
    "\n",
    "- Let's capture the good data into a dataset. We can now test correctness by comparing the output of these golden examples with whatever our `task` function generates\n",
    "- The incorrect query didn't seem to get the date format correct. That would probably be improved by showing a sample of the data to the model\n",
    "- There are two binder errors, which may be quirks in DuckDB syntax. Let's see if we can fix this.\n",
    "\n",
    "Let's start by reworking our data function to include our new queries with expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '614006b1-a8b1-40c2-b700-3634c4fb14f5',\n",
       " '_xact_id': '1000193117554478505',\n",
       " 'created': '2024-05-29 16:23:59.989+00',\n",
       " 'project_id': 'b8d44d19-7999-49b0-911b-1f0aaafc5bac',\n",
       " 'dataset_id': 'a6c337e3-f7f7-4a96-8529-05cb172f847e',\n",
       " 'input': 'Which team won the most games?',\n",
       " 'expected': {'error': None,\n",
       "  'query': \"SELECT Team, COUNT(*) AS Wins\\nFROM nba\\nWHERE WINorLOSS = 'W'\\nGROUP BY Team\\nORDER BY Wins DESC\\nLIMIT 1;\",\n",
       "  'results': [{'Team': 'GSW', 'Wins': 265}]},\n",
       " 'metadata': {},\n",
       " 'tags': [],\n",
       " 'span_id': '614006b1-a8b1-40c2-b700-3634c4fb14f5',\n",
       " 'root_span_id': '614006b1-a8b1-40c2-b700-3634c4fb14f5'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import init_dataset\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    golden_data = init_dataset(PROJECT_NAME, \"Golden data\")\n",
    "    golden_questions = set(d[\"input\"] for d in golden_data)\n",
    "    return list(golden_data) + [{\"input\": q} for q in questions if q not in golden_questions]\n",
    "\n",
    "\n",
    "load_data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tweak the prompt to include a sample of each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W' AND Date LIKE '%/15'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "samples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": dedent(f\"\"\"\\\n",
    "        You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "\n",
    "        Column | Type | Example\n",
    "        -------|------|--------\n",
    "        {\"\\n\".join(f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\" for column in columns)}\n",
    "\n",
    "        Write a DuckDB SQL query corresponding to the user's request. Return just the query text, with no\n",
    "        formatting (backticks, markdown, etc.).\n",
    "\"\"\"),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(await generate_query(\"Which team won the most games in 2015?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoevals import JSONDiff, Sql\n",
    "\n",
    "\n",
    "def extract_values(results):\n",
    "    return [list(result.values()) for result in results]\n",
    "\n",
    "\n",
    "def correct_result(output, expected):\n",
    "    if expected is None or expected.get(\"results\") is None or output.get(\"results\") is None:\n",
    "        return None\n",
    "    return JSONDiff()(output=extract_values(output[\"results\"]), expected=extract_values(expected[\"results\"])).score\n",
    "\n",
    "\n",
    "def correct_sql(input, output, expected):\n",
    "    if expected is None or expected.get(\"query\") is None or output.get(\"query\") is None:\n",
    "        return None\n",
    "    return Sql()(input=input, output=output[\"query\"], expected=expected[\"query\"]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's plug these pieces together and run an eval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment With samples is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/With%20samples\n",
      "LLM Eval for Text2SQL [experiment_name=With samples] (data): 5it [00:00, 17848.10it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411ec1a94e0946cd8f598b68db8994b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM Eval for Text2SQL [experiment_name=With samples] (tasks):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "With samples compared to Initial dataset:\n",
      "80.00% (+20.00%) 'no_error'       score\t(1 improvements, 0 regressions)\n",
      "100.00% 'correct_result' score\n",
      "100.00% 'correct_sql'    score\n",
      "\n",
      "5.78s duration\n",
      "\n",
      "See results for With samples at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/With%20samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    PROJECT_NAME,\n",
    "    experiment_name=\"With samples\",\n",
    "    data=load_data,\n",
    "    task=text2sql,\n",
    "    scores=[no_error, correct_result, correct_sql],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. The fixed query's result is not quite right, but let's add the other query to our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more data\n",
    "\n",
    "Now that we have a basic flow in place, let's generate some data. We're going to use the dataset itself to generate expected queries, and have a model describe the queries.\n",
    "This is a slightly more robust method than having it generate queries, because we'd expect a model to describe a query more accurately than generate one from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql': \"SELECT Team, COUNT(*) as TotalGames, SUM(CASE WHEN WINorLOSS = 'W' THEN 1 ELSE 0 END) as Wins, SUM(CASE WHEN WINorLOSS = 'L' THEN 1 ELSE 0 END) as Losses FROM nba GROUP BY Team;\",\n",
       " 'question': 'What is the total number of games played, wins, and losses for each team?'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    sql: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    questions: list[Question]\n",
    "\n",
    "\n",
    "logger = braintrust.init_logger(\"question generator\")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": dedent(f\"\"\"\\\n",
    "        You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "\n",
    "        Column | Type | Example\n",
    "        -------|------|--------\n",
    "        {\"\\n\".join(f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\" for column in columns)}\n",
    "\n",
    "        Generate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\n",
    "        question that the query answers.\"\"\"),\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_questions\",\n",
    "                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n",
    "                \"parameters\": Questions.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n",
    ")\n",
    "\n",
    "generated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\"questions\"]\n",
    "generated_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query failed: SELECT Team, AVG(FieldGoals.) as AvgFieldGoalPercentage, AVG(X3PointShots.) as Avg3PointPercentage, AVG(FreeThrows.) as AvgFreeThrowPercentage FROM nba GROUP BY Team; Parser Error: syntax error at or near \")\"\n",
      "Skipping...\n",
      "Query failed: SELECT Team, AVG(Opp.FieldGoals.) as AvgOppFieldGoalPercentage, AVG(Opp.3PointShots.) as AvgOpp3PointPercentage, AVG(Opp.FreeThrows.) as AvgOppFreeThrowPercentage FROM nba GROUP BY Team; Parser Error: syntax error at or near \")\"\n",
      "Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the total number of games played, wins, and losses for each team?',\n",
       " 'expected': {'results': [{'Team': 'ATL',\n",
       "    'TotalGames': 328,\n",
       "    'Wins': 175.0,\n",
       "    'Losses': 153.0},\n",
       "   {'Team': 'CHI', 'TotalGames': 328, 'Wins': 160.0, 'Losses': 168.0},\n",
       "   {'Team': 'NYK', 'TotalGames': 328, 'Wins': 109.0, 'Losses': 219.0},\n",
       "   {'Team': 'POR', 'TotalGames': 328, 'Wins': 185.0, 'Losses': 143.0},\n",
       "   {'Team': 'DEN', 'TotalGames': 328, 'Wins': 149.0, 'Losses': 179.0},\n",
       "   {'Team': 'UTA', 'TotalGames': 328, 'Wins': 177.0, 'Losses': 151.0},\n",
       "   {'Team': 'BRK', 'TotalGames': 328, 'Wins': 107.0, 'Losses': 221.0},\n",
       "   {'Team': 'CHO', 'TotalGames': 328, 'Wins': 153.0, 'Losses': 175.0},\n",
       "   {'Team': 'DAL', 'TotalGames': 328, 'Wins': 149.0, 'Losses': 179.0},\n",
       "   {'Team': 'LAC', 'TotalGames': 328, 'Wins': 202.0, 'Losses': 126.0},\n",
       "   {'Team': 'DET', 'TotalGames': 328, 'Wins': 152.0, 'Losses': 176.0},\n",
       "   {'Team': 'GSW', 'TotalGames': 328, 'Wins': 265.0, 'Losses': 63.0},\n",
       "   {'Team': 'IND', 'TotalGames': 328, 'Wins': 173.0, 'Losses': 155.0},\n",
       "   {'Team': 'MIA', 'TotalGames': 328, 'Wins': 170.0, 'Losses': 158.0},\n",
       "   {'Team': 'MIL', 'TotalGames': 328, 'Wins': 160.0, 'Losses': 168.0},\n",
       "   {'Team': 'SAC', 'TotalGames': 328, 'Wins': 121.0, 'Losses': 207.0},\n",
       "   {'Team': 'OKC', 'TotalGames': 328, 'Wins': 195.0, 'Losses': 133.0},\n",
       "   {'Team': 'PHI', 'TotalGames': 328, 'Wins': 108.0, 'Losses': 220.0},\n",
       "   {'Team': 'PHO', 'TotalGames': 328, 'Wins': 107.0, 'Losses': 221.0},\n",
       "   {'Team': 'SAS', 'TotalGames': 328, 'Wins': 230.0, 'Losses': 98.0},\n",
       "   {'Team': 'BOS', 'TotalGames': 328, 'Wins': 196.0, 'Losses': 132.0},\n",
       "   {'Team': 'HOU', 'TotalGames': 328, 'Wins': 217.0, 'Losses': 111.0},\n",
       "   {'Team': 'LAL', 'TotalGames': 328, 'Wins': 99.0, 'Losses': 229.0},\n",
       "   {'Team': 'MIN', 'TotalGames': 328, 'Wins': 123.0, 'Losses': 205.0},\n",
       "   {'Team': 'TOR', 'TotalGames': 328, 'Wins': 215.0, 'Losses': 113.0},\n",
       "   {'Team': 'CLE', 'TotalGames': 328, 'Wins': 211.0, 'Losses': 117.0},\n",
       "   {'Team': 'MEM', 'TotalGames': 328, 'Wins': 162.0, 'Losses': 166.0},\n",
       "   {'Team': 'NOP', 'TotalGames': 328, 'Wins': 157.0, 'Losses': 171.0},\n",
       "   {'Team': 'ORL', 'TotalGames': 328, 'Wins': 114.0, 'Losses': 214.0},\n",
       "   {'Team': 'WAS', 'TotalGames': 328, 'Wins': 179.0, 'Losses': 149.0}],\n",
       "  'error': None,\n",
       "  'query': \"SELECT Team, COUNT(*) as TotalGames, SUM(CASE WHEN WINorLOSS = 'W' THEN 1 ELSE 0 END) as Wins, SUM(CASE WHEN WINorLOSS = 'L' THEN 1 ELSE 0 END) as Losses FROM nba GROUP BY Team;\"},\n",
       " 'metadata': {'category': 'Generated'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset = []\n",
    "for q in generated_questions:\n",
    "    try:\n",
    "        result = execute_query(q[\"sql\"])\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"input\": q[\"question\"],\n",
    "                \"expected\": {\n",
    "                    \"results\": result,\n",
    "                    \"error\": None,\n",
    "                    \"query\": q[\"sql\"],\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"Generated\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Query failed: {q['sql']}\", e)\n",
    "        print(\"Skipping...\")\n",
    "\n",
    "generated_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's update our dataset with the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    golden_data = init_dataset(PROJECT_NAME, \"Golden data\")\n",
    "    golden_questions = set(d[\"input\"] for d in golden_data)\n",
    "    return (\n",
    "        [{**x, \"metadata\": {\"category\": \"Golden data\"}} for x in golden_data]\n",
    "        + [\n",
    "            {\"input\": q, \"metadata\": {\"category\": \"Handwritten question\"}}\n",
    "            for q in questions\n",
    "            if q not in golden_questions\n",
    "        ]\n",
    "        + [x for x in generated_dataset if x[\"input\"] not in golden_questions]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Generated data is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Generated%20data\n",
      "LLM Eval for Text2SQL [experiment_name=Generated data] (data): 13it [00:00, 36916.69it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878bb9811d0d43d29c4011e9526cb75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM Eval for Text2SQL [experiment_name=Generated data] (tasks):   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Generated data compared to With samples:\n",
      "84.62% (-) 'no_error'       score\t(0 improvements, 0 regressions)\n",
      "69.72% (-) 'correct_result' score\t(0 improvements, 0 regressions)\n",
      "63.64% (-) 'correct_sql'    score\t(0 improvements, 0 regressions)\n",
      "\n",
      "4.23s (-155.93%) 'duration'\t(0 improvements, 0 regressions)\n",
      "\n",
      "See results for Generated data at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Generated%20data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(...)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    PROJECT_NAME,\n",
    "    experiment_name=\"Generated data\",\n",
    "    data=load_data,\n",
    "    task=text2sql,\n",
    "    scores=[no_error, correct_result, correct_sql],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing, now we have a rich dataset to work with, and more importantly, a rich workflow for generating data, capturing good examples as golden datasets, and iterating on our app. Let's quickly try gpt-4 and see how it does!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Try gpt-4 is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Try%20gpt-4\n",
      "LLM Eval for Text2SQL [experiment_name=Try gpt-4] (data): 13it [00:00, 25491.33it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7e00d40b2a4af29a72902e778c8b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM Eval for Text2SQL [experiment_name=Try gpt-4] (tasks):   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Try gpt-4 compared to Generated data:\n",
      "46.14% (-23.58%) 'correct_result' score\t(1 improvements, 5 regressions)\n",
      "84.62% (-) 'no_error'       score\t(1 improvements, 1 regressions)\n",
      "54.55% (-09.09%) 'correct_sql'    score\t(1 improvements, 2 regressions)\n",
      "\n",
      "6.77s (+254.27%) 'duration'\t(0 improvements, 1 regressions)\n",
      "\n",
      "See results for Try gpt-4 at https://www.braintrust.dev/app/braintrustdata.com/p/LLM%20Eval%20for%20Text2SQL/experiments/Try%20gpt-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK_MODEL = \"gpt-4\"\n",
    "\n",
    "await Eval(\n",
    "    PROJECT_NAME,\n",
    "    experiment_name=\"Try gpt-4\",\n",
    "    data=load_data,\n",
    "    task=text2sql,\n",
    "    scores=[no_error, correct_result, correct_sql],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
