{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt versioning\n",
    "\n",
    "Successfully moving an AI project from prototype to production requires systematic testing of different prompts. A common challenge that causes lots of iteration is ensuring the responses are accurate and aligned with your brand's guidelines. This process involves creating variations, measuring their effectiveness, and sometimes returning to previous versions that performed better.\n",
    "\n",
    "In this cookbook, we'll build a support chatbot and walk through the complete cycle of prompt development. Starting with a basic implementation, we'll create increasingly sophisticated prompts, evaluate their performance, and demonstrate how to revert to previous versions when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before getting started, make sure you have a [Braintrust account](https://www.braintrust.dev/signup) and an API key for [OpenAI](https://platform.openai.com/signup). Make sure to plug the OpenAI key into your Braintrust account's [AI provider configuration](https://www.braintrust.dev/app/settings?subroute=secrets). \n",
    "\n",
    "Once you have your Braintrust account set up with an OpenAI API key, install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install braintrust autoevals openai asyncio nest_asyncio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we'll import the libraries we need and authenticate with Braintrust. To do this, export your `BRAINTRUST_API_KEY` as an environment variable:\n",
    "```bash\n",
    "export BRAINTRUST_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "```\n",
    "<Callout type=\"info\">\n",
    "Exporting your API key is a best practice, but to make it easier to follow along with this cookbook, you can also hardcode it into the code below.\n",
    "</Callout>\n",
    "\n",
    "Once the API key is set, we initialize the OpenAI client using the AI proxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import OpenAI\n",
    "from braintrust import Eval, wrap_openai, invoke\n",
    "from autoevals import LLMClassifier\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in a notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "os.environ[\"BRAINTRUST_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n",
    "\n",
    "# Initialize OpenAI client with Braintrust wrapper\n",
    "client = wrap_openai(\n",
    "    OpenAI(\n",
    "        base_url=\"https://api.braintrust.dev/v1/proxy\",\n",
    "        api_key=os.environ[\"BRAINTRUST_API_KEY\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Project name for consistency\n",
    "project_name = \"SupportChatbot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset\n",
    "\n",
    "We'll create a dataset of customer inquiries to evaluate our prompts. In a production application, you'll want to use real customer interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"input\": \"Why did my package disappear after tracking showed it was delivered?\",\n",
    "        \"metadata\": {\"category\": \"shipping\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Your product smells like burnt rubber - what’s wrong with it?\",\n",
    "        \"metadata\": {\"category\": \"product\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I ordered 3 items but only got 1, where’s the rest?\",\n",
    "        \"metadata\": {\"category\": \"shipping\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Why does your app crash every time I try to check out?\",\n",
    "        \"metadata\": {\"category\": \"tech\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"My refund was supposed to be here 2 weeks ago - what’s the holdup?\",\n",
    "        \"metadata\": {\"category\": \"returns\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Your instructions say ‘easy setup’ but it took me 3 hours!\",\n",
    "        \"metadata\": {\"category\": \"product\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Why does your delivery guy keep leaving packages at the wrong house?\",\n",
    "        \"metadata\": {\"category\": \"shipping\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"The discount code you sent me doesn’t work - fix it!\",\n",
    "        \"metadata\": {\"category\": \"sales\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Your support line hung up on me twice - what’s going on?\",\n",
    "        \"metadata\": {\"category\": \"support\"},\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Why is your website saying my account doesn’t exist when I just made it?\",\n",
    "        \"metadata\": {\"category\": \"tech\"},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a scoring function\n",
    "\n",
    "When evaluating support responses, we need to measure more than just accuracy. To do this, we use an [LLMClassifier](https://github.com/braintrustdata/autoevals?tab=readme-ov-file#python-3) to assess if responses maintain the right tone while solving customer problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand alignment scorer defined with specific guidelines.\n"
     ]
    }
   ],
   "source": [
    "brand_alignment_scorer = LLMClassifier(\n",
    "    name=\"BrandAlignment\",\n",
    "    prompt_template=\"\"\"\n",
    "    Evaluate if the response aligns with our brand guidelines (Y/N):\n",
    "    1. **Positive Tone**: Uses upbeat language, avoids negativity (e.g., \"We’re thrilled to help!\" vs. \"That’s your problem\").\n",
    "    2. **Proactive Approach**: Offers a clear next step or solution (e.g., \"We’ll track it now!\" vs. vague promises).\n",
    "    3. **Apologetic When Appropriate**: Acknowledges issues with empathy (e.g., \"So sorry for the mix-up!\" vs. ignoring the complaint).\n",
    "    4. **Solution-Oriented**: Focuses on fixing the issue for the customer (e.g., \"Here’s how we’ll make it right!\" vs. excuses).\n",
    "    5. **Professionalism**: There should be no profanity, or emojis.\n",
    "    \n",
    "    Response: {{output}}\n",
    "\n",
    "\n",
    "    Only give a Y if all the criteria are met. If one is missing and it should be there, give a N.\n",
    "    \"\"\",\n",
    "    choice_scores={\"Y\": 1, \"N\": 0},\n",
    "    use_cot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt\n",
    "\n",
    "Let's start with a basic prompt that focuses on providing direct responses to customer inquiries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1_content = \"\"\"\n",
    "import braintrust\n",
    "\n",
    "project = braintrust.projects.create(name=\"SupportChatbot\")\n",
    "\n",
    "prompt_v1 = project.prompts.create(\n",
    "    name=\"Brand Support V1\",\n",
    "    slug=\"brand-support-v1\",\n",
    "    description=\"Simple support prompt\",\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"{{{input}}}\"}\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"prompt_v1.py\", \"w\") as f:\n",
    "    f.write(prompt_v1_content)\n",
    "\n",
    "# Push to Braintrust with overwrite and verbosity\n",
    "push_command = \"braintrust push prompt_v1.py --if-exists replace --verbose\"\n",
    "result = subprocess.run(push_command, shell=True, capture_output=True, text=True)\n",
    "print(f\"Pushing prompt_v1.py to Braintrust:\")\n",
    "print(f\"CLI Output: {result.stdout}\")\n",
    "if result.stderr:\n",
    "    print(f\"CLI Error: {result.stderr}\")\n",
    "if result.returncode != 0:\n",
    "    print(f\"Push failed with return code {result.returncode}\")\n",
    "else:\n",
    "    print(\"Push succeeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pushing the prompt, you'll see it in the Braintrust UI.\n",
    "\n",
    "![prompts](./assets/prompts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our first evaluation\n",
    "\n",
    "With our prompt ready, we'll create a task function and run our first evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task using invoke with correct input\n",
    "def task_v1(input):\n",
    "    result = invoke(\n",
    "        project_name=project_name,\n",
    "        slug=\"brand-support-v1\",\n",
    "        input={\"input\": input},  # Matches {{{input}}} in our prompt\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "eval_task = Eval(\n",
    "    project_name,\n",
    "    data=lambda: dataset,\n",
    "    task=task_v1,\n",
    "    scores=[brand_alignment_scorer],\n",
    "    experiment_name=\"prompt_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our prompt\n",
    "\n",
    "Our initial evaluation showed that there is room for improvement. Let's create a more sophisticated prompt that incorporates our brand guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v2_content = \"\"\"\n",
    "import braintrust\n",
    "\n",
    "project = braintrust.projects.create(name=\"SupportChatbot\")\n",
    "\n",
    "prompt_v2 = project.prompts.create(\n",
    "    name=\"Brand Support V2\",\n",
    "    slug=\"brand-support-v2\",\n",
    "    description=\"Brand-aligned support prompt\",\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You’re a cheerful, proactive assistant for Sunshine Co. Always use a positive tone, apologize for issues with empathy, and offer clear solutions to delight customers!\"},\n",
    "        {\"role\": \"user\", \"content\": \"{{{input}}}\"}\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"prompt_v2.py\", \"w\") as f:\n",
    "    f.write(prompt_v2_content)\n",
    "\n",
    "push_command = \"braintrust push prompt_v2.py --if-exists replace --verbose\"\n",
    "result = subprocess.run(push_command, shell=True, capture_output=True, text=True)\n",
    "print(f\"Pushing prompt_v2.py to Braintrust:\")\n",
    "print(f\"CLI Output: {result.stdout}\")\n",
    "if result.stderr:\n",
    "    print(f\"CLI Error: {result.stderr}\")\n",
    "if result.returncode != 0:\n",
    "    print(f\"Push failed with return code {result.returncode}\")\n",
    "else:\n",
    "    print(\"Push succeeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that we're pointing to the slug of our new prompt in our task function since that's what we're evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_v2(input):\n",
    "    result = invoke(\n",
    "        project_name=project_name,\n",
    "        slug=\"brand-support-v2\",\n",
    "        input={\"input\": input},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "eval_task = Eval(\n",
    "    project_name,\n",
    "    data=lambda: dataset,\n",
    "    task=task_v2,\n",
    "    scores=[brand_alignment_scorer],\n",
    "    experiment_name=\"prompt_v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with tone\n",
    "\n",
    "Building on these improvements, we'll push the brand voice into a more energetic direction, testing how increased enthusiasm affects response quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v3_content = \"\"\"\n",
    "import braintrust\n",
    "\n",
    "project = braintrust.projects.create(name=\"SupportChatbot\")\n",
    "\n",
    "prompt_v3 = project.prompts.create(\n",
    "    name=\"Brand Support V3\",\n",
    "    slug=\"brand-support-v3\",\n",
    "    description=\"Over-enthusiastic support prompt with middling performance\",\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You’re a SUPER EXCITED Sunshine Co. assistant! SHOUT IN ALL CAPS WITH LOTS OF EXCLAMATIONS!!!! SAY SORRY IF SOMETHING’S WRONG BUT KEEP IT VAGUE AND FUN!!! Make customers HAPPY with BIG ENERGY, even if solutions are UNCLEAR!!!!\"},\n",
    "        {\"role\": \"user\", \"content\": \"{{{input}}}\"}\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(\"prompt_v3.py\", \"w\") as f:\n",
    "    f.write(prompt_v3_content)\n",
    "\n",
    "\n",
    "push_command = \"braintrust push prompt_v3.py --if-exists replace --verbose\"\n",
    "result = subprocess.run(push_command, shell=True, capture_output=True, text=True)\n",
    "print(f\"Pushing prompt_v3.py to Braintrust:\")\n",
    "print(f\"CLI Output: {result.stdout}\")\n",
    "if result.stderr:\n",
    "    print(f\"CLI Error: {result.stderr}\")\n",
    "if result.returncode != 0:\n",
    "    print(f\"Push failed with return code {result.returncode}\")\n",
    "else:\n",
    "    print(\"Push succeeded.\")\n",
    "\n",
    "\n",
    "def task_v3(input):\n",
    "    result = invoke(\n",
    "        project_name=project_name,\n",
    "        slug=\"brand-support-v3\",\n",
    "        input={\"input\": input},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "eval_task = Eval(\n",
    "    project_name,\n",
    "    data=lambda: dataset,\n",
    "    task=task_v3,\n",
    "    scores=[brand_alignment_scorer],\n",
    "    experiment_name=\"prompt_v3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the new prompt doesn't perform as well as the previous one. In a production application, this is where you'll want to run more evaluations to find an optimal prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing prompt versions\n",
    "\n",
    "After running evaluations on all three versions, we found that our second prompt achieved the highest score. While we've iterated on the prompt, Braintrust makes it simple to revert to this high-performing version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_reverted(input):\n",
    "    result = invoke(\n",
    "        project_name=project_name,\n",
    "        slug=\"brand-support-v2\",\n",
    "        input={\"input\": input},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "eval_task = Eval(\n",
    "    project_name,\n",
    "    data=lambda: dataset,\n",
    "    task=task_reverted,\n",
    "    scores=[brand_alignment_scorer],\n",
    "    experiment_name=\"prompt_v2_reverted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to not change the slug of your prompt, don't worry. Braintrust keeps track of changes allowing you to revert to any previous version.\n",
    "\n",
    "![versions](./assets/versions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Now that you have some prompts saved, you can rapidly test them with new models in our [prompt playground](/docs/guides/playground).\n",
    "- Learn more about [evaluating a chat assistant](/docs/cookbook/recipes/EvaluatingChatAssistant).\n",
    "- Think about how you might add more sophisticated [scoring functions](/docs/guides/evals/write#scorers) to your evals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
