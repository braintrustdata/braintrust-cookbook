# Using hosted tools to perform RAG

## Introducing the project

Using Braintrust functions, a RAG agent can be defined as just two components:

- A system prompt containing instructions for how to retrieve content and synthesize answers
- A vector search tool, implemented in TypeScript, which embeds a query, searches for relevant documents, and returns them

We're going to define an agent that answers questions about the Braintrust documentation.

Along the way, we'll also experience how simple it is to work across Braintrust's UI and working in your code â€” the new AI engineering workflow.

## Getting started

To get started, you'll need a few accounts:

- [Braintrust](https://www.braintrust.dev/signup)
- [Pinecone](https://app.pinecone.io/?sessionType=signup)
- [OpenAI](https://platform.openai.com/signup)

and `node`, `npm`, and `typescript` installed locally. If you'd like to follow along in code,
the [tool-rag](https://github.com/braintrustdata/braintrust-cookbook/tree/main/examples/ToolRAG/tool-rag)
project contains a working example with all of the documents and code snippets we'll use.

## Clone the repo

To start, clone the repo and install the dependencies:

```bash
git clone https://github.com/braintrustdata/braintrust-cookbook.git
cd braintrust-cookbook/examples/ToolRAG/tool-rag
npm install
```

Next, create a `.env.local` file with your API keys:

```bash
BRAINTRUST_API_KEY=<your-api-key>
PINECONE_API_KEY=<your-pinecone-api-key>
```

Finally, make sure to set your `OPENAI_API_KEY` environment variable in the [AI secrets](https://www.braintrust.dev/app/settings?subroute=api-keys) section
of your account, and set the `PINECONE_API_KEY` environment variable in the [Environment variables](https://www.braintrust.dev/app/settings?subroute=env-vars) section.

<Callout type="info">
  We'll use the local environment variables to embed and upload the vectors, and
  the Braintrust variables to run the RAG tool and LLM calls remotely.
</Callout>

## Upload the vectors

To upload the vectors, simply run the `upload-vectors.ts` script:

```bash
npx tsx upload-vectors.ts
```

That's it for setup! Now let's try to retrieve them using Braintrust.

## Creating a RAG tool

Braintrust makes it simple to create tools, and then run them in the UI, API, and of course, via prompts. This is
an easy way to iterate on assistant-style agents.

The retrieval tool is defined in `retrieval.ts`. The tool itself is incredibly simple:

```javascript
async ({ query, top_k }) => {
  const embedding = await openai.embeddings
    .create({
      input: query,
      model: "text-embedding-3-small",
    })
    .then((res) => res.data[0].embedding);

  const queryResponse = await pc.query({
    vector: embedding,
    topK: top_k,
    includeMetadata: true,
  });

  return queryResponse.matches.map((match) => ({
    title: match.metadata?.title,
    content: match.metadata?.content,
  }));
};
```

To push the tool to Braintrust, simply run:

```bash
npx braintrust push retrieval.ts
```

You should see:

```
1 file uploaded successfully
```

### Try out the tool

To try out the tool, visit the project in Braintrust, and navigate to the "Tools" section of your "Library".

![Test tool](./assets/Test-tool.gif)

This is an easy way to try out searches, and refine the logic. For example, you could try playing with various
`top_k` values, or adding a prefix to the query to guide the results. If you change the code, simply run
`npx braintrust push retrieval.ts` again to update the tool.

## Writing a prompt

Next, let's wire the tool into a prompt. In `prompt.ts`, you'll see an initial definition of the prompt:

```javascript
  messages: [
    {
      role: "system",
      content:
        "You are a helpful assistant that can " +
        "answer questions about the Braintrust documentation.",
    },
    {
      role: "user",
      content: "{{{question}}}",
    },
  ],
```

Run the following command to initialize the prompt:

```
npx braintrust push prompt.ts
```

Once the prompt uploads, you can run it in the UI and even try it out on some examples:

![Test prompt](./assets/Test-prompt.gif)

If you visit the logs tab, you'll see detailed logs for each call:

![Prompt logs](./assets/Prompt-logs.png)

<Callout type="info">
  We recommend using code-based prompts to initialize projects, but as we'll see
  in a moment, it is very convenient to tweak your prompts in the UI once you
  initialize them.
</Callout>

## Import a dataset

TODO:

- Upload dataset
- Create playground
- Add scorer
- Run eval
- Duplicate, instruct to show both py and ts examples
- Run full experiments

## Writing a custom scorer

Write a custom LLM as a judge scorer

## Running your first evaluation

- Bad eval without tool, not accurate

## Adding in the tool and re-running your evaluation

- success!
